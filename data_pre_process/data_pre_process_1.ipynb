{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) \n",
    "from os import listdir\n",
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "job_path = \"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data\"\n",
    "courses_path = \"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/courses_data\"\n",
    "skills_path = \"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/Datasets/skills_df_updated.csv\"\n",
    "\n",
    "files_j= [f for f in listdir(job_path) if f.endswith(\".csv\")]\n",
    "df_j = pd.concat([pd.read_csv(os.path.join(job_path,f)) for f in files_j])\n",
    "\n",
    "files_c= [f for f in listdir(courses_path) if f.endswith(\".csv\")]\n",
    "df_c = pd.concat([pd.read_csv(os.path.join(courses_path,f)) for f in files_c])\n",
    "\n",
    "#files_s= [f for f in listdir(skills_path) if f.endswith(\".csv\")]\n",
    "df_s = pd.read_csv(skills_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_s[\"Unnamed: 5\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure you have the necessary NLTK tokens downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "    # Use spaCy's pipeline for processing the passed text\n",
    "    doc = nlp(text)\n",
    "    preprocessed_text= []\n",
    "    # Return a preprocessed version of the text (for instance, lemmatized, lowercased, no stop words/punctuations)\n",
    "    for token in doc:\n",
    "        # Skip stop words and punctuation\n",
    "        if token.is_stop or token.is_punct or token.is_space:\n",
    "            continue\n",
    "        # Special case for 'data' to keep it in plural form\n",
    "        if token.lemma_ == 'datum':\n",
    "            preprocessed_text.append('data')\n",
    "        else:\n",
    "            preprocessed_text.append(token.text.lower())\n",
    "\n",
    "    return ' '.join(preprocessed_text)\n",
    "\n",
    "\n",
    "# Function to clean and lemmatize job title text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean skills dataset \n",
    "\n",
    "skills = pd.DataFrame(df_s[df_s[\"Unnamed: 5\"].notna()][\"Unnamed: 5\"].unique())\n",
    "skills.columns=[\"skills\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills[\"skills\"] = skills[\"skills\"].apply(preprocess_text_spacy)\n",
    "skills[\"skills\"] = skills[\"skills\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visualization',\n",
       " 'b testing',\n",
       " 'ansi c',\n",
       " 'api gateway',\n",
       " 'api management',\n",
       " 'apl programming language',\n",
       " 'asp net',\n",
       " 'asp net core',\n",
       " 'asp net core mvc',\n",
       " 'asp net extension ajax',\n",
       " 'asp net fundamental',\n",
       " 'asp net identity',\n",
       " 'asp net mvc',\n",
       " 'asp net mvc framework',\n",
       " 'asp net razor',\n",
       " 'asp net web api',\n",
       " 'awk programming language',\n",
       " 'aws amplify',\n",
       " 'aws app mesh',\n",
       " 'aws appsync',\n",
       " 'aws auto scaling',\n",
       " 'aws backup',\n",
       " 'aws batch',\n",
       " 'aws cli command line interface',\n",
       " 'aws cloud development kit cdk',\n",
       " 'aws cloudformation',\n",
       " 'aws cloudhsm',\n",
       " 'aws cloudtrail',\n",
       " 'aws codebuild',\n",
       " 'aws codecommit',\n",
       " 'aws codedeploy',\n",
       " 'aws codepipeline',\n",
       " 'aws cost management',\n",
       " 'aws directory service',\n",
       " 'aws elastic beanstalk',\n",
       " 'aws elastic mapreduce emr',\n",
       " 'aws fargate',\n",
       " 'aws glue',\n",
       " 'aws identity access management iam',\n",
       " 'aws inferentia',\n",
       " 'aws internet thing iot',\n",
       " 'aws key management service km',\n",
       " 'aws kinesis',\n",
       " 'aws lambda',\n",
       " 'aws opsworks',\n",
       " 'aws outpost',\n",
       " 'aws sdk',\n",
       " 'aws sagemaker',\n",
       " 'aws serverless',\n",
       " 'aws user pool',\n",
       " 'abstract class',\n",
       " 'abstract data type',\n",
       " 'acceptance testing',\n",
       " 'activepython python package',\n",
       " 'ad hoc analysis',\n",
       " 'ad hoc marketing',\n",
       " 'ad hoc reporting',\n",
       " 'ad hoc testing',\n",
       " 'ada programming support environment apse',\n",
       " 'adaboost adaptive boosting',\n",
       " 'adhoc query',\n",
       " 'aggregation analysis',\n",
       " 'agile management',\n",
       " 'agile methodology',\n",
       " 'agile project management',\n",
       " 'agile project',\n",
       " 'agile software development',\n",
       " 'agile testing',\n",
       " 'airflow',\n",
       " 'ajax net',\n",
       " 'alation data catalog',\n",
       " 'algebra',\n",
       " 'algebraic modeling language',\n",
       " 'algorithm analysis',\n",
       " 'algorithm design',\n",
       " 'algorithmic trading',\n",
       " 'algorithm',\n",
       " 'alteryx',\n",
       " 'amazon api gateway',\n",
       " 'amazon alexa',\n",
       " 'amazon appstore',\n",
       " 'amazon appstream',\n",
       " 'amazon athena',\n",
       " 'amazon aurora',\n",
       " 'amazon cloud directory',\n",
       " 'amazon cloud drive',\n",
       " 'amazon cloudsearch',\n",
       " 'amazon cloudwatch',\n",
       " 'amazon cloudfront',\n",
       " 'amazon cognito',\n",
       " 'amazon comprehend',\n",
       " 'amazon connect',\n",
       " 'amazon data pipeline',\n",
       " 'amazon documentdb',\n",
       " 'amazon dynamodb',\n",
       " 'amazon elasticache',\n",
       " 'amazon elastic block store',\n",
       " 'amazon elastic compute cloud',\n",
       " 'amazon elastic container registry',\n",
       " 'amazon elastic container service',\n",
       " 'amazon elastic file system',\n",
       " 'amazon elastic kubernetes service',\n",
       " 'amazon elasticsearch service',\n",
       " 'amazon flexible payment service',\n",
       " 'amazon forecast',\n",
       " 'amazon guardduty',\n",
       " 'amazon inspector',\n",
       " 'amazon lex',\n",
       " 'amazon lightsail',\n",
       " 'amazon linux amazon machine image ami',\n",
       " 'amazon lumberyard',\n",
       " 'amazon mq',\n",
       " 'amazon macie',\n",
       " 'amazon managed blockchain',\n",
       " 'amazon managed streaming apache kafka amazon msk',\n",
       " 'amazon marketplace',\n",
       " 'amazon mechanical turk',\n",
       " 'amazon neptune',\n",
       " 'amazon personalize',\n",
       " 'amazon polly',\n",
       " 'amazon product advertising api',\n",
       " 'amazon quantum ledger database qldb',\n",
       " 'amazon quicksight',\n",
       " 'amazon redshift',\n",
       " 'amazon rekognition',\n",
       " 'amazon relational database service',\n",
       " 'amazon route',\n",
       " 'amazon',\n",
       " 'amazon bucket',\n",
       " 'amazon glacier',\n",
       " 'amazon simple email service s',\n",
       " 'amazon simple notification service sn',\n",
       " 'amazon simple queue service',\n",
       " 'amazon simple workflow service swf',\n",
       " 'amazon simpledb',\n",
       " 'amazon textract',\n",
       " 'amazon timestream',\n",
       " 'amazon transcribe',\n",
       " 'amazon translate',\n",
       " 'amazon virtual private cloud vpc',\n",
       " 'amazon web service',\n",
       " 'amazon workspace',\n",
       " 'analysis variance anova',\n",
       " 'analytic application',\n",
       " 'analytical dashboard',\n",
       " 'analytical technique',\n",
       " 'analytical testing',\n",
       " 'analytics',\n",
       " 'analytics j javascript library',\n",
       " 'android software development',\n",
       " 'android studio',\n",
       " 'android testing',\n",
       " 'android ui',\n",
       " 'angelscript',\n",
       " 'angular web framework',\n",
       " 'angular cli',\n",
       " 'angular component',\n",
       " 'angular material',\n",
       " 'angular reactive form',\n",
       " 'angular ui',\n",
       " 'ansi sql',\n",
       " 'apache',\n",
       " 'apache accumulo',\n",
       " 'apache activemq',\n",
       " 'apache administration',\n",
       " 'apache airflow',\n",
       " 'apache ambari',\n",
       " 'apache ant',\n",
       " 'apache apex',\n",
       " 'apache archiva',\n",
       " 'apache atlas',\n",
       " 'apache avro',\n",
       " 'apache axis',\n",
       " 'apache beam',\n",
       " 'apache beehive',\n",
       " 'apache cxf',\n",
       " 'apache camel',\n",
       " 'apache cassandra',\n",
       " 'apache cloudstack',\n",
       " 'apache common ognl',\n",
       " 'apache cordova',\n",
       " 'apache derby',\n",
       " 'apache directory',\n",
       " 'apache drill',\n",
       " 'apache druid',\n",
       " 'apache felix',\n",
       " 'apache flex',\n",
       " 'apache flink',\n",
       " 'apache flume',\n",
       " 'apache fop',\n",
       " 'apache giraph',\n",
       " 'apache hbase',\n",
       " 'apache http server',\n",
       " 'apache hadoop',\n",
       " 'apache hive',\n",
       " 'apache ibatis',\n",
       " 'apache iceberg',\n",
       " 'apache ignite',\n",
       " 'apache impala',\n",
       " 'apache jmeter',\n",
       " 'apache jserv protocol',\n",
       " 'apache jackrabbit',\n",
       " 'apache kafka',\n",
       " 'apache karaf',\n",
       " 'apache lucene',\n",
       " 'apache lucene net',\n",
       " 'apache madlib',\n",
       " 'apache mxnet',\n",
       " 'apache mahout',\n",
       " 'apache maven',\n",
       " 'apache mesos',\n",
       " 'apache module',\n",
       " 'apache myfaces',\n",
       " 'apache nifi',\n",
       " 'apache oozie',\n",
       " 'apache opennlp',\n",
       " 'apache openoffice',\n",
       " 'apache openjpa',\n",
       " 'apache openoffice calc',\n",
       " 'apache pdfbox',\n",
       " 'apache poi',\n",
       " 'apache parquet',\n",
       " 'apache phoenix',\n",
       " 'apache pig',\n",
       " 'apache pulsar',\n",
       " 'apache ranger',\n",
       " 'apache singa',\n",
       " 'apache samza',\n",
       " 'apache servicemix',\n",
       " 'apache shiro',\n",
       " 'apache sling',\n",
       " 'apache solr',\n",
       " 'apache spark',\n",
       " 'apache storm',\n",
       " 'apache strut',\n",
       " 'apache subversion',\n",
       " 'apache thrift',\n",
       " 'apache tika',\n",
       " 'apache tile',\n",
       " 'apache tinkerpop',\n",
       " 'apache tomee',\n",
       " 'apache tomcat',\n",
       " 'apache torque',\n",
       " 'apache traffic server',\n",
       " 'apache turbine',\n",
       " 'apache velocity',\n",
       " 'apache wicket',\n",
       " 'apache yarn',\n",
       " 'apache zeppelin',\n",
       " 'apollo graphql',\n",
       " 'application data',\n",
       " 'application deployment',\n",
       " 'application design',\n",
       " 'application development',\n",
       " 'application development language',\n",
       " 'application development system online adso',\n",
       " 'application environment',\n",
       " 'application firewall',\n",
       " 'application foundation class',\n",
       " 'application framework',\n",
       " 'application integration architecture',\n",
       " 'application interface framework',\n",
       " 'application kit',\n",
       " 'application layer',\n",
       " 'application level gateway',\n",
       " 'application level multicast infrastructure almi',\n",
       " 'application lifecycle management',\n",
       " 'application lifecycle management alm software',\n",
       " 'application monitoring',\n",
       " 'application note',\n",
       " 'application packaging',\n",
       " 'application performance management',\n",
       " 'application planning',\n",
       " 'application portfolio management',\n",
       " 'application programming interface api',\n",
       " 'application release automation',\n",
       " 'application remediation',\n",
       " 'application restart',\n",
       " 'application retirement',\n",
       " 'application security',\n",
       " 'application security testing',\n",
       " 'application server',\n",
       " 'application service',\n",
       " 'application setting',\n",
       " 'application specific instruction set processor',\n",
       " 'application specific integrated circuit',\n",
       " 'application streaming',\n",
       " 'application virtualization',\n",
       " 'application xml',\n",
       " 'application level gateway',\n",
       " 'application specific information',\n",
       " 'application architecture',\n",
       " 'application artificial intelligence',\n",
       " 'applied behavior analysis',\n",
       " 'applied business technology',\n",
       " 'applied science',\n",
       " 'applied statistic',\n",
       " 'aqua data studio',\n",
       " 'arangodb',\n",
       " 'architect engineer contract administration support system',\n",
       " 'architecture flow diagram',\n",
       " 'argo cd',\n",
       " 'artificial general intelligence',\n",
       " 'artificial intelligence',\n",
       " 'artificial intelligence development',\n",
       " 'artificial intelligence markup language aiml',\n",
       " 'artificial intelligence system',\n",
       " 'artificial neural network',\n",
       " 'assembly language',\n",
       " 'assessment basic language learning skill',\n",
       " 'association management',\n",
       " 'association rule learning',\n",
       " 'asynchronous javascript',\n",
       " 'asynchronous javascript xml ajax',\n",
       " 'asynchronous learning',\n",
       " 'asynchronous module definition',\n",
       " 'asynchronous serial communication',\n",
       " 'asynchronous transfer mode atm',\n",
       " 'attention mechanism',\n",
       " 'autocad',\n",
       " 'autocad architecture',\n",
       " 'autocad civil',\n",
       " 'autocad dxf',\n",
       " 'autocad plant',\n",
       " 'autoencoders',\n",
       " 'automated code review',\n",
       " 'automated machine learning',\n",
       " 'automatic data processing equipment',\n",
       " 'automatic data processing software adp',\n",
       " 'autoregressive integrated moving average arima',\n",
       " 'autoregressive model',\n",
       " 'average cost method',\n",
       " 'azure api apps',\n",
       " 'azure api management',\n",
       " 'azure active directory',\n",
       " 'azure application insight',\n",
       " 'azure automation',\n",
       " 'azure batch',\n",
       " 'azure blob storage',\n",
       " 'azure blueprint',\n",
       " 'azure cloud service',\n",
       " 'azure cognitive service',\n",
       " 'azure command line interface azure cli',\n",
       " 'azure content delivery network azure cdn',\n",
       " 'azure cosmos db',\n",
       " 'azure cost management',\n",
       " 'azure data catalog',\n",
       " 'azure data explorer kusto',\n",
       " 'azure data factory',\n",
       " 'azure data lake',\n",
       " 'azure databricks',\n",
       " 'azure devops',\n",
       " 'azure firewall',\n",
       " 'azure internet thing iot',\n",
       " 'azure kubernetes service',\n",
       " 'azure load balancer',\n",
       " 'azure logic apps',\n",
       " 'azure mfa',\n",
       " 'azure machine learning',\n",
       " 'azure monitor',\n",
       " 'azure pipeline',\n",
       " 'azure policy',\n",
       " 'azure security',\n",
       " 'azure sentinel',\n",
       " 'azure service bus',\n",
       " 'azure service fabric',\n",
       " 'azure web apps',\n",
       " 'b tree file system btrfs',\n",
       " 'bert nlp model',\n",
       " 'bgfs algorithm',\n",
       " 'end software engineering',\n",
       " 'backbone j javascript library',\n",
       " 'backpropagation',\n",
       " 'batch message processing',\n",
       " 'batch processing',\n",
       " 'batch production',\n",
       " 'bayes classifier',\n",
       " 'bayes estimator',\n",
       " 'bayesian inference',\n",
       " 'bayesian modeling',\n",
       " 'bayesian network',\n",
       " 'bayesian probability',\n",
       " 'bayesian statistic',\n",
       " 'beautifulsoup',\n",
       " 'behavioral analytics',\n",
       " 'behavioral segmentation',\n",
       " 'big data',\n",
       " 'big data analytics',\n",
       " 'big ip',\n",
       " 'big notation',\n",
       " 'bigmachines query language bmql',\n",
       " 'bigquery',\n",
       " 'bigtable',\n",
       " 'binary search algorithm',\n",
       " 'binary search tree',\n",
       " 'binary space partitioning',\n",
       " 'binary system',\n",
       " 'binary tree',\n",
       " 'bit ly',\n",
       " 'bitbucket',\n",
       " 'blockchain',\n",
       " 'blockchain indexing',\n",
       " 'blockchain security',\n",
       " 'bokeh',\n",
       " 'boltzmann machine',\n",
       " 'boolean expression',\n",
       " 'boolean network',\n",
       " 'boolean search',\n",
       " 'boost c library',\n",
       " 'bootstrap end framework',\n",
       " 'bootstrap protocol',\n",
       " 'bootstrapping',\n",
       " 'build automation',\n",
       " 'build event',\n",
       " 'build management',\n",
       " 'build pipeline',\n",
       " 'build process',\n",
       " 'build time',\n",
       " 'build tool',\n",
       " 'build v buy analysis',\n",
       " 'building information modeling',\n",
       " 'business analysis',\n",
       " 'business analysis body knowledge babok',\n",
       " 'business analytics',\n",
       " 'business architecture',\n",
       " 'business case analysis',\n",
       " 'business communication',\n",
       " 'business computer system',\n",
       " 'business development',\n",
       " 'business integration software',\n",
       " 'business intelligence',\n",
       " 'business intelligence architecture',\n",
       " 'business intelligence data modeling',\n",
       " 'business intelligence development',\n",
       " 'business intelligence development studio',\n",
       " 'business intelligence reporting',\n",
       " 'business intelligence testing',\n",
       " 'business intelligence tool',\n",
       " 'business performance management',\n",
       " 'business reporting',\n",
       " 'business requirement',\n",
       " 'business requirement documentation',\n",
       " 'c programming language',\n",
       " 'c compiler',\n",
       " 'c data type c programming language',\n",
       " 'c dynamic memory allocation',\n",
       " 'c file input output',\n",
       " 'c graphic',\n",
       " 'c mathematical function c standard library',\n",
       " 'c preprocessor',\n",
       " 'c sharp software',\n",
       " 'c sharp syntax',\n",
       " 'c shell',\n",
       " 'c standard library',\n",
       " 'c programming language',\n",
       " 'c fundamental',\n",
       " 'c programming language',\n",
       " 'c concept',\n",
       " 'c fundamental',\n",
       " 'c module',\n",
       " 'c server page',\n",
       " 'c cli',\n",
       " 'c',\n",
       " 'c based programming language',\n",
       " 'c treeace',\n",
       " 'c c standard library',\n",
       " 'c family',\n",
       " 'c j javascript library',\n",
       " 'cgi scripting',\n",
       " 'chi squared automatic interaction detection chaid',\n",
       " 'ci cd',\n",
       " 'cpython python package',\n",
       " 'cs animation',\n",
       " 'cs code',\n",
       " 'cs flex box layout',\n",
       " 'cs framework',\n",
       " 'cs grid',\n",
       " 'calculus',\n",
       " 'candidate key',\n",
       " 'canva software',\n",
       " 'cascading style sheet cs',\n",
       " 'chart j javascript library',\n",
       " 'chatgpt',\n",
       " 'chatbot',\n",
       " 'chi squared test',\n",
       " 'class diagram',\n",
       " 'class hierarchy',\n",
       " 'client server application language c al',\n",
       " 'cloud application',\n",
       " 'cloud automation',\n",
       " 'cloud collaboration',\n",
       " 'cloud computing',\n",
       " 'cloud computing architecture',\n",
       " 'cloud data management interface',\n",
       " 'cloud database',\n",
       " 'cloud development',\n",
       " 'cloud engineering',\n",
       " 'cloud infrastructure',\n",
       " 'cloud infrastructure management interface cimi',\n",
       " 'cloud management',\n",
       " 'cloud management platform',\n",
       " 'cloud migration',\n",
       " 'cloud operation',\n",
       " 'cloud penetration testing',\n",
       " 'cloud platform system',\n",
       " 'cloud storage',\n",
       " 'cloud strategy',\n",
       " 'cloud technology',\n",
       " 'cluster analysis',\n",
       " 'code editor',\n",
       " 'code enforcement',\n",
       " 'code formatting',\n",
       " 'code generation',\n",
       " 'code injection',\n",
       " 'code insight',\n",
       " 'code inspection',\n",
       " 'code migration',\n",
       " 'code federal regulation',\n",
       " 'code project open licensing',\n",
       " 'code refactoring',\n",
       " 'code reuse',\n",
       " 'code review',\n",
       " 'code sharing',\n",
       " 'code signing',\n",
       " 'code snippet',\n",
       " 'code structure',\n",
       " 'code testing',\n",
       " 'codebase',\n",
       " 'cognitive processing',\n",
       " 'collaborative filtering',\n",
       " 'command data handling',\n",
       " 'common gateway interface',\n",
       " 'compiler theory',\n",
       " 'compiler',\n",
       " 'computational design',\n",
       " 'computational intelligence',\n",
       " 'computational science engineering',\n",
       " 'computer data storage',\n",
       " 'computer design',\n",
       " 'computer display',\n",
       " 'computer engineering',\n",
       " 'computer programming',\n",
       " 'computer science',\n",
       " 'conditional expression',\n",
       " 'confusion matrix',\n",
       " 'continuous integration',\n",
       " 'conversational ai',\n",
       " 'convex optimization',\n",
       " 'convolutional neural network',\n",
       " 'couchdb',\n",
       " 'counterintelligence',\n",
       " 'course catalog',\n",
       " 'course development',\n",
       " 'create react app',\n",
       " 'cross functional integration',\n",
       " 'cross functional project management',\n",
       " 'cross functional team leadership',\n",
       " 'crypto mining',\n",
       " 'crypto',\n",
       " 'cryptocurrency',\n",
       " 'customer analysis',\n",
       " 'customer analytics',\n",
       " 'customer data integration',\n",
       " 'customer data management',\n",
       " 'cyber security management',\n",
       " 'cython',\n",
       " 'j javascript library',\n",
       " 'databus programming language',\n",
       " 'db sql',\n",
       " 'dfsm',\n",
       " 'dashboard',\n",
       " 'data abstraction',\n",
       " 'data access',\n",
       " 'data access object dao pattern',\n",
       " 'data acquisition',\n",
       " 'data administration',\n",
       " 'data analysis',\n",
       " 'data analysis display dadisp',\n",
       " 'data analysis expression dax',\n",
       " 'data annotation',\n",
       " 'data architecture',\n",
       " 'data archive',\n",
       " 'data archiving service',\n",
       " 'data service daas',\n",
       " 'data auditing',\n",
       " 'data base query language',\n",
       " 'data binding',\n",
       " 'data blending',\n",
       " 'data buffer',\n",
       " 'data build tool',\n",
       " 'data cabling',\n",
       " 'data caching',\n",
       " 'data capture',\n",
       " 'data center bridging',\n",
       " 'data center hardware',\n",
       " 'data center infrastructure efficiency',\n",
       " 'data center infrastructure management cim',\n",
       " 'data center unified computing system implementation dcuci',\n",
       " 'data center',\n",
       " 'data class',\n",
       " 'data classification',\n",
       " 'data cleansing',\n",
       " 'data collection',\n",
       " 'data comparison',\n",
       " 'data compression',\n",
       " 'data conditioning',\n",
       " 'data consistency',\n",
       " 'data control',\n",
       " 'data control language',\n",
       " 'data conversion',\n",
       " 'data corruption',\n",
       " 'data cube',\n",
       " 'data curation',\n",
       " 'data definition language',\n",
       " 'data definition specification',\n",
       " 'data dictionary',\n",
       " 'data direct network',\n",
       " 'data discovery',\n",
       " 'data display debugger',\n",
       " 'data distribution service',\n",
       " 'data domain',\n",
       " 'data driven instruction',\n",
       " 'data duplication management',\n",
       " 'data element',\n",
       " 'data encoding',\n",
       " 'data encryption',\n",
       " 'data encryption standard',\n",
       " 'data engineering',\n",
       " 'data engineering scripting language',\n",
       " 'data entry',\n",
       " 'data erasure',\n",
       " 'data ethic',\n",
       " 'data exchange',\n",
       " 'data exploitation',\n",
       " 'data explorer',\n",
       " 'data extraction',\n",
       " 'data facility data set service',\n",
       " 'data facility storage management',\n",
       " 'data farming',\n",
       " 'data feed',\n",
       " 'data file',\n",
       " 'data flow diagram',\n",
       " 'data format description language',\n",
       " 'data frame',\n",
       " 'data fusion',\n",
       " 'data general aviion computer',\n",
       " 'data governance',\n",
       " 'data grid',\n",
       " 'data hiding encapsulation',\n",
       " 'data highway plus',\n",
       " 'data hub',\n",
       " 'data import export',\n",
       " 'data infrastructure',\n",
       " 'data ingestion',\n",
       " 'data integration',\n",
       " 'data integrity',\n",
       " 'data intelligence',\n",
       " 'data interface',\n",
       " 'data item description',\n",
       " 'data lake',\n",
       " 'data language interface',\n",
       " 'data layer',\n",
       " 'data library',\n",
       " 'data link',\n",
       " 'data link connection identifier',\n",
       " 'data link control',\n",
       " 'data link layer',\n",
       " 'data literacy',\n",
       " 'data localization',\n",
       " 'data loss prevention',\n",
       " 'data maintenance',\n",
       " 'data management',\n",
       " 'data management plan',\n",
       " 'data management platform',\n",
       " 'data manipulation',\n",
       " 'data manipulation language',\n",
       " 'data mapper pattern',\n",
       " 'data mapping',\n",
       " 'data mart',\n",
       " 'data masking',\n",
       " 'data migration',\n",
       " 'data mining',\n",
       " 'data mining method',\n",
       " 'data mining query language dmql',\n",
       " 'data modeling',\n",
       " 'data monetization',\n",
       " 'data normalization',\n",
       " 'data ontap server appliance',\n",
       " 'data palette',\n",
       " 'data partitioning',\n",
       " 'data pipeline management',\n",
       " 'data pipeline',\n",
       " 'data plane development kit dpdk',\n",
       " 'data policy development',\n",
       " 'data preprocessing',\n",
       " 'data presentation',\n",
       " 'data privacy law',\n",
       " 'data processing',\n",
       " 'data processing system',\n",
       " 'data processing unit',\n",
       " 'data profiling',\n",
       " 'data protection planning',\n",
       " 'data protection strategy',\n",
       " 'data quality',\n",
       " 'data quality assessment',\n",
       " 'data radio channel',\n",
       " 'data recording',\n",
       " 'data recovery',\n",
       " 'data recovery software',\n",
       " 'data reduction',\n",
       " 'data redundancy',\n",
       " 'data reference model',\n",
       " 'data remanence',\n",
       " 'data retention',\n",
       " 'data retrieval',\n",
       " 'data room',\n",
       " 'data science',\n",
       " 'data scraping',\n",
       " 'data security',\n",
       " 'data selection',\n",
       " 'data server interface',\n",
       " 'data sharing',\n",
       " 'data smoothing',\n",
       " 'data storage',\n",
       " 'data storage device',\n",
       " 'data store',\n",
       " 'data storytelling',\n",
       " 'data strategy',\n",
       " 'data stream management system',\n",
       " 'data streaming',\n",
       " 'data striping',\n",
       " 'data structure alignment',\n",
       " 'data structure',\n",
       " 'data synchronization',\n",
       " 'data synthesis',\n",
       " 'data system',\n",
       " 'data targeting',\n",
       " 'data taxonomy',\n",
       " 'data terminal equipment',\n",
       " 'data transfer object',\n",
       " 'data transformation',\n",
       " 'data transformation service',\n",
       " 'data transmission',\n",
       " 'data transport utility',\n",
       " 'data transposition',\n",
       " 'data validation',\n",
       " 'data vault',\n",
       " 'data verification',\n",
       " 'data virtualization',\n",
       " 'data visualization',\n",
       " 'data warehouse appliance',\n",
       " 'data warehouse architecture',\n",
       " 'data warehouse system',\n",
       " 'data warehousing',\n",
       " 'data warehousing business intelligence dwbi',\n",
       " 'data wrangling',\n",
       " 'data centric testing',\n",
       " 'data driven decision making',\n",
       " 'data driven manufacturing',\n",
       " 'data driven testing',\n",
       " 'data flow analysis',\n",
       " 'data link switching',\n",
       " 'data structured language',\n",
       " 'dataadapters ado net',\n",
       " 'database markup language',\n",
       " 'databasic',\n",
       " 'datacad',\n",
       " 'dataflex',\n",
       " 'dataflux',\n",
       " 'datahub software',\n",
       " 'datanucleus',\n",
       " 'datastax enterprise',\n",
       " 'datastax enterprise graph',\n",
       " 'datatransfer workbench sap',\n",
       " 'databags',\n",
       " 'database abstraction layer',\n",
       " 'database activity monitoring',\n",
       " 'database administration',\n",
       " 'database analysis',\n",
       " 'database application',\n",
       " 'database architecture',\n",
       " 'database service dbaas',\n",
       " 'database audit',\n",
       " 'database availability group',\n",
       " 'database cloning',\n",
       " 'database cluster',\n",
       " 'database comparison',\n",
       " 'database connection',\n",
       " 'database consistency',\n",
       " 'database console command dbcc',\n",
       " 'database consolidation',\n",
       " 'database conversion',\n",
       " 'database cursor',\n",
       " 'database deployment management',\n",
       " 'database design',\n",
       " 'database development',\n",
       " 'database diagram',\n",
       " 'database directive',\n",
       " 'database dump',\n",
       " 'database encryption',\n",
       " 'database engine tuning advisor',\n",
       " 'database engine',\n",
       " 'database',\n",
       " 'database independent',\n",
       " 'database index',\n",
       " 'database life cycle management',\n",
       " 'database management',\n",
       " 'database management system',\n",
       " 'database marketing',\n",
       " 'database mirroring',\n",
       " 'database modeling',\n",
       " 'database normalization',\n",
       " 'database partitioning',\n",
       " 'database performance analyzer',\n",
       " 'database permission',\n",
       " 'database programmer toolkits',\n",
       " 'database programming',\n",
       " 'database publishing',\n",
       " 'database query',\n",
       " 'database query tool',\n",
       " 'database reporting software',\n",
       " 'database scanner',\n",
       " 'database schema',\n",
       " 'database search engine',\n",
       " 'database security',\n",
       " 'database server',\n",
       " 'database software',\n",
       " 'database storage structure',\n",
       " 'database system',\n",
       " 'database testing',\n",
       " 'database theory',\n",
       " 'database transaction',\n",
       " 'database trigger',\n",
       " 'database tuning',\n",
       " 'database upgrade',\n",
       " 'database virtualization',\n",
       " 'databricks',\n",
       " 'datacap',\n",
       " 'datacards',\n",
       " 'datacom db',\n",
       " 'datadog',\n",
       " 'datafeed',\n",
       " 'datafield',\n",
       " 'dataflow',\n",
       " 'dataflow architecture',\n",
       " 'dataframe',\n",
       " 'datagram',\n",
       " 'datagram congestion control protocol',\n",
       " 'datagram transport layer security',\n",
       " 'datakit',\n",
       " 'datalog',\n",
       " 'datamaps',\n",
       " 'datamodel',\n",
       " 'datanet',\n",
       " 'dataportability',\n",
       " 'datapump',\n",
       " 'dataset',\n",
       " 'datasheets',\n",
       " 'dataspaces',\n",
       " 'datastax',\n",
       " 'datatable',\n",
       " 'dataweave',\n",
       " 'datawindow',\n",
       " 'date manipulation',\n",
       " 'datediff',\n",
       " 'datomic',\n",
       " 'daughterboard',\n",
       " 'dbvisualizer',\n",
       " 'dbeaver',\n",
       " 'dbscan',\n",
       " 'dc j javascript library',\n",
       " 'decision model',\n",
       " 'decision science',\n",
       " 'decision support operation maintenance',\n",
       " 'decision support system',\n",
       " 'decision table',\n",
       " 'decision theory',\n",
       " 'decision tree learning',\n",
       " 'decision matrix method',\n",
       " 'deep learning',\n",
       " 'deep learning method',\n",
       " 'descriptive statistic',\n",
       " 'dev testing',\n",
       " 'dev c',\n",
       " 'devexpress',\n",
       " 'devops',\n",
       " 'differential calculus',\n",
       " 'digital data',\n",
       " 'digital data communication message protocol',\n",
       " 'digital data storage',\n",
       " 'digital data system',\n",
       " 'dimension table',\n",
       " 'dimensionality reduction',\n",
       " 'disk partitioning',\n",
       " 'distance learning',\n",
       " 'distributed computing',\n",
       " 'distributed database',\n",
       " 'distributed file system',\n",
       " 'distributed programming',\n",
       " 'docker engine',\n",
       " 'dot product',\n",
       " 'dropbox api',\n",
       " 'dust j javascript library',\n",
       " 'dynamic application security testing dast',\n",
       " 'dynamic data',\n",
       " 'dynamic program analysis',\n",
       " 'dynamic programming',\n",
       " 'ecmascript c programming language family',\n",
       " 'emc cloud computing',\n",
       " 'exec scripting language',\n",
       " 'economic policy analysis',\n",
       " 'edge computing',\n",
       " 'educational data mining',\n",
       " 'eigen c library',\n",
       " 'elasticity computing',\n",
       " 'electronic system level design verification',\n",
       " 'elixir programming language',\n",
       " 'eltron programming language',\n",
       " 'email processing',\n",
       " 'embedded c',\n",
       " 'embedded c',\n",
       " 'embedded code',\n",
       " 'embedded database',\n",
       " 'embedded domain specific language',\n",
       " 'embedded firmware',\n",
       " 'embedded http server',\n",
       " 'embedded intelligence',\n",
       " 'embedded java',\n",
       " 'embedded operating system',\n",
       " 'embedded sql',\n",
       " 'embedded software',\n",
       " 'embedded system',\n",
       " 'ember j javascript library',\n",
       " 'emulator high level language application program interface ehllapi',\n",
       " 'encrypted key exchange',\n",
       " 'encrypting file system',\n",
       " 'encryption',\n",
       " 'encryption software',\n",
       " 'environmental data analysis',\n",
       " 'environmental data management',\n",
       " 'enzyme javascript testing utility',\n",
       " 'error analysis numerical analysis',\n",
       " 'espresso android testing framework',\n",
       " 'espresso java',\n",
       " 'evolutionary programming',\n",
       " 'exception handling',\n",
       " 'expense forecasting',\n",
       " 'experience api xapi',\n",
       " 'express j javascript library',\n",
       " 'ext j',\n",
       " 'ext net',\n",
       " 'extendscript',\n",
       " 'extended file system',\n",
       " 'extract transform load etl',\n",
       " 'f programming language',\n",
       " 'foil programming language',\n",
       " 'facebook api',\n",
       " 'facebook analytics',\n",
       " 'facebook graph api',\n",
       " 'facebook query language',\n",
       " 'factset analytics software',\n",
       " 'famo javascript framework',\n",
       " 'feature engineering',\n",
       " 'feature extraction',\n",
       " 'file handling',\n",
       " 'file transfer protocol ftp',\n",
       " 'financial data management',\n",
       " 'financial forecasting',\n",
       " 'financial information exchange fix protocol',\n",
       " 'flask web framework',\n",
       " 'flux react j',\n",
       " 'foglight database software',\n",
       " 'forth programming language',\n",
       " 'fortran programming language',\n",
       " 'framer j javascript library',\n",
       " 'frisby j javascript library',\n",
       " 'end software engineering',\n",
       " 'end design',\n",
       " 'end engineering',\n",
       " 'g programming language',\n",
       " 'g code',\n",
       " 'g standard',\n",
       " 'g standard',\n",
       " 'g standard',\n",
       " 'g standard',\n",
       " 'g standard',\n",
       " 'g standard',\n",
       " 'g standard',\n",
       " 'g protocol',\n",
       " 'g standard',\n",
       " 'g standard',\n",
       " 'g protocol',\n",
       " 'g standard',\n",
       " 'g standard',\n",
       " 'gnu c library',\n",
       " 'gpu optimization',\n",
       " 'gatsby j javascript library',\n",
       " 'generic java',\n",
       " 'generic java',\n",
       " 'git version control system',\n",
       " ...]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills[\"skills\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lem_stop(df,column_name):\n",
    "    df[column_name] = df[column_name].apply(preprocess_text_spacy)\n",
    "    df[column_name] = df[column_name].apply(clean_text)\n",
    "    return df\n",
    "df_j = df_j[:100]\n",
    "df_c = df_c[:100]\n",
    "df_j = clean_lem_stop(df_j,\"title\")\n",
    "df_j = clean_lem_stop(df_j,\"jobDescription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct hire opportunity client hybrid role located minneapolis mn candidate able work sponsorship seeking highly motivated detail oriented data engineer join dynamic team ideal candidate responsible developing maintaining data solution drive business objective forward passion working data technical background desire contribute project leverage advanced analytics role excellent opportunity meaningful impact work collaboratively team create innovative data driven solution focus data engineering cloud technology data analytics product advanced analytics ready career level difference data driven world encourage apply responsibility data solution development design develop implement data solution address business need leveraging understanding data modeling sql proficiency coding skill language python c javascript continuous improvement identify seize opportunity enhance existing data solution optimizing performance scalability efficiency data visualization collaborate cross functional team create engaging data visualization provide actionable insight stakeholder data engineering cloud keen interest data engineering cloud technology actively participating related project support data infrastructure architecture data analytics product contribute development maintenance data analytics product ensuring meet highest quality standard satisfy business requirement advanced analytics work project related advanced analytics including predictive modeling machine learning statistical analysis help drive informed decision making collaborative teamwork act effective team member participating collaborative project sharing knowledge supporting colleague achieving project goal project documentation assist creation high quality project documentation project artifact ensuring aspect solution documented future reference audit qualification bachelor degree technical field equivalent practical experience ability understand implement solution business need effectively bridging gap technical knowledge business objective excellent written oral communication skill convey complex technical concept non technical stakeholder exceptional attention detail note taking skill precise problem solving documentation proficiency sql data manipulation analysis experience coding language like python c javascript data driven mindset strong analytical orientation demonstrated interest data visualization data engineering cloud technology data analytics product advanced analytics willingness contribute collaborative team member project fostering culture knowledge sharing innovation vetting process emergent software work hard find data engineer right fit client step vetting process position application minute online assessment minute initial phone interview minute interview client job offer job type time pay year benefit k k matching dental insurance health insurance paid time vision insurance compensation package bonus opportunity schedule hour shift monday friday work location person'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions = df_j[\"jobDescription\"].tolist()\n",
    "skills_list = skills[\"skills\"].tolist()\n",
    "\n",
    "df_c[\"full_description\"] = df_c[\"description\"]+df_c[\"what_you_will_learn_data\"]+df_c[\"objectives\"]\n",
    "df_j = clean_lem_stop(df_c,\"full_description\")\n",
    "course_description = df_c[\"full_description\"].tolist()\n",
    "\n",
    "job_descriptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1518"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data partitioning', 0.36450003161067684), ('data entry', 0.36450003161067684), ('data transmission', 0.36450003161067684), ('data transport utility', 0.36450003161067684), ('data transposition', 0.36450003161067684), ('data vault', 0.36450003161067684), ('data monetization', 0.36450003161067684), ('data masking', 0.36450003161067684), ('data localization', 0.36450003161067684), ('data hiding encapsulation', 0.36450003161067684), ('data grid', 0.36450003161067684), ('data farming', 0.36450003161067684), ('data explorer', 0.36450003161067684), ('data ethic', 0.36450003161067684), ('data erasure', 0.36450003161067684), ('data encryption', 0.36450003161067684), ('data synchronization', 0.36450003161067684), ('data encoding', 0.36450003161067684), ('data dictionary', 0.36450003161067684), ('data corruption', 0.36450003161067684), ('data conditioning', 0.36450003161067684), ('data compression', 0.36450003161067684), ('data comparison', 0.36450003161067684), ('data caching', 0.36450003161067684), ('data buffer', 0.36450003161067684), ('data blending', 0.36450003161067684), ('data binding', 0.36450003161067684), ('data archive', 0.36450003161067684), ('data annotation', 0.36450003161067684), ('data palette', 0.36450003161067684), ('data synthesis', 0.36450003161067684), ('data loss prevention', 0.36450003161067684), ('data remanence', 0.36450003161067684), ('data preprocessing', 0.36450003161067684), ('data smoothing', 0.36450003161067684), ('data radio channel', 0.36450003161067684), ('data striping', 0.36450003161067684), ('data storytelling', 0.36450003161067684), ('data recording', 0.36450003161067684), ('data scraping', 0.36450003161067684), ('data engineering', 0.28277050192215464), ('business analytics', 0.25217903730787716), ('big data analytics', 0.2517193656424271), ('facebook analytics', 0.2412497562589917), ('analytics', 0.2412497562589917), ('data driven instruction', 0.24122379544374933), ('data visualization', 0.24117447641309825), ('data quality', 0.23301146356345487), ('data system', 0.22392535979974998), ('survey data analysis', 0.21808956636202043), ('data analysis', 0.21808956636202043), ('data modeling', 0.21295164995081872), ('application data', 0.20686399800794977), ('webspeed openedge advanced business language', 0.20184963004947215), ('data sharing', 0.2015927302365289), ('data pipeline', 0.20026387533546547), ('data architecture', 0.1926001667366875), ('data service daas', 0.190936663302684), ('data archiving service', 0.190936663302684), ('wcf data service', 0.190936663302684), ('wireless data service', 0.190936663302684), ('data driven decision making', 0.18945727241896065), ('data duplication management', 0.18935485253215015), ('data management', 0.18935485253215015), ('data infrastructure', 0.18865450679397958), ('technical data management system', 0.18505286689238465), ('data science', 0.18455896212548425), ('factset analytics software', 0.18151197689897042), ('data driven testing', 0.1812763778134398), ('business intelligence data modeling', 0.18013194884014266), ('data encryption standard', 0.17666298349343704), ('data maintenance', 0.1753156092349097), ('data item description', 0.171355017855701), ('customer analytics', 0.1695532074975728), ('data integration', 0.16794553517708793), ('dynamic data', 0.16306044166679723), ('data quality assessment', 0.1619109969733589), ('data manipulation language', 0.16073727934616794), ('data warehouse appliance', 0.15739719181615117), ('data facility data set service', 0.15411694067938897), ('data engineering scripting language', 0.1528018536222746), ('azure data explorer kusto', 0.15191006035589363), ('data processing', 0.15004377995686902), ('data intelligence', 0.15004377995686902), ('analytics j javascript library', 0.14966137871412583), ('data manipulation', 0.14960955443065244), ('data pipeline management', 0.14858108228240335), ('test data', 0.1481570862321126), ('data control language', 0.14698208583408987), ('data security', 0.14624874293162124), ('data highway plus', 0.14624874293162124), ('data build tool', 0.14611056084064125), ('data management platform', 0.14464992019645068), ('agile project', 0.1446032530104648), ('cross functional project management', 0.14442586186980297), ('technology solution', 0.14373909620661085), ('data access', 0.14236151613048983), ('data structure alignment', 0.14236151613048983), ('data structure', 0.14236151613048983), ('data policy development', 0.14230102737867012), ('c data type c programming language', 0.14144328700910122), ('big data', 0.14037945843540645), ('data class', 0.14037945843540645), ('data warehouse architecture', 0.13945849692554507), ('customer data management', 0.1387932803082149), ('data control', 0.13836935781143025), ('data warehouse system', 0.1376445451826038), ('data storage', 0.13425647689976475), ('data processing system', 0.13264542787219966), ('data reference model', 0.1325216574050718), ('collaborative filtering', 0.13245340274766798), ('business requirement documentation', 0.13174451886253793), ('cloud engineering', 0.13117749848897314), ('web analytics', 0.13115875627840032), ('data structured language', 0.130615715499714), ('data flow analysis', 0.13024918157418575), ('data transformation', 0.1300029753635813), ('data warehousing', 0.1300029753635813), ('data driven manufacturing', 0.12991543581716894), ('customer data integration', 0.12966759868843333), ('data management plan', 0.1292900970531473), ('data warehousing business intelligence dwbi', 0.1284914870406355), ('cloud data management interface', 0.12823672535396238), ('data strategy', 0.1278157785136448), ('data ontap server appliance', 0.1278157785136448), ('data mining', 0.1278157785136448), ('data lake', 0.1278157785136448), ('agile project management', 0.1262429762197186), ('data center bridging', 0.1254580399468139), ('test data management', 0.12316596459042255), ('data language interface', 0.12166689429449376), ('business analysis body knowledge babok', 0.1215203472029662), ('data direct network', 0.11983491049733044), ('cloud technology', 0.11799555357309417), ('data mining query language dmql', 0.11620401880637929), ('data base query language', 0.11620401880637929), ('asynchronous javascript', 0.11613460189283696), ('data integrity', 0.11609840342280327), ('data extraction', 0.11609840342280327), ('data center', 0.11609840342280327), ('data center infrastructure efficiency', 0.11591293911628751), ('visualization', 0.11500331674091861), ('visualization', 0.11500331674091861), ('visualization toolkit vtk', 0.11500331674091861), ('digital data', 0.1135544748475288), ('data acquisition', 0.1135544748475288), ('data transformation service', 0.11245762444060062), ('computer data storage', 0.11215397417677837), ('code project open licensing', 0.11098184337719269), ('data terminal equipment', 0.1109228719299658), ('data streaming', 0.1109228719299658), ('financial data management', 0.1092585696969713), ('data ingestion', 0.10819055325223062), ('data domain', 0.10819055325223062), ('data store', 0.10819055325223062), ('data format description language', 0.10818397448110695), ('cloud development', 0.10601934038992238), ('data definition language', 0.10600826398180097), ('digital data system', 0.10542790505272528), ('aqua data studio', 0.10534115046916871), ('data governance', 0.10534115046916871), ('data interface', 0.10534115046916871), ('data center infrastructure management cim', 0.10456592694777898), ('technical analysis', 0.10350012018965746), ('business analysis', 0.10273333888700417), ('azure data lake', 0.1015250221065846), ('data processing unit', 0.10096219561339496), ('xml data package', 0.10016817548127634), ('enzyme javascript testing utility', 0.10011424451121806), ('data administration', 0.09920057431406903), ('data file', 0.09920057431406903), ('technical analysis software', 0.09855113885718658), ('software business analysis', 0.09790501747391481), ('data general aviion computer', 0.09745242438842815), ('business development', 0.09657150028458218), ('xml data binding', 0.0958444136672082), ('data presentation', 0.0958444136672082), ('data classification', 0.0958444136672082), ('data validation', 0.0958444136672082), ('data migration', 0.0958444136672082), ('data feed', 0.0958444136672082), ('data consistency', 0.0958444136672082), ('test data generation', 0.0957099443488007), ('famo javascript framework', 0.09457536892111915), ('data room', 0.09223242214036861), ('data exchange', 0.09223242214036861), ('data profiling', 0.09223242214036861), ('data cleansing', 0.09223242214036861), ('cloud application', 0.09028131063666543), ('data analysis display dadisp', 0.08953351031215058), ('environmental data analysis', 0.08953351031215058), ('data mining method', 0.08950724574278432), ('algebraic modeling language', 0.08926676500938849), ('digital data storage', 0.08926289660866854), ('data recovery software', 0.08905414545158567), ('cloud infrastructure', 0.08900224136922459), ('data stream management system', 0.0883078732308015), ('data conversion', 0.08828619206565766), ('data element', 0.08828619206565766), ('data collection', 0.08828619206565766)]\n"
     ]
    }
   ],
   "source": [
    "# parse job description to identify required skills from skill list \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Let's say you have your job descriptions, course descriptions, and skills\n",
    "job_descriptions = df_j[\"jobDescription\"].tolist() # List of job description texts\n",
    "#course_descriptions = filtered_keywords_c # List of course description texts\n",
    "skills_list = skills[\"skills\"].tolist()\n",
    "  # List of skills\n",
    "# Combine job descriptions and course descriptions\n",
    "all_descriptions = job_descriptions \n",
    "\n",
    "# Vectorize the descriptions and skills\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "all_descriptions_tfidf = tfidf_vectorizer.fit_transform(all_descriptions)\n",
    "\n",
    "# Transform skills with the same vectorizer\n",
    "skills_tfidf = tfidf_vectorizer.transform(skills_list)\n",
    "\n",
    "# Calculate cosine similarity and rank skills\n",
    "def rank_skills_for_description(description_tfidf, skills_tfidf):\n",
    "    cosine_similarities = cosine_similarity(description_tfidf, skills_tfidf).flatten()\n",
    "    \n",
    "    # Get the top matching skills indices and scores\n",
    "    top_skills_indices = cosine_similarities.argsort()[::-1]\n",
    "    \n",
    "    # Map indices to skill names and scores\n",
    "    top_skills = [(skills_list[i], cosine_similarities[i]) for i in top_skills_indices]\n",
    "\n",
    "    return top_skills\n",
    "\n",
    "# Example: Get top skills for the first job description\n",
    "top_skills_for_first_job = rank_skills_for_description(all_descriptions_tfidf[1], skills_tfidf)\n",
    "\n",
    "# Now, to avoid printing out all skills, we slice the list to get the top N\n",
    "N = 100\n",
    "print(top_skills_for_first_job[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data warehousing business intelligence dwbi', 0.538889), ('architect engineer contract administration support system', 0.52003807), ('data warehouse appliance', 0.4981823), ('tableau business intelligence software', 0.4980299), ('emulator high level language application program interface ehllapi', 0.49613374), ('visualization toolkit vtk', 0.4955115), ('data ontap server appliance', 0.49481046), ('decision support operation maintenance', 0.49081674), ('business analysis body knowledge babok', 0.48497632), ('cross functional project management', 0.48326913), ('amazon simple workflow service swf', 0.48174405), ('cloud infrastructure management interface cimi', 0.4807723), ('application lifecycle management alm software', 0.47936153), ('data center infrastructure management cim', 0.47277617), ('webspeed openedge advanced business language', 0.47124884), ('data analysis display dadisp', 0.4686407), ('user interface ui design', 0.46846822), ('data access object dao pattern', 0.46842873), ('structured system analysis design method', 0.46824065), ('artificial intelligence markup language aiml', 0.46707505), ('data hiding encapsulation', 0.46556944), ('asynchronous javascript xml ajax', 0.464109), ('extract transform load etl', 0.46315092), ('structured analysis design technique', 0.4624834), ('structured query language procedural language sql pl', 0.4618765), ('datastax enterprise graph', 0.46077412), ('data center unified computing system implementation dcuci', 0.45879257), ('ada programming support environment apse', 0.45716482), ('data link connection identifier', 0.45680422), ('azure command line interface azure cli', 0.45364252), ('data mining query language dmql', 0.45340714), ('dynamic application security testing dast', 0.4518863), ('financial information exchange fix protocol', 0.45086512), ('embedded domain specific language', 0.4500005), ('aws key management service km', 0.44933844), ('datatransfer workbench sap', 0.44835037), ('chi squared automatic interaction detection chaid', 0.44800225), ('client server application language c al', 0.44799843), ('autoregressive integrated moving average arima', 0.4478711), ('cross functional team leadership', 0.44758472), ('espresso android testing framework', 0.44702524), ('bigmachines query language bmql', 0.4469419), ('xml transformation language xml based standard', 0.44586134), ('analytics j javascript library', 0.4455509), ('spacy nlp software', 0.44541848), ('b tree file system btrfs', 0.4451228), ('ad hoc analysis', 0.44349283), ('workflow technology', 0.44337684), ('elixir programming language', 0.44319326), ('ecmascript c programming language family', 0.4428409), ('azure content delivery network azure cdn', 0.44210678), ('application development system online adso', 0.44069478), ('statistical static timing analysis ssta', 0.4406427), ('business intelligence development studio', 0.44046313), ('software release life cycle', 0.4402688), ('databus programming language', 0.43904), ('aws cloud development kit cdk', 0.4389171), ('technical analysis software', 0.43881), ('adaboost adaptive boosting', 0.43875855), ('unit test driven development', 0.43854228), ('troubleshooting problem solving', 0.43750793), ('technology strategy development', 0.43750584), ('automatic data processing software adp', 0.43749404), ('data base query language', 0.43745166), ('flask web framework', 0.43715435), ('amazon managed streaming apache kafka amazon msk', 0.4370985), ('application level multicast infrastructure almi', 0.43563098), ('data archiving service', 0.43559468), ('amazon quantum ledger database qldb', 0.43373165), ('assessment basic language learning skill', 0.43270475), ('code refactoring', 0.43158263), ('data plane development kit dpdk', 0.43037358), ('data warehousing', 0.43026456), ('azure internet thing iot', 0.4295362), ('data wrangling', 0.42925876), ('bayes estimator', 0.4284936), ('database deployment management', 0.428199), ('aws identity access management iam', 0.42804155), ('data centric testing', 0.42722598), ('chart j javascript library', 0.42677662), ('data facility data set service', 0.42505342), ('data stream management system', 0.4246053), ('famo javascript framework', 0.424059), ('aws elastic mapreduce emr', 0.4238652), ('web ui design', 0.42318666), ('standard generalized markup language', 0.42218694), ('test driven development tdd', 0.42199856), ('aws internet thing iot', 0.42129916), ('cpython python package', 0.42086393), ('dataadapters ado net', 0.42083746), ('decision support system', 0.41975918), ('asynchronous learning', 0.41904357), ('asynchronous javascript', 0.41890728), ('business requirement documentation', 0.4183322), ('express j javascript library', 0.4181161), ('enzyme javascript testing utility', 0.41775554), ('uniface programming language', 0.41686505), ('cloud data management interface', 0.41599768), ('data management platform', 0.41576064), ('azure cost management', 0.41533673)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(texts):\n",
    "    # Tokenize and encode sequences in the list of textual data\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform mean pooling to get sentence-level embeddings\n",
    "    input_mask_expanded = encoded_input['attention_mask'].unsqueeze(-1).expand(model_output.last_hidden_state.size()).float()\n",
    "    sum_embeddings = torch.sum(model_output.last_hidden_state * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# Let's say you have your job descriptions, course descriptions, and skills in DataFrames\n",
    "\n",
    "# Combine job descriptions and course descriptions\n",
    "all_descriptions = job_descriptions \n",
    "\n",
    "# Get BERT embeddings for descriptions and skills\n",
    "all_descriptions_embeddings = get_bert_embeddings(all_descriptions)\n",
    "skills_embeddings = get_bert_embeddings(skills_list)\n",
    "\n",
    "# Convert embeddings to numpy arrays for cosine similarity\n",
    "all_descriptions_embeddings_np = all_descriptions_embeddings.numpy()\n",
    "skills_embeddings_np = skills_embeddings.numpy()\n",
    "\n",
    "# Calculate cosine similarity and rank skills\n",
    "def rank_skills_for_description(description_embedding, skills_embeddings):\n",
    "    cosine_similarities = cosine_similarity([description_embedding], skills_embeddings).flatten()\n",
    "    \n",
    "    # Get the top matching skills indices and scores\n",
    "    top_skills_indices = cosine_similarities.argsort()[::-1]\n",
    "    \n",
    "    # Map indices to skill names and scores\n",
    "    top_skills = [(skills_list[i], cosine_similarities[i]) for i in top_skills_indices]\n",
    "\n",
    "    return top_skills\n",
    "\n",
    "# Example: Get top skills for the first job description\n",
    "top_skills_for_first_job = rank_skills_for_description(all_descriptions_embeddings_np[1], skills_embeddings_np)\n",
    "\n",
    "# Now, to avoid printing out all skills, we slice the list to get the top N\n",
    "N = 100\n",
    "print(top_skills_for_first_job[:N])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct hire opportunity client hybrid role located minneapolis mn candidate able work sponsorship seeking highly motivated detail oriented data engineer join dynamic team ideal candidate responsible developing maintaining data solution drive business objective forward passion working data technical background desire contribute project leverage advanced analytics role excellent opportunity meaningful impact work collaboratively team create innovative data driven solution focus data engineering cloud technology data analytics product advanced analytics ready career level difference data driven world encourage apply responsibility data solution development design develop implement data solution address business need leveraging understanding data modeling sql proficiency coding skill language python c javascript continuous improvement identify seize opportunity enhance existing data solution optimizing performance scalability efficiency data visualization collaborate cross functional team create engaging data visualization provide actionable insight stakeholder data engineering cloud keen interest data engineering cloud technology actively participating related project support data infrastructure architecture data analytics product contribute development maintenance data analytics product ensuring meet highest quality standard satisfy business requirement advanced analytics work project related advanced analytics including predictive modeling machine learning statistical analysis help drive informed decision making collaborative teamwork act effective team member participating collaborative project sharing knowledge supporting colleague achieving project goal project documentation assist creation high quality project documentation project artifact ensuring aspect solution documented future reference audit qualification bachelor degree technical field equivalent practical experience ability understand implement solution business need effectively bridging gap technical knowledge business objective excellent written oral communication skill convey complex technical concept non technical stakeholder exceptional attention detail note taking skill precise problem solving documentation proficiency sql data manipulation analysis experience coding language like python c javascript data driven mindset strong analytical orientation demonstrated interest data visualization data engineering cloud technology data analytics product advanced analytics willingness contribute collaborative team member project fostering culture knowledge sharing innovation vetting process emergent software work hard find data engineer right fit client step vetting process position application minute online assessment minute initial phone interview minute interview client job offer job type time pay year benefit k k matching dental insurance health insurance paid time vision insurance compensation package bonus opportunity schedule hour shift monday friday work location person'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>price_detail</th>\n",
       "      <th>is_paid</th>\n",
       "      <th>visible_instructors</th>\n",
       "      <th>locale</th>\n",
       "      <th>description</th>\n",
       "      <th>headline</th>\n",
       "      <th>created</th>\n",
       "      <th>num_subscribers</th>\n",
       "      <th>discount</th>\n",
       "      <th>discount_price</th>\n",
       "      <th>rating</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>num_quizzes</th>\n",
       "      <th>num_lectures</th>\n",
       "      <th>num_curriculum_items</th>\n",
       "      <th>features</th>\n",
       "      <th>image</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>primary_subcategory</th>\n",
       "      <th>requirements_data</th>\n",
       "      <th>what_you_will_learn_data</th>\n",
       "      <th>labels</th>\n",
       "      <th>target_audiences</th>\n",
       "      <th>estimated_content_length</th>\n",
       "      <th>content_info</th>\n",
       "      <th>instructional_level</th>\n",
       "      <th>objectives</th>\n",
       "      <th>has_certificate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>950390</td>\n",
       "      <td>Machine Learning A-Z: AI, Python &amp; R + ChatGP...</td>\n",
       "      <td>https://www.udemy.com/course/machinelearning/</td>\n",
       "      <td>{'amount': 109.99, 'currency': 'USD', 'price_s...</td>\n",
       "      <td>True</td>\n",
       "      <td>[{'title': 'Kirill Eremenko', 'name': 'Kirill'...</td>\n",
       "      <td>{'locale': 'en_US', 'title': 'English (US)', '...</td>\n",
       "      <td>&lt;p&gt;Interested in the field of Machine Learning...</td>\n",
       "      <td>Learn to create Machine Learning Algorithms in...</td>\n",
       "      <td>2016-09-05T09:54:22Z</td>\n",
       "      <td>993456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.513425</td>\n",
       "      <td>176950</td>\n",
       "      <td>39</td>\n",
       "      <td>468</td>\n",
       "      <td>507</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-c.udemycdn.com/course/750x422/9503...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['Just some high school mathematics level.']</td>\n",
       "      <td>['Master Machine Learning on Python &amp; R', 'Hav...</td>\n",
       "      <td>[{'id': 5336, 'title': 'Data Science', 'url': ...</td>\n",
       "      <td>['Anyone interested in Machine Learning.', 'St...</td>\n",
       "      <td>2559</td>\n",
       "      <td>42.5 total hours</td>\n",
       "      <td>All Levels</td>\n",
       "      <td>['Master Machine Learning on Python &amp; R', 'Hav...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>903744</td>\n",
       "      <td>Python for Data Science and Machine Learning B...</td>\n",
       "      <td>https://www.udemy.com/course/python-for-data-s...</td>\n",
       "      <td>{'amount': 99.99, 'currency': 'USD', 'price_st...</td>\n",
       "      <td>True</td>\n",
       "      <td>[{'title': 'Jose Portilla', 'name': 'Jose', 'd...</td>\n",
       "      <td>{'locale': 'en_US', 'title': 'English (US)', '...</td>\n",
       "      <td>&lt;p&gt;Are you ready to start your path to becomin...</td>\n",
       "      <td>Learn how to use NumPy, Pandas, Seaborn , Matp...</td>\n",
       "      <td>2016-07-13T05:22:58Z</td>\n",
       "      <td>674125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.588927</td>\n",
       "      <td>136746</td>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-c.udemycdn.com/course/750x422/9037...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['Some programming experience', 'Admin permiss...</td>\n",
       "      <td>['Use Python for Data Science and Machine Lear...</td>\n",
       "      <td>[{'id': 5336, 'title': 'Data Science', 'url': ...</td>\n",
       "      <td>['This course is meant for people with at leas...</td>\n",
       "      <td>1494</td>\n",
       "      <td>25 total hours</td>\n",
       "      <td>All Levels</td>\n",
       "      <td>['Use Python for Data Science and Machine Lear...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1754098</td>\n",
       "      <td>The Data Science Course: Complete Data Science...</td>\n",
       "      <td>https://www.udemy.com/course/the-data-science-...</td>\n",
       "      <td>{'amount': 109.99, 'currency': 'USD', 'price_s...</td>\n",
       "      <td>True</td>\n",
       "      <td>[{'title': '365 Careers', 'name': '365', 'disp...</td>\n",
       "      <td>{'locale': 'en_US', 'title': 'English (US)', '...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;The Problem&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Data sci...</td>\n",
       "      <td>Complete Data Science Training: Mathematics, S...</td>\n",
       "      <td>2018-06-18T12:30:08Z</td>\n",
       "      <td>634583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.594564</td>\n",
       "      <td>131509</td>\n",
       "      <td>281</td>\n",
       "      <td>518</td>\n",
       "      <td>799</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-c.udemycdn.com/course/750x422/1754...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['No prior experience is required. We will sta...</td>\n",
       "      <td>['The course provides the entire toolbox you n...</td>\n",
       "      <td>[{'id': 5336, 'title': 'Data Science', 'url': ...</td>\n",
       "      <td>['You should take this course if you want to b...</td>\n",
       "      <td>1906</td>\n",
       "      <td>32 total hours</td>\n",
       "      <td>All Levels</td>\n",
       "      <td>['The course provides the entire toolbox you n...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>765242</td>\n",
       "      <td>R Programming A-Z: R For Data Science With Re...</td>\n",
       "      <td>https://www.udemy.com/course/r-programming/</td>\n",
       "      <td>{'amount': 94.99, 'currency': 'USD', 'price_st...</td>\n",
       "      <td>True</td>\n",
       "      <td>[{'title': 'Kirill Eremenko', 'name': 'Kirill'...</td>\n",
       "      <td>{'locale': 'en_US', 'title': 'English (US)', '...</td>\n",
       "      <td>&lt;p&gt;Learn R Programming by doing!&lt;/p&gt;&lt;p&gt;There a...</td>\n",
       "      <td>Learn Programming In R And R Studio. Data Anal...</td>\n",
       "      <td>2016-02-17T09:34:04Z</td>\n",
       "      <td>261248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.662838</td>\n",
       "      <td>51993</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>94</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-b.udemycdn.com/course/750x422/7652...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['No prior knowledge or experience needed. Onl...</td>\n",
       "      <td>['Learn to program in R at a good level', 'Lea...</td>\n",
       "      <td>[{'id': 5332, 'title': 'Data Mining', 'url': '...</td>\n",
       "      <td>['This course is for you if you want to learn ...</td>\n",
       "      <td>638</td>\n",
       "      <td>10.5 total hours</td>\n",
       "      <td>All Levels</td>\n",
       "      <td>['Learn to program in R at a good level', 'Lea...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1151632</td>\n",
       "      <td>Deep Learning A-Z 2023: Neural Networks, AI &amp;...</td>\n",
       "      <td>https://www.udemy.com/course/deeplearning/</td>\n",
       "      <td>{'amount': 99.99, 'currency': 'USD', 'price_st...</td>\n",
       "      <td>True</td>\n",
       "      <td>[{'title': 'Kirill Eremenko', 'name': 'Kirill'...</td>\n",
       "      <td>{'locale': 'en_US', 'title': 'English (US)', '...</td>\n",
       "      <td>&lt;p&gt;***As seen on Kickstarter ***&lt;/p&gt;&lt;p&gt;Artifi...</td>\n",
       "      <td>Learn to create Deep Learning Algorithms in Py...</td>\n",
       "      <td>2017-03-20T13:49:45Z</td>\n",
       "      <td>366900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.501419</td>\n",
       "      <td>44403</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-c.udemycdn.com/course/750x422/1151...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['High school mathematics level', 'Basic Pytho...</td>\n",
       "      <td>['Understand the intuition behind Artificial N...</td>\n",
       "      <td>[{'id': 5374, 'title': 'Deep Learning', 'url':...</td>\n",
       "      <td>['Anyone interested in Deep Learning', 'Studen...</td>\n",
       "      <td>1367</td>\n",
       "      <td>23 total hours</td>\n",
       "      <td>All Levels</td>\n",
       "      <td>['Understand the intuition behind Artificial N...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3288</th>\n",
       "      <td>3288</td>\n",
       "      <td>4906662</td>\n",
       "      <td>[]  (TensorFlow) </td>\n",
       "      <td>https://www.udemy.com/course/tensorflow-a/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'title': ' ', 'name': '', 'display_nam...</td>\n",
       "      <td>{'locale': 'ko_KR', 'title': '', 'english_t...</td>\n",
       "      <td>&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;       ...</td>\n",
       "      <td>      (TensorFlow) </td>\n",
       "      <td>2022-09-30T08:03:09Z</td>\n",
       "      <td>133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.850647</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-c.udemycdn.com/course/750x422/4906...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['   .']</td>\n",
       "      <td>['(Pandas)', '   1.   ', '...</td>\n",
       "      <td>[{'id': 5374, 'title': 'Deep Learning', 'url':...</td>\n",
       "      <td>['  ', '     ', '...</td>\n",
       "      <td>109</td>\n",
       "      <td>2 total hours</td>\n",
       "      <td>All Levels</td>\n",
       "      <td>['(Pandas)', '   1.   ', '...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>3289</td>\n",
       "      <td>5219668</td>\n",
       "      <td>Crer un Modern Data Pipeline en moins de 2h</td>\n",
       "      <td>https://www.udemy.com/course/creer-un-modern-d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'title': 'Zied BEN OTHMANE', 'name': 'Zied',...</td>\n",
       "      <td>{'locale': 'fr_FR', 'title': 'Franais (France...</td>\n",
       "      <td>&lt;p&gt;Pour rpondre  un besoin mtier, un ingni...</td>\n",
       "      <td>Full data engineering pipeline</td>\n",
       "      <td>2023-03-18T14:40:02Z</td>\n",
       "      <td>289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.970639</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-b.udemycdn.com/course/750x422/5219...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['Connaissances pralable en Python et SQL', '...</td>\n",
       "      <td>[\"Creation d'objets sous snowflake\", 'Comprend...</td>\n",
       "      <td>[{'id': 4460, 'title': 'Amazon S3', 'url': 'ht...</td>\n",
       "      <td>['Les ingnieurs de donnes', 'Les data scient...</td>\n",
       "      <td>116</td>\n",
       "      <td>2 total hours</td>\n",
       "      <td>Intermediate Level</td>\n",
       "      <td>[\"Creation d'objets sous snowflake\", 'Comprend...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>3290</td>\n",
       "      <td>5264986</td>\n",
       "      <td>Introduzione all'Intelligenza Artificiale e Ca...</td>\n",
       "      <td>https://www.udemy.com/course/introduzione-alli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'title': 'DEEP LEARNING ITALIA .COM', 'name'...</td>\n",
       "      <td>{'locale': 'it_IT', 'title': 'Italiano', 'engl...</td>\n",
       "      <td>&lt;p&gt;Il corso contiene i video introduttivi dei ...</td>\n",
       "      <td>Raccolta di video introduttivi sui principali ...</td>\n",
       "      <td>2023-04-10T17:08:33Z</td>\n",
       "      <td>146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.287920</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-b.udemycdn.com/course/750x422/5264...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['Chi ha una mente aperta e predisposta allan...</td>\n",
       "      <td>['Introduzione ai principali approcci del Deep...</td>\n",
       "      <td>[{'id': 4624, 'title': 'Artificial Intelligenc...</td>\n",
       "      <td>[\"Introduzione ai principali campi dell'intell...</td>\n",
       "      <td>48</td>\n",
       "      <td>1 total hour</td>\n",
       "      <td>Beginner Level</td>\n",
       "      <td>['Introduzione ai principali approcci del Deep...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>3291</td>\n",
       "      <td>5371304</td>\n",
       "      <td>Databricks Certified Data Engineer Associate E...</td>\n",
       "      <td>https://www.udemy.com/course/databricks-certif...</td>\n",
       "      <td>{'amount': 69.99, 'currency': 'USD', 'price_st...</td>\n",
       "      <td>True</td>\n",
       "      <td>[{'title': 'Henry Habib', 'name': 'Henry', 'di...</td>\n",
       "      <td>{'locale': 'en_US', 'title': 'English (US)', '...</td>\n",
       "      <td>&lt;p&gt;Are you ready to take &lt;strong&gt;Data Engineer...</td>\n",
       "      <td>Databricks Certified Data Engineer Associate 2...</td>\n",
       "      <td>2023-06-07T00:50:58Z</td>\n",
       "      <td>429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.857799</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-b.udemycdn.com/course/750x422/5371...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['Basic understanding of cloud computing conce...</td>\n",
       "      <td>['Pass the Databricks Data Engineer Associate ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Data engineers and enthusiasts pursuing the ...</td>\n",
       "      <td>480</td>\n",
       "      <td>8 total hours</td>\n",
       "      <td>All Levels</td>\n",
       "      <td>['Pass the Databricks Data Engineer Associate ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>3292</td>\n",
       "      <td>2482910</td>\n",
       "      <td>Natural Language Processing (NLP) with Python ...</td>\n",
       "      <td>https://www.udemy.com/course/natural-language-...</td>\n",
       "      <td>{'amount': 34.99, 'currency': 'USD', 'price_st...</td>\n",
       "      <td>True</td>\n",
       "      <td>[{'title': 'Abhishek Kumar', 'name': 'Abhishek...</td>\n",
       "      <td>{'locale': 'en_US', 'title': 'English (US)', '...</td>\n",
       "      <td>&lt;p&gt;Natural Language Processing or NLP is a ver...</td>\n",
       "      <td>Master Natural Language with Python and NLP us...</td>\n",
       "      <td>2019-07-29T12:02:35Z</td>\n",
       "      <td>1159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.410844</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>{'discussions_create': True, 'discussions_view...</td>\n",
       "      <td>https://img-c.udemycdn.com/course/750x422/2482...</td>\n",
       "      <td>{'id': 288, 'title': 'Development', 'title_cle...</td>\n",
       "      <td>{'id': 558, 'title': 'Data Science', 'title_cl...</td>\n",
       "      <td>['Basic programming skills in Python. Familiar...</td>\n",
       "      <td>['Natural Language Processing using Python']</td>\n",
       "      <td>[{'id': 6874, 'title': 'Natural Language Proce...</td>\n",
       "      <td>['Data scientists, Applied Machine Learning en...</td>\n",
       "      <td>213</td>\n",
       "      <td>3.5 total hours</td>\n",
       "      <td>Intermediate Level</td>\n",
       "      <td>['Natural Language Processing using Python']</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3353 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       id                                              title  \\\n",
       "0              0   950390  Machine Learning A-Z: AI, Python & R + ChatGP...   \n",
       "1              1   903744  Python for Data Science and Machine Learning B...   \n",
       "2              2  1754098  The Data Science Course: Complete Data Science...   \n",
       "3              3   765242  R Programming A-Z: R For Data Science With Re...   \n",
       "4              4  1151632  Deep Learning A-Z 2023: Neural Networks, AI &...   \n",
       "...          ...      ...                                                ...   \n",
       "3288        3288  4906662                    []  (TensorFlow)    \n",
       "3289        3289  5219668       Crer un Modern Data Pipeline en moins de 2h   \n",
       "3290        3290  5264986  Introduzione all'Intelligenza Artificiale e Ca...   \n",
       "3291        3291  5371304  Databricks Certified Data Engineer Associate E...   \n",
       "3292        3292  2482910  Natural Language Processing (NLP) with Python ...   \n",
       "\n",
       "                                                    url  \\\n",
       "0         https://www.udemy.com/course/machinelearning/   \n",
       "1     https://www.udemy.com/course/python-for-data-s...   \n",
       "2     https://www.udemy.com/course/the-data-science-...   \n",
       "3           https://www.udemy.com/course/r-programming/   \n",
       "4            https://www.udemy.com/course/deeplearning/   \n",
       "...                                                 ...   \n",
       "3288         https://www.udemy.com/course/tensorflow-a/   \n",
       "3289  https://www.udemy.com/course/creer-un-modern-d...   \n",
       "3290  https://www.udemy.com/course/introduzione-alli...   \n",
       "3291  https://www.udemy.com/course/databricks-certif...   \n",
       "3292  https://www.udemy.com/course/natural-language-...   \n",
       "\n",
       "                                           price_detail  is_paid  \\\n",
       "0     {'amount': 109.99, 'currency': 'USD', 'price_s...     True   \n",
       "1     {'amount': 99.99, 'currency': 'USD', 'price_st...     True   \n",
       "2     {'amount': 109.99, 'currency': 'USD', 'price_s...     True   \n",
       "3     {'amount': 94.99, 'currency': 'USD', 'price_st...     True   \n",
       "4     {'amount': 99.99, 'currency': 'USD', 'price_st...     True   \n",
       "...                                                 ...      ...   \n",
       "3288                                                NaN    False   \n",
       "3289                                                NaN    False   \n",
       "3290                                                NaN    False   \n",
       "3291  {'amount': 69.99, 'currency': 'USD', 'price_st...     True   \n",
       "3292  {'amount': 34.99, 'currency': 'USD', 'price_st...     True   \n",
       "\n",
       "                                    visible_instructors  \\\n",
       "0     [{'title': 'Kirill Eremenko', 'name': 'Kirill'...   \n",
       "1     [{'title': 'Jose Portilla', 'name': 'Jose', 'd...   \n",
       "2     [{'title': '365 Careers', 'name': '365', 'disp...   \n",
       "3     [{'title': 'Kirill Eremenko', 'name': 'Kirill'...   \n",
       "4     [{'title': 'Kirill Eremenko', 'name': 'Kirill'...   \n",
       "...                                                 ...   \n",
       "3288  [{'title': ' ', 'name': '', 'display_nam...   \n",
       "3289  [{'title': 'Zied BEN OTHMANE', 'name': 'Zied',...   \n",
       "3290  [{'title': 'DEEP LEARNING ITALIA .COM', 'name'...   \n",
       "3291  [{'title': 'Henry Habib', 'name': 'Henry', 'di...   \n",
       "3292  [{'title': 'Abhishek Kumar', 'name': 'Abhishek...   \n",
       "\n",
       "                                                 locale  \\\n",
       "0     {'locale': 'en_US', 'title': 'English (US)', '...   \n",
       "1     {'locale': 'en_US', 'title': 'English (US)', '...   \n",
       "2     {'locale': 'en_US', 'title': 'English (US)', '...   \n",
       "3     {'locale': 'en_US', 'title': 'English (US)', '...   \n",
       "4     {'locale': 'en_US', 'title': 'English (US)', '...   \n",
       "...                                                 ...   \n",
       "3288  {'locale': 'ko_KR', 'title': '', 'english_t...   \n",
       "3289  {'locale': 'fr_FR', 'title': 'Franais (France...   \n",
       "3290  {'locale': 'it_IT', 'title': 'Italiano', 'engl...   \n",
       "3291  {'locale': 'en_US', 'title': 'English (US)', '...   \n",
       "3292  {'locale': 'en_US', 'title': 'English (US)', '...   \n",
       "\n",
       "                                            description  \\\n",
       "0     <p>Interested in the field of Machine Learning...   \n",
       "1     <p>Are you ready to start your path to becomin...   \n",
       "2     <p><strong>The Problem</strong></p><p>Data sci...   \n",
       "3     <p>Learn R Programming by doing!</p><p>There a...   \n",
       "4     <p>***As seen on Kickstarter ***</p><p>Artifi...   \n",
       "...                                                 ...   \n",
       "3288  <p><br></p><p>       ...   \n",
       "3289  <p>Pour rpondre  un besoin mtier, un ingni...   \n",
       "3290  <p>Il corso contiene i video introduttivi dei ...   \n",
       "3291  <p>Are you ready to take <strong>Data Engineer...   \n",
       "3292  <p>Natural Language Processing or NLP is a ver...   \n",
       "\n",
       "                                               headline               created  \\\n",
       "0     Learn to create Machine Learning Algorithms in...  2016-09-05T09:54:22Z   \n",
       "1     Learn how to use NumPy, Pandas, Seaborn , Matp...  2016-07-13T05:22:58Z   \n",
       "2     Complete Data Science Training: Mathematics, S...  2018-06-18T12:30:08Z   \n",
       "3     Learn Programming In R And R Studio. Data Anal...  2016-02-17T09:34:04Z   \n",
       "4     Learn to create Deep Learning Algorithms in Py...  2017-03-20T13:49:45Z   \n",
       "...                                                 ...                   ...   \n",
       "3288            (TensorFlow)   2022-09-30T08:03:09Z   \n",
       "3289                     Full data engineering pipeline  2023-03-18T14:40:02Z   \n",
       "3290  Raccolta di video introduttivi sui principali ...  2023-04-10T17:08:33Z   \n",
       "3291  Databricks Certified Data Engineer Associate 2...  2023-06-07T00:50:58Z   \n",
       "3292  Master Natural Language with Python and NLP us...  2019-07-29T12:02:35Z   \n",
       "\n",
       "      num_subscribers  discount  discount_price    rating  num_reviews  \\\n",
       "0              993456       NaN             NaN  4.513425       176950   \n",
       "1              674125       NaN             NaN  4.588927       136746   \n",
       "2              634583       NaN             NaN  4.594564       131509   \n",
       "3              261248       NaN             NaN  4.662838        51993   \n",
       "4              366900       NaN             NaN  4.501419        44403   \n",
       "...               ...       ...             ...       ...          ...   \n",
       "3288              133       NaN             NaN  4.850647            8   \n",
       "3289              289       NaN             NaN  3.970639            7   \n",
       "3290              146       NaN             NaN  4.287920            6   \n",
       "3291              429       NaN             NaN  3.857799           39   \n",
       "3292             1159       NaN             NaN  4.410844           37   \n",
       "\n",
       "      num_quizzes  num_lectures  num_curriculum_items  \\\n",
       "0              39           468                   507   \n",
       "1               1           184                   185   \n",
       "2             281           518                   799   \n",
       "3               5            89                    94   \n",
       "4               0           230                   230   \n",
       "...           ...           ...                   ...   \n",
       "3288            0            19                    19   \n",
       "3289            0            20                    20   \n",
       "3290            0             5                     5   \n",
       "3291            0            82                    82   \n",
       "3292            0            26                    26   \n",
       "\n",
       "                                               features  \\\n",
       "0     {'discussions_create': True, 'discussions_view...   \n",
       "1     {'discussions_create': True, 'discussions_view...   \n",
       "2     {'discussions_create': True, 'discussions_view...   \n",
       "3     {'discussions_create': True, 'discussions_view...   \n",
       "4     {'discussions_create': True, 'discussions_view...   \n",
       "...                                                 ...   \n",
       "3288  {'discussions_create': True, 'discussions_view...   \n",
       "3289  {'discussions_create': True, 'discussions_view...   \n",
       "3290  {'discussions_create': True, 'discussions_view...   \n",
       "3291  {'discussions_create': True, 'discussions_view...   \n",
       "3292  {'discussions_create': True, 'discussions_view...   \n",
       "\n",
       "                                                  image  \\\n",
       "0     https://img-c.udemycdn.com/course/750x422/9503...   \n",
       "1     https://img-c.udemycdn.com/course/750x422/9037...   \n",
       "2     https://img-c.udemycdn.com/course/750x422/1754...   \n",
       "3     https://img-b.udemycdn.com/course/750x422/7652...   \n",
       "4     https://img-c.udemycdn.com/course/750x422/1151...   \n",
       "...                                                 ...   \n",
       "3288  https://img-c.udemycdn.com/course/750x422/4906...   \n",
       "3289  https://img-b.udemycdn.com/course/750x422/5219...   \n",
       "3290  https://img-b.udemycdn.com/course/750x422/5264...   \n",
       "3291  https://img-b.udemycdn.com/course/750x422/5371...   \n",
       "3292  https://img-c.udemycdn.com/course/750x422/2482...   \n",
       "\n",
       "                                       primary_category  \\\n",
       "0     {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "1     {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "2     {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "3     {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "4     {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "...                                                 ...   \n",
       "3288  {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "3289  {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "3290  {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "3291  {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "3292  {'id': 288, 'title': 'Development', 'title_cle...   \n",
       "\n",
       "                                    primary_subcategory  \\\n",
       "0     {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "1     {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "2     {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "3     {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "4     {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "...                                                 ...   \n",
       "3288  {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "3289  {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "3290  {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "3291  {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "3292  {'id': 558, 'title': 'Data Science', 'title_cl...   \n",
       "\n",
       "                                      requirements_data  \\\n",
       "0          ['Just some high school mathematics level.']   \n",
       "1     ['Some programming experience', 'Admin permiss...   \n",
       "2     ['No prior experience is required. We will sta...   \n",
       "3     ['No prior knowledge or experience needed. Onl...   \n",
       "4     ['High school mathematics level', 'Basic Pytho...   \n",
       "...                                                 ...   \n",
       "3288                             ['   .']   \n",
       "3289  ['Connaissances pralable en Python et SQL', '...   \n",
       "3290  ['Chi ha una mente aperta e predisposta allan...   \n",
       "3291  ['Basic understanding of cloud computing conce...   \n",
       "3292  ['Basic programming skills in Python. Familiar...   \n",
       "\n",
       "                               what_you_will_learn_data  \\\n",
       "0     ['Master Machine Learning on Python & R', 'Hav...   \n",
       "1     ['Use Python for Data Science and Machine Lear...   \n",
       "2     ['The course provides the entire toolbox you n...   \n",
       "3     ['Learn to program in R at a good level', 'Lea...   \n",
       "4     ['Understand the intuition behind Artificial N...   \n",
       "...                                                 ...   \n",
       "3288  ['(Pandas)', '   1.   ', '...   \n",
       "3289  [\"Creation d'objets sous snowflake\", 'Comprend...   \n",
       "3290  ['Introduzione ai principali approcci del Deep...   \n",
       "3291  ['Pass the Databricks Data Engineer Associate ...   \n",
       "3292       ['Natural Language Processing using Python']   \n",
       "\n",
       "                                                 labels  \\\n",
       "0     [{'id': 5336, 'title': 'Data Science', 'url': ...   \n",
       "1     [{'id': 5336, 'title': 'Data Science', 'url': ...   \n",
       "2     [{'id': 5336, 'title': 'Data Science', 'url': ...   \n",
       "3     [{'id': 5332, 'title': 'Data Mining', 'url': '...   \n",
       "4     [{'id': 5374, 'title': 'Deep Learning', 'url':...   \n",
       "...                                                 ...   \n",
       "3288  [{'id': 5374, 'title': 'Deep Learning', 'url':...   \n",
       "3289  [{'id': 4460, 'title': 'Amazon S3', 'url': 'ht...   \n",
       "3290  [{'id': 4624, 'title': 'Artificial Intelligenc...   \n",
       "3291                                                 []   \n",
       "3292  [{'id': 6874, 'title': 'Natural Language Proce...   \n",
       "\n",
       "                                       target_audiences  \\\n",
       "0     ['Anyone interested in Machine Learning.', 'St...   \n",
       "1     ['This course is meant for people with at leas...   \n",
       "2     ['You should take this course if you want to b...   \n",
       "3     ['This course is for you if you want to learn ...   \n",
       "4     ['Anyone interested in Deep Learning', 'Studen...   \n",
       "...                                                 ...   \n",
       "3288  ['  ', '     ', '...   \n",
       "3289  ['Les ingnieurs de donnes', 'Les data scient...   \n",
       "3290  [\"Introduzione ai principali campi dell'intell...   \n",
       "3291  ['Data engineers and enthusiasts pursuing the ...   \n",
       "3292  ['Data scientists, Applied Machine Learning en...   \n",
       "\n",
       "      estimated_content_length      content_info instructional_level  \\\n",
       "0                         2559  42.5 total hours          All Levels   \n",
       "1                         1494    25 total hours          All Levels   \n",
       "2                         1906    32 total hours          All Levels   \n",
       "3                          638  10.5 total hours          All Levels   \n",
       "4                         1367    23 total hours          All Levels   \n",
       "...                        ...               ...                 ...   \n",
       "3288                       109     2 total hours          All Levels   \n",
       "3289                       116     2 total hours  Intermediate Level   \n",
       "3290                        48      1 total hour      Beginner Level   \n",
       "3291                       480     8 total hours          All Levels   \n",
       "3292                       213   3.5 total hours  Intermediate Level   \n",
       "\n",
       "                                             objectives  has_certificate  \n",
       "0     ['Master Machine Learning on Python & R', 'Hav...             True  \n",
       "1     ['Use Python for Data Science and Machine Lear...             True  \n",
       "2     ['The course provides the entire toolbox you n...             True  \n",
       "3     ['Learn to program in R at a good level', 'Lea...             True  \n",
       "4     ['Understand the intuition behind Artificial N...             True  \n",
       "...                                                 ...              ...  \n",
       "3288  ['(Pandas)', '   1.   ', '...            False  \n",
       "3289  [\"Creation d'objets sous snowflake\", 'Comprend...            False  \n",
       "3290  ['Introduzione ai principali approcci del Deep...            False  \n",
       "3291  ['Pass the Databricks Data Engineer Associate ...             True  \n",
       "3292       ['Natural Language Processing using Python']             True  \n",
       "\n",
       "[3353 rows x 32 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'randstad technology data center site engineer located wilmington de relocation assistance available global financial leader exciting data center opportunity physical hand essential worker creative schedule enables extra day compared normal day week family free weekly covid testing opportunity growth advancement unique workload day thing day world class data center seeking experienced data center technician long term project role located wilmington de area basic responsibility include limited rack stack enterprise class technology network storage enterprise server running tracing dressing testing copper fiber cable patch panel device familiarity patch panel troubleshooting diagnostics hardware connectivity commission decommission hardware able perform component replacement dimms hard drive power supply adhere change control process required skill previous experience working complex high availability data center environment providing layer hardware installation ongoing maintenance problem identification incident resolution strong working knowledge data center process design hand expertise midrange system large scale server environment ibm hp oracle amp dell server hardware exceptional troubleshooting problem resolution skill ability document complex problem resolution summary repetitive task interface online ticketing system understanding switch port configuration critical thinking decision making related outage risk assessment strong cable management skill experience managing fiber amp copper data center environment desired skill certification server network technical writing documentation skill preferred proficient microsoft office role operates hour shift amp rotating schedule benefit condition waiting period apply duration long term long term opportunity growth professional development promote job type time pay hour benefit k dental insurance employee assistance program employee discount health insurance life insurance paid time referral program vision insurance schedule hour shift overtime weekend needed covid consideration regular free testing education high school equivalent preferred experience data center year preferred work location person'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_j[\"jobDescription\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'randstad technology data center site engineer located wilmington de relocation assistance available global financial leader exciting data center opportunity physical hand essential worker creative schedule enables extra day compared normal day week family free weekly covid testing opportunity growth advancement unique workload day thing day world class data center seeking experienced data center technician long term project role located wilmington de area basic responsibility include limited rack stack enterprise class technology network storage enterprise server running tracing dressing testing copper fiber cable patch panel device familiarity patch panel troubleshooting diagnostics hardware connectivity commission decommission hardware able perform component replacement dimms hard drive power supply adhere change control process required skill previous experience working complex high availability data center environment providing layer hardware installation ongoing maintenance problem identification incident resolution strong working knowledge data center process design hand expertise midrange system large scale server environment ibm hp oracle amp dell server hardware exceptional troubleshooting problem resolution skill ability document complex problem resolution summary repetitive task interface online ticketing system understanding switch port configuration critical thinking decision making related outage risk assessment strong cable management skill experience managing fiber amp copper data center environment desired skill certification server network technical writing documentation skill preferred proficient microsoft office role operates hour shift amp rotating schedule benefit condition waiting period apply duration long term long term opportunity growth professional development promote job type time pay hour benefit k dental insurance employee assistance program employee discount health insurance life insurance paid time referral program vision insurance schedule hour shift overtime weekend needed covid consideration regular free testing education high school equivalent preferred experience data center year preferred work location person'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import pipeline\\n\\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\\n\\n# Your job description\\ntext = randstad technology data center site engineer located wilmington de relocation assistance available global financial leader exciting data center opportunity physical hand essential worker creative schedule enables extra day compared normal day week family free weekly covid testing opportunity growth advancement unique workload day thing day world class data center seeking experienced data center technician long term project role located wilmington de area basic responsibility include limited rack stack enterprise class technology network storage enterprise server running tracing dressing testing copper fiber cable patch panel device familiarity patch panel troubleshooting diagnostics hardware connectivity commission decommission hardware able perform component replacement dimms hard drive power supply adhere change control process required skill previous experience working complex high availability data center environment providing layer hardware installation ongoing maintenance problem identification incident resolution strong working knowledge data center process design hand expertise midrange system large scale server environment ibm hp oracle amp dell server hardware exceptional troubleshooting problem resolution skill ability document complex problem resolution summary repetitive task interface online ticketing system understanding switch port configuration critical thinking decision making related outage risk assessment strong cable management skill experience managing fiber amp copper data center environment desired skill certification server network technical writing documentation skill preferred proficient microsoft office role operates hour shift amp rotating schedule benefit condition waiting period apply duration long term long term opportunity growth professional development promote job type time pay hour benefit k dental insurance employee assistance program employee discount health insurance life insurance paid time referral program vision insurance schedule hour shift overtime weekend needed covid consideration regular free testing education high school equivalent preferred experience data center year preferred work location person\\n\\n# Candidate labels\\ncandidate_labels = skills_list\\n\\n# Using the classifier to predict the relevance of each candidate label to the text\\nresults = classifier(text, candidate_labels)\\nprint(results)\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Your job description\n",
    "text = randstad technology data center site engineer located wilmington de relocation assistance available global financial leader exciting data center opportunity physical hand essential worker creative schedule enables extra day compared normal day week family free weekly covid testing opportunity growth advancement unique workload day thing day world class data center seeking experienced data center technician long term project role located wilmington de area basic responsibility include limited rack stack enterprise class technology network storage enterprise server running tracing dressing testing copper fiber cable patch panel device familiarity patch panel troubleshooting diagnostics hardware connectivity commission decommission hardware able perform component replacement dimms hard drive power supply adhere change control process required skill previous experience working complex high availability data center environment providing layer hardware installation ongoing maintenance problem identification incident resolution strong working knowledge data center process design hand expertise midrange system large scale server environment ibm hp oracle amp dell server hardware exceptional troubleshooting problem resolution skill ability document complex problem resolution summary repetitive task interface online ticketing system understanding switch port configuration critical thinking decision making related outage risk assessment strong cable management skill experience managing fiber amp copper data center environment desired skill certification server network technical writing documentation skill preferred proficient microsoft office role operates hour shift amp rotating schedule benefit condition waiting period apply duration long term long term opportunity growth professional development promote job type time pay hour benefit k dental insurance employee assistance program employee discount health insurance life insurance paid time referral program vision insurance schedule hour shift overtime weekend needed covid consideration regular free testing education high school equivalent preferred experience data center year preferred work location person\n",
    "\n",
    "# Candidate labels\n",
    "candidate_labels = skills_list\n",
    "\n",
    "# Using the classifier to predict the relevance of each candidate label to the text\n",
    "results = classifier(text, candidate_labels)\n",
    "print(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data architecture SKILL\n",
      "data infrastructure SKILL\n",
      "data pipeline SKILL\n",
      "azure data factory SKILL\n",
      "azure data factory SKILL\n",
      "azure data lake SKILL\n",
      "analytics SKILL\n",
      "data storage SKILL\n",
      "sql SKILL\n",
      "database SKILL\n",
      "analytics SKILL\n",
      "data mart SKILL\n",
      "azure data factory SKILL\n",
      "sql SKILL\n",
      "azure data lake SKILL\n",
      "data pipeline SKILL\n",
      "ci cd SKILL\n",
      "azure databricks SKILL\n",
      "analytics SKILL\n",
      "power bi SKILL\n",
      "sql SKILL\n",
      "database SKILL\n",
      "azure data factory SKILL\n",
      "data store SKILL\n",
      "c SKILL\n",
      "computer science SKILL\n",
      "azure data factory SKILL\n",
      "data modeling SKILL\n",
      "database management SKILL\n",
      "sql azure SKILL\n",
      "sql SKILL\n",
      "azure data lake SKILL\n",
      "data warehousing SKILL\n",
      "problem solving SKILL\n",
      "project management SKILL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the model\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")  # Create the matcher object\n",
    "\n",
    "# Assuming 'skills_list' is a list of skills, and 'job_descriptions' is a list containing job descriptions\n",
    "\n",
    "# Add patterns to the matcher. Patterns are made by converting each skill string into a Doc object\n",
    "patterns = [nlp.make_doc(skill) for skill in skills_list]\n",
    "matcher.add(\"Skills\", patterns)\n",
    "\n",
    "# Process the job description to create a Spacy Doc\n",
    "doc = nlp(job_descriptions[4])\n",
    "\n",
    "# Match the patterns to the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Create Span objects for the matched sequences\n",
    "spans = [Span(doc, start, end, label=\"SKILL\") for match_id, start, end in matches]\n",
    "\n",
    "# Filter the spans to remove overlaps\n",
    "filtered_spans = filter_spans(spans)\n",
    "\n",
    "# Now you can create new entities in the doc using the filtered spans\n",
    "doc.ents = filtered_spans  # Overwrite or append to doc.ents with the non-overlapping skill entities\n",
    "\n",
    "# Print the entities in the document\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence SKILL\n",
      "deep learning SKILL\n",
      "artificial intelligence SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "algorithm SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "algorithm SKILL\n",
      "artificial neural network SKILL\n",
      "convolutional neural network SKILL\n",
      "boltzmann machine SKILL\n",
      "autoencoders SKILL\n",
      "autoencoders SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "dataset SKILL\n",
      "algorithm SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "algorithm SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "analytics SKILL\n",
      "dataset SKILL\n",
      "dataset SKILL\n",
      "artificial neural network SKILL\n",
      "probability SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "convolutional neural network SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "artificial intelligence SKILL\n",
      "deep learning SKILL\n",
      "deep learning method SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "dataset SKILL\n",
      "amazon SKILL\n",
      "dataset SKILL\n",
      "dataset SKILL\n",
      "dataset SKILL\n",
      "deep learning SKILL\n",
      "deep learning SKILL\n",
      "boltzmann machine SKILL\n",
      "autoencoders SKILL\n",
      "dataset SKILL\n",
      "deep learning SKILL\n",
      "artificial neural network SKILL\n",
      "artificial neural network SKILL\n",
      "convolutional neural network SKILL\n",
      "convolutional neural network SKILL\n",
      "boltzmann machine SKILL\n",
      "boltzmann machine SKILL\n",
      "autoencoders SKILL\n",
      "autoencoders SKILL\n",
      "artificial neural network SKILL\n",
      "artificial neural network SKILL\n",
      "convolutional neural network SKILL\n",
      "convolutional neural network SKILL\n",
      "boltzmann machine SKILL\n",
      "boltzmann machine SKILL\n",
      "autoencoders SKILL\n",
      "autoencoders SKILL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the model\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")  # Create the matcher object\n",
    "\n",
    "# Assuming 'skills_list' is a list of skills, and 'job_descriptions' is a list containing job descriptions\n",
    "\n",
    "# Add patterns to the matcher. Patterns are made by converting each skill string into a Doc object\n",
    "patterns = [nlp.make_doc(skill) for skill in skills_list]\n",
    "matcher.add(\"Skills\", patterns)\n",
    "\n",
    "# Process the job description to create a Spacy Doc\n",
    "doc = nlp(course_description[4])\n",
    "\n",
    "# Match the patterns to the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Create Span objects for the matched sequences\n",
    "spans = [Span(doc, start, end, label=\"SKILL\") for match_id, start, end in matches]\n",
    "\n",
    "# Filter the spans to remove overlaps\n",
    "filtered_spans = filter_spans(spans)\n",
    "\n",
    "# Now you can create new entities in the doc using the filtered spans\n",
    "doc.ents = filtered_spans  # Overwrite or append to doc.ents with the non-overlapping skill entities\n",
    "\n",
    "# Print the entities in the document\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[artificial intelligence,\n",
       " deep learning,\n",
       " artificial intelligence,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " algorithm,\n",
       " deep learning,\n",
       " deep learning,\n",
       " algorithm,\n",
       " artificial neural network,\n",
       " convolutional neural network,\n",
       " boltzmann machine,\n",
       " autoencoders,\n",
       " autoencoders,\n",
       " deep learning,\n",
       " deep learning,\n",
       " dataset,\n",
       " algorithm,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " algorithm,\n",
       " deep learning,\n",
       " deep learning,\n",
       " analytics,\n",
       " dataset,\n",
       " dataset,\n",
       " artificial neural network,\n",
       " probability,\n",
       " deep learning,\n",
       " deep learning,\n",
       " convolutional neural network,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " artificial intelligence,\n",
       " deep learning,\n",
       " deep learning method,\n",
       " deep learning,\n",
       " deep learning,\n",
       " deep learning,\n",
       " dataset,\n",
       " amazon,\n",
       " dataset,\n",
       " dataset,\n",
       " dataset,\n",
       " deep learning,\n",
       " deep learning,\n",
       " boltzmann machine,\n",
       " autoencoders,\n",
       " dataset,\n",
       " deep learning,\n",
       " artificial neural network,\n",
       " artificial neural network,\n",
       " convolutional neural network,\n",
       " convolutional neural network,\n",
       " boltzmann machine,\n",
       " boltzmann machine,\n",
       " autoencoders,\n",
       " autoencoders,\n",
       " artificial neural network,\n",
       " artificial neural network,\n",
       " convolutional neural network,\n",
       " convolutional neural network,\n",
       " boltzmann machine,\n",
       " boltzmann machine,\n",
       " autoencoders,\n",
       " autoencoders]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the model\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")  # Create the matcher object\n",
    "\n",
    "\n",
    "\n",
    "# Convert the skills list to Doc objects and add them as patterns to the matcher\n",
    "patterns = [nlp.make_doc(skill) for skill in skills_list]\n",
    "matcher.add(\"Skills\", patterns)\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "# Loop through the job descriptions and create training data\n",
    "for job_description in job_descriptions:\n",
    "    # Process the job description to create a Spacy Doc\n",
    "    doc = nlp(job_description)\n",
    "    # Match the patterns to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create Span objects for the matched sequences\n",
    "    spans = [Span(doc, start, end, label=\"SKILLS\") for match_id, start, end in matches]\n",
    "    # Filter the spans to remove overlaps\n",
    "    #print(spans)\n",
    "    filtered_spans = filter_spans(spans)\n",
    "    #print(spans)\n",
    "    entities = [(span.start_char, span.end_char, span) for span in filtered_spans]\n",
    "    TRAIN_DATA.append((job_description, {\"entities\": entities}))\n",
    "\n",
    "#print(TRAIN_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('description data engineer responsible creation ongoing maintenance internal external data pipeline service includes timely resolution data issue customer ad hoc data request assigned data architect leadership team collaborate product owner operation process owner identify opportunity define business requirement assist designing implementing solution assist reporting critical process system responsibility life cycle project analysis development delivery solution modify existing process improve operational efficiency entire data processing team perform data mining aggregation combining multiple datasets develop streamlined efficient solution internal external party perform data validation analysis ensure accuracy quality data handle data conversion data cleansing report generation scheduled data delivery standard ability deliver data feature based operation leadership requirement requirement required experience year data engineering experience relative analytical engineering experience year sql mssql relative sql syntax advanced trigger procedure managed statement understanding dimensional programming year python java experience experience power bi google data studio looker tableau experience version control git experience automated infrastructure infrastructure code e ansible terraform understanding sdlc secure programming skill strong understanding cloud platform azure amp gcp google cloud platform experience data warehouse environment azure synapse google bigquery aws redshift snowflake etc education bachelor degree related field equivalent experience required preferred experience skill ability experience machine learning infrastructure modeling experience training routine creation physical technical environment noise level work environment moderate ability maintain focus high level pressure multiple priority',\n",
       "  {'entities': [(85, 98, data pipeline),\n",
       "    (292, 312, business requirement),\n",
       "    (528, 543, data processing),\n",
       "    (557, 568, data mining),\n",
       "    (680, 695, data validation),\n",
       "    (741, 756, data conversion),\n",
       "    (757, 771, data cleansing),\n",
       "    (928, 944, data engineering),\n",
       "    (1004, 1007, sql),\n",
       "    (1023, 1026, sql),\n",
       "    (1129, 1133, java),\n",
       "    (1156, 1164, power bi),\n",
       "    (1481, 1489, bigquery)]}),\n",
       " ('innovating connection risk capital velocity risk delivers specialty property insurance catastrophe prone community embracing nature volatility turning problem potential transforming old insurance practice uncommon approach business creates uncommon career employee empowered flourish enjoy career work routine task project keen attention detail focus like look problem analyze perceived outcome research based smart decision consider steady consistent work diligence product create ability execute task assigned looking data engineer crucial data architecture velocity utilizing knowledge detailed mind develop reliable tool system data better business consumption analysis thrive position core capability high attention detail focused attentive correct data pinpointing inaccurate information quickly efficiently intellectual curiosity autonomy business acumen organization prioritization ability systematic record keeping skill able recognize time sensitivity adapt accordingly time management able shift adjust daily activity according business need dropping ball collaboration spirit camaraderie core responsibility role maintaining end end data pipeline produce documentation related internal data flow assist internal business partner data informed decision improve process develop tooling facilitate end user consumption company data merge business knowledge complex data source produce report ad hoc basis stay date latest technology trend related data engineering data governance automation data related process demonstrated experience success role year experience query language relational database management system year experience etl product experience informatica cloud plus year experience scripting language powershell python plus year experience report writing data visualization business intelligence software experience tableau plus velocity risk provides equal opportunity tolerate discrimination harassment strive ensure employee best experience possible provide comprehensive competitive benefit pack time employee medical dental vision fully subsidized velocity health saving account employer contribution k month employment education certification reimbursement bonus program paid time company holiday floating holiday flexibility gym membership reimbursement volunteer reward initiative office lunch provided hybrid team day remote day office velocity risk participates e verify http e verify uscis gov web medium resourcescontents e verify participation poster pdf',\n",
       "  {'entities': [(542, 559, data architecture),\n",
       "    (1145, 1158, data pipeline),\n",
       "    (1456, 1472, data engineering),\n",
       "    (1473, 1488, data governance),\n",
       "    (1589, 1626, relational database management system),\n",
       "    (1778, 1796, data visualization),\n",
       "    (1797, 1818, business intelligence),\n",
       "    (2404, 2408, http)]}),\n",
       " ('location united state america illinois chicago chicago united state america texas houston travel required travel expected role job category digital amp technology relocation available role eligible relocation country job type professional job code rq experience level intermediate job summary entity innovation amp engineering job family group amp group job summary responsible supporting delivery business analysis consulting process procedure defined specialism basic technical capability developing working relationship provide support query issue ad hoc request assisting quality assurance service specialism business analysis data management data science digital innovation job description bp innovation amp engineering team help drive digital transformation bp use data analytics major digital sub team amp e asset management responsible digital data initiative operation following area bp business production amp project health safety environment amp carbon refining amp operation well amp subsurface strategy amp sustainability dataworx data team responsible data area developing deep data capability transform access supply control quality vast growing data reserve measured petabyte dataworx team cover data sub discipline including data science data analytics data engineering data management specialist area geospatial remote sensing knowledge management digital twin dataworx team work wide variety data structured data unstructured data amp work real time streaming data processing batch data processing travel requirement relocation assistance remote type position hybrid office remote working skill business acumen commercial acumen communication data analysis data cleansing transformation data domain knowledge data integration data management data manipulation data sourcing data strategy governance data structure algorithm data visualization interpretation digital security extract transform load problem solving legal disclaimer equal opportunity employer value diversity company discriminate basis race religion color national origin sex gender gender expression sexual orientation age marital status socioeconomic status neurodiversity neurocognitive functioning veteran status disability status individual disability request reasonable accommodation related bp recruiting process e g accessing job application completing required assessment participating telephone screening interview etc like request accommodation related recruitment process contact request accommodation selected position depending role employment contingent adherence local policy include pre placement drug screening medical review physical fitness role background check',\n",
       "  {'entities': [(398, 415, business analysis),\n",
       "    (613, 630, business analysis),\n",
       "    (631, 646, data management),\n",
       "    (647, 659, data science),\n",
       "    (776, 785, analytics),\n",
       "    (844, 856, digital data),\n",
       "    (1243, 1255, data science),\n",
       "    (1261, 1270, analytics),\n",
       "    (1271, 1287, data engineering),\n",
       "    (1288, 1303, data management),\n",
       "    (1480, 1495, data processing),\n",
       "    (1502, 1517, data processing),\n",
       "    (1663, 1676, data analysis),\n",
       "    (1677, 1691, data cleansing),\n",
       "    (1707, 1718, data domain),\n",
       "    (1729, 1745, data integration),\n",
       "    (1746, 1761, data management),\n",
       "    (1762, 1779, data manipulation),\n",
       "    (1794, 1807, data strategy),\n",
       "    (1819, 1833, data structure),\n",
       "    (1834, 1843, algorithm),\n",
       "    (1844, 1862, data visualization),\n",
       "    (1918, 1933, problem solving)]}),\n",
       " ('note team working hybrid schedule day week client moline il office fully remote candidate considered position verify willingness work required hybrid schedule prior submitting req required skill need hand experience python aws apache spark year data engineering work experience building pipeline etl process hand databricks experience year hand experience aws java j ee design development testing integration high complex backend experience working big data nice skill proven work experience designing implementing loosely coupled microservices year experience cloud development platform like aws year hand experience stack design pattern designing loosely coupled architecture year experience ci cd build process configuration year proven experience agile scrum team environment year deep experience leveraging devsecops lean development principle continuous integration continuous delivery year experience working test driven development unit test framework experience implementing high quality code ensure feature meet business need scalable skilled interpersonal communication negotiation conflict resolution demonstrates proven experience work lead providing technical coaching mentoring bachelor degree computer science computer engineering equivalent experience related career field',\n",
       "  {'entities': [(227, 239, apache spark),\n",
       "    (245, 261, data engineering),\n",
       "    (313, 323, databricks),\n",
       "    (360, 364, java),\n",
       "    (449, 457, big data),\n",
       "    (561, 578, cloud development),\n",
       "    (694, 699, ci cd),\n",
       "    (700, 713, build process),\n",
       "    (849, 871, continuous integration),\n",
       "    (1209, 1225, computer science),\n",
       "    (1226, 1246, computer engineering)]}),\n",
       " ('hello hope urgently hiring data engineer united state title data engineer location columbia md experience year duration fulltime interested position share updated resume praneetha otssolutions com job description data engineer hybrid columbia md mid level time id salary k description looking talented data engineer support acquisition mission critical mission support data set preferred candidate background supporting cyber network related mission military space developer analyst engineer willing work hybrid role site work great team ft gordon ga columbia md essential job responsibility ideal candidate worked big data system complex structured unstructured data set supported government data acquisition analysis sharing effort past excel position candidate shall strong attention detail able understand technical complexity willingness learn adapt situation candidate work independently large team accomplish client objective minimum qualification security clearance current secret level security clearance required candidate u citizen year experience developer analyst engineer bachelor related field year relevant experience master related field high school diploma equivalent year relevant experience experience programming language python java proficiency acquisition understanding network data associated metadata fluency data extraction translation loading including data prep labeling enable data analytics experience kibana elasticsearch familiarity log format json xml experience data flow management storage solution e kafka nifi aws sqs solution security certification ability decompose technical problem troubleshoot system dataflow issue able work site hybrid role great team desired skill optional experience nosql database accumulo desired prior experience supporting cyber network security operation large enterprise analyst engineer architect developer job type time salary year experience level year schedule hour shift ability commute relocate columbia md reliably commute planning relocate starting work required security clearance secret required work location person',\n",
       "  {'entities': [(615, 623, big data),\n",
       "    (693, 709, data acquisition),\n",
       "    (1250, 1254, java),\n",
       "    (1334, 1349, data extraction),\n",
       "    (1411, 1420, analytics),\n",
       "    (1643, 1651, dataflow),\n",
       "    (1730, 1735, nosql),\n",
       "    (1736, 1744, database)]}),\n",
       " ('candidate expected relocate moline il expense day assignment hybrid role selected online assessment required completed interview required skill need hand experience python aws apache spark year data engineering work experience building pipeline etl process hand databricks experience hand machine learning experience year hand experience aws java j ee design development testing integration high complex backend experience working big data year experience machine learning life cycle planning data preparation model engineering model evaluation model deployment monitoring maintenance nice skill proven work experience designing implementing loosely coupled microservices year experience cloud development platform like aws year hand experience stack design pattern designing loosely coupled architecture year experience ci cd build process configuration year proven experience agile scrum team environment year deep experience leveraging devsecops lean development principle continuous integration continuous delivery year experience working test driven development unit test framework experience implementing high quality code ensure feature meet business need scalable skilled interpersonal communication negotiation conflict resolution demonstrates proven experience work lead providing technical coaching mentoring additional information bachelor degree computer science computer engineering equivalent experience related career field',\n",
       "  {'entities': [(176, 188, apache spark),\n",
       "    (194, 210, data engineering),\n",
       "    (262, 272, databricks),\n",
       "    (342, 346, java),\n",
       "    (431, 439, big data),\n",
       "    (688, 705, cloud development),\n",
       "    (821, 826, ci cd),\n",
       "    (827, 840, build process),\n",
       "    (976, 998, continuous integration),\n",
       "    (1359, 1375, computer science),\n",
       "    (1376, 1396, computer engineering)]}),\n",
       " ('fanduel group fanduel group innovative sport tech entertainment company changing way consumer engage favorite sport team league premier gaming destination united state fanduel group consists portfolio leading brand gaming sport betting daily fantasy sport advance deposit wagering tv medium fanduel group presence state approximately million customer retail location company based new york office california new jersey florida oregon georgia portugal romania scotland network fanduel tv fanduel broadly distributed linear cable television relationship leading direct consumer ott platform fanduel group subsidiary flutter entertainment plc world largest sport betting gaming operator portfolio globally recognized brand constituent ftse index london stock exchange roster fanduel group fan new innovative way interact favorite game sport team dedicated building winning team pride able moment mean especially come career winning look like fanduel recognition hard earned result culture brings best work roster talented coworkers mistake win believe winning right mean compromise come looking teammate creatives professional cutting edge technology innovator fanduel offer wide range career opportunity best class benefit tool explore grow best self fanduel principle team run office globe expect exciting company opportunity grow successful position roster opening fanduel group looking experienced data engineer deep understanding large scale data handling processing best practice cloud environment help build scalable system data key component business facet company including product development marketing operation finance vital deliver robust solution ensure reliable access data focus quality availability competitive edge come making decision based accurate timely data work provide access data company looking ahead phase data platform keen real time data processing working data scientist create machine learning pipeline game plan team play creating maintaining optimal data pipeline architecture designing implementing data pipeline required data warehouse data lake batch real time data transformation technology identifying designing implementing internal process improvement automating manual process optimizing data delivery designing infrastructure greater scalability designing deploying data model view large datasets meet functional non functional business requirement working stakeholder department assist data related technical issue supporting data infrastructure need delivering quality production ready code agile environment delivering test plan monitoring debugging technical document development cycle stats looking teammate advanced working sql knowledge experience working relational database build process supporting data transformation data structure metadata dependency workload management proficiency understanding complex etl process experience reading writing python script knowledge data integrity relational rule understanding aws google cloud ability quickly learn new technology critical proficiency agile lean development practice experience databrick dbt player contract treat team right opportunity professional development generous insurance paid leave policy committed making sure employee fanduel ask competitive compensation beginning team expect exciting fun environment committed driving real growth opportunity build cool product fan love mentorship professional development resource help refine game flexible vacation allowance let refuel hall fame benefit program platform fanduel group equal opportunity employer believe principal state team committed equal employment opportunity regardless race color ethnicity ancestry religion creed sex national origin sexual orientation age citizenship status marital status disability gender identity gender expression veteran status believe fanduel strongest best able compete employee feel valued respected included want team include diverse individual diversity thought diversity perspective diversity experience lead better performance diverse inclusive workforce core value believe make company stronger competitive team applicable salary range position dependent variety factor including relevant experience location business need market demand role offer following benefit medical vision dental insurance life insurance disability insurance k matching program employee benefit role eligible short term long term incentive compensation including limited cash bonus stock program participation role includes flexible time including unlimited paid time time employee paid company holiday fanduel offer paid sick time accordance applicable state federal law li hybrid',\n",
       "  {'entities': [(1860, 1875, data processing),\n",
       "    (1981, 1994, data pipeline),\n",
       "    (2031, 2044, data pipeline),\n",
       "    (2069, 2078, data lake),\n",
       "    (2095, 2114, data transformation),\n",
       "    (2368, 2388, business requirement),\n",
       "    (2467, 2486, data infrastructure),\n",
       "    (2670, 2673, sql),\n",
       "    (2714, 2722, database),\n",
       "    (2723, 2736, build process),\n",
       "    (2748, 2767, data transformation),\n",
       "    (2768, 2782, data structure),\n",
       "    (2920, 2934, data integrity)]}),\n",
       " ('data engineer healthcare k pto medical dental vision new york fully remote data engineer experience azure data warehouse data pipeline worked healthcare sector looking fresh exciting opportunity join health tech firm early stage offer exciting opportunity join innovative health tech development team forefront expanding optimizing data infrastructure playing vital role transforming healthcare cutting edge technology company pinnacle revolutionizing healthcare data driven solution create maintain data pipeline architecture work big data technology collaborating team enabling leverage data effectively role suit experienced data engineer looking chance advance career data engineering expand technical knowledge team transform technology based healthcare person year experience data engineering experience sql azure python knowledge data warehouse database development creating etl pipeline experience working healthcare degree computer science great communicator team player role develop maintain data pipeline automate task optimize data delivery build scalable data infrastructure construct analytics tool actionable insight collaborating cross functional team ensure data security develop data tool analytics enhance data system functionality',\n",
       "  {'entities': [(121, 134, data pipeline),\n",
       "    (332, 351, data infrastructure),\n",
       "    (500, 513, data pipeline),\n",
       "    (532, 540, big data),\n",
       "    (672, 688, data engineering),\n",
       "    (782, 798, data engineering),\n",
       "    (810, 819, sql azure),\n",
       "    (852, 872, database development),\n",
       "    (932, 948, computer science),\n",
       "    (1002, 1015, data pipeline),\n",
       "    (1068, 1087, data infrastructure),\n",
       "    (1098, 1107, analytics),\n",
       "    (1175, 1188, data security),\n",
       "    (1207, 1216, analytics),\n",
       "    (1225, 1236, data system)]}),\n",
       " ('leading data platform trusted thousand organization protect activate saas data transform business empowers customer ensure availability security compliance mission critical data unlocking new way gain deeper insight faster partnering world largest saas ecosystem salesforce servicenow microsoft dynamic enables customer world truly data power business platform data job data engineer collaborate closely analytics engineering business intelligence cross functional team craft business critical data asset build strong relationship key data consumer role report head data engineering play critical maintaining expanding internal database day day role develop data engineering pipeline asset data engineering peer document communicate complexity work user team identify area improvement database code power troubleshoot data issue arise define promote db management tool consolidation work experience generally year mathematics data science statistic c finance degree year professional experience clear verbal written communicator customer centric focus internal staff able write complex sql previous experience dbt desirable hand experience working relational database snowflake amazon redshift microsoft sql deploy code administer platform experience writing python script perform elt etl reverse etl experience airflow dagster orchestration tool experience working scrum agile methodology plus important detail time position ideal candidate work san diego office minimum day week maximize collaboration interaction business travel required base salary hiring range position actual offered successful candidate dependent factor education training skill qualification competency year experience job related knowledge scope role location dedicated creating environment employee thrive base pay total compensation package provided compensate recognize employee work role eligible unlimited pto generous medical benefit k saving plan employer match discretionary bonus incentive stock option offer catered lunch office day week creating environment employee thrive mean making sure employee feel accepted scale help type company protect precious data team reflect diversity serve equal opportunity employer believe employee company brings unique perspective contribute order impact day strive team culture build trust transparency discriminate based race color religion sex sexual orientation gender identity age national origin protected veteran status disability status learn owndata com li onsite',\n",
       "  {'entities': [(404, 413, analytics),\n",
       "    (426, 447, business intelligence),\n",
       "    (566, 582, data engineering),\n",
       "    (628, 636, database),\n",
       "    (658, 674, data engineering),\n",
       "    (690, 706, data engineering),\n",
       "    (785, 793, database),\n",
       "    (926, 938, data science),\n",
       "    (949, 950, c),\n",
       "    (1086, 1089, sql),\n",
       "    (1159, 1167, database),\n",
       "    (1178, 1193, amazon redshift),\n",
       "    (1204, 1207, sql),\n",
       "    (1312, 1319, airflow),\n",
       "    (1372, 1389, agile methodology)]}),\n",
       " ('data engineer month w contract san jose benefit love nextdeavor offer health vision dental benefit contract employee eligible receive paid sick leave varies state mentorship job coaching working company value physical mental spiritual opportunity foot door established corporation likelihood extension conversion time employment nextdeavor conversion rate approximately impact team use data drive strategic tactical decision finance program identify solve broad complex technical data issue lead cross team collaboration influence demonstrate strong analytical quantitive skill drive data based decision making craft thought proposal recommendation specify clear course action create streamlined content tailored data lake present management stack need successful role bachelor degree year relevant finance data modeling experience strong proficiency querying manipulating large data set analytical purpose sql like language hive hadoop experience preferred proficient python strong analytical quantitative problem solving ability excellent communication relationship skill strong team player previous experience creating maintaining financial model subscription saas business supporting financial reporting system automation complex organization plus strong communication amp interpersonal skill ability influence drive decision making cross functional team comfortable working technical resource ensure financial reporting infrastructure aligns business logic strong project management planning skill including ability handle project time fast paced environment problem solver challenge status quo drive new way thinking organization pay range hour ready mark leap apply directly lt http j brt mv jb reqgk amp refresh true gt application good hand',\n",
       "  {'entities': [(713, 722, data lake),\n",
       "    (807, 820, data modeling),\n",
       "    (907, 910, sql),\n",
       "    (1007, 1022, problem solving),\n",
       "    (1469, 1487, project management),\n",
       "    (1685, 1689, http)]}),\n",
       " ('data engineer company munich america service location new york united state data engineer munich america service data analytics division daa dedicated data asset creation data engineering data analytics shape coordinate munich agenda field advantage growing opportunity provided data analytics goal enable new better business data centric solution create new strategic opportunity data analytics excellence data asset team daa driver munich data agenda key partner service data data product strategically invest creation data asset source external data business central unit build accessible high quality data product exciting forefront digital transformation rapidly growing importance data mission enable organization generate additional line result leveraging power data regional analytics center north america looking data engineer drive creation valuable accessible trustful data product servicing north american business entity global specialty insurance gsi munich looking entrepreneurial product mindset curious committed excited shaping future munich technology data job drive munich data strategy implementing valuable accessible trustful data product work closely business central innovation team reinsurance specialty line understand data requirement work cross functional team quickly build mvp proving value new data product idea globally managed cloud native platform guidance mentoring outsourced implementation team define data model best suited identified use case build required data pipeline transform data right structure decide adequate technology chose option provided support use case leverage state art ai technology large language model structure unstructured data link additional external information needed monitor govern data quality requirement fill rate duplicate ratio update frequency etc provide meaningful data visualization closely work global sure data product mvp seamlessly integrated operational process later profile outstanding proficiency technical data handling e g writing sql python script manipulate data year excellent understanding cloud architecture data modeling data pipelining e g data engineer year understanding insurance market e g working closely business team insurance company year proven technical leadership skill agile setup year entrepreneurial customer centric mindset ability develop strategic target conceptually transform requirement solution integrative personality strong building maintaining relationship internally externally drive dedication creativity hand attitude initiate execute use case getting thing base salary range anticipated position plus opportunity company bonus based percentage eligible pay addition company make available variety benefit employee including health insurance coverage employee wellness program life disability insurance k match retirement saving plan paid holiday paid time pto apply',\n",
       "  {'entities': [(118, 127, analytics),\n",
       "    (171, 187, data engineering),\n",
       "    (193, 202, analytics),\n",
       "    (284, 293, analytics),\n",
       "    (386, 395, analytics),\n",
       "    (783, 792, analytics),\n",
       "    (1093, 1106, data strategy),\n",
       "    (1498, 1511, data pipeline),\n",
       "    (1686, 1695, data link),\n",
       "    (1750, 1762, data quality),\n",
       "    (1841, 1859, data visualization),\n",
       "    (2018, 2021, sql),\n",
       "    (2100, 2113, data modeling)]}),\n",
       " ('data engineer looking data engineer ground breaking exciting healthcare company carering health healthcare home leader dedicated bringing comprehensive coordinated ring care including home health home based primary care homecare hospice care chronically ill medicaid medicare eligible population innovative healthcare delivery model result better patient outcome bring health equity underserved population seeking talented passionate individual join mission help patient live happier healthier life want home offer know best place patient best place work employee offer following employee unique healthcare company help need help competitive pay paid weekly basis medical dental vision life insurance paid holiday pto k match career growth opportunity great collaborative work environment data pipeline management building data system pipeline combining raw information multiple source identifying opportunity data acquisition architecture design analyzing organizing raw data cloud based tool elt extract load transform process preparing data modeling building algorithm prototype data cleansing cloud management work multiple cloud based service tool hosted cloud provider data monitoring ensure data quality process routine optimized performance efficiency collaboration work data scientist data analyst multiple concurrent project qualification degree computer science information technology similar field master degree preferred year experience data engineer similar role knowledge cloud based service tool e azure aws snowflake fivetran dbt experience healthcare transaction data set including hl fhir use apis technical expertise data model data mining pipeline construction process automation data normalization knowledge programming language e sql java python json hand experience sql database design development proficiency microsoft suite including microsoft access proficiency data analytics tool e g powerbi data modeling data visualization technique familiarity robotic process automation experience mulesoft preferred strong communication skill attention detail eligibility certain benefit depend employment status carering health equal opportunity employer committed providing equal employment opportunity regard race color religion sex including pregnancy sexual orientation age national origin disability genetic information veteran status classification protected applicable law job type time education bachelor required ability commute louisville ky required work location person',\n",
       "  {'entities': [(789, 813, data pipeline management),\n",
       "    (823, 834, data system),\n",
       "    (910, 926, data acquisition),\n",
       "    (1039, 1052, data modeling),\n",
       "    (1062, 1071, algorithm),\n",
       "    (1082, 1096, data cleansing),\n",
       "    (1097, 1113, cloud management),\n",
       "    (1198, 1210, data quality),\n",
       "    (1356, 1372, computer science),\n",
       "    (1648, 1659, data mining),\n",
       "    (1701, 1719, data normalization),\n",
       "    (1753, 1756, sql),\n",
       "    (1757, 1761, java),\n",
       "    (1790, 1793, sql),\n",
       "    (1794, 1809, database design),\n",
       "    (1894, 1903, analytics),\n",
       "    (1921, 1934, data modeling),\n",
       "    (1935, 1953, data visualization)]}),\n",
       " ('chs inc leading global agribusiness owned farmer rancher cooperative united state provides grain food energy resource business consumer world serve agriculture customer consumer united state world employee united state today employee country chs creating connection empower agriculture chs inc data engineer expert location inver grove height mn job description data engineer expert position work chs information technology division business intelligence team apply knowledge enterprise resource planning integration bi technology including life cycle large complex enterprise platform upgrade implementation implementing solution according agile methodology managing task delivery multiple concurrent project reporting tool metadata creation specific duty include following designing developing testing information model hana bw hana platform interacting project united functional analyst enterprise information management enterprise solution architect ea business analyst ba gather high level business requirement translating functional specification technical specification conforming enterprise standard industry best practice creating database schema virtual model index compose view developing script working stakeholder identify establish data feed hana instance participating planning coordination release production ensure project timeline release schedule followed deadline ultimately met ensuring promotion content hana landscape sap solution manager participating user acceptance testing uat associated troubleshooting issue collaborating sap basis security team design implement analytic privilege secure limit access hana artifact assisting hana performance tuning query optimization effort creating extending semantically rich model cd view creating query advanced sql bw hana query position report worksite location incumbent work home united state job requirement qualified candidate bachelor degree foreign equivalent degree computer information system computer science computer engineering electrical engineering closely related field qualified candidate year month progressive post bachelor experience working sap bw sap hana qualified candidate year month experience following participating blueprint workshop session business process owner capture advanced sap bw hana modeling real time modeling requirement b debugging bw hana data model high performance stand point accommodate real time reporting scenario avoid data latency data redundancy data footprint c interacting team solution design live phase accurate integration team like system analyst business analyst end user group responsible end end testing application planning designing evaluating test approach test plan test procedure process unit integration performance user acceptance testing e translating functional specification technical specification conforming enterprise standard industry best practice f creating database schema physical model index compose view develop script g developing hana bw hana solution virtual persistent data modelling analytics cd view like basic composite consumption view table function amdp composite provider advanced dso open od view process chain development bw hana cockpit h performing work following tool technology sap hana sap netweaver bw sap bo sap ecc position report worksite location incumbent work home united state experience gained concurrently',\n",
       "  {'entities': [(433, 454, business intelligence),\n",
       "    (641, 658, agile methodology),\n",
       "    (995, 1015, business requirement),\n",
       "    (1140, 1155, database schema),\n",
       "    (1246, 1255, data feed),\n",
       "    (1481, 1499, acceptance testing),\n",
       "    (1780, 1783, sql),\n",
       "    (1971, 1987, computer science),\n",
       "    (1988, 2008, computer engineering),\n",
       "    (2451, 2466, data redundancy),\n",
       "    (2482, 2483, c),\n",
       "    (2634, 2654, application planning),\n",
       "    (2757, 2775, acceptance testing),\n",
       "    (2904, 2919, database schema),\n",
       "    (3038, 3047, analytics)]}),\n",
       " ('role data engineer location mossville il year remote identity access management iam salesforce description position member small team coordinate implement salesforce security risk assessment providing consulting service define design develop implement maintain security process technology system position monitor security risk aware security trend deliver appropriate risk mitigation proactive auditing technical skill required required previous experience salesforce administrator knowledge experience implementing security control salesforce experience cybersecurity concept threat monitoring privileged access identity access management security expertise salesforce com platform experience leading remediation effort complex landscape year experience overall cyber security working experience salesforce enterprise scale cloud security engineering experience working architectural design process procedure experience working closely salesforce information security tool process e g salesforce health check salesforce shield security center desired ranger level badge security administration salesforce trailhead training platform demonstrated experience communicating technical information business client experienced technologist cissp cism equivalent soft skill required required excellent communication skill incl documentation ability collaborate team ability influence experience agile methodology azure devops plus taking initiative self starter focus quality education requirement bachelor degree security focus preferred',\n",
       "  {'entities': [(1389, 1406, agile methodology), (1407, 1419, azure devops)]}),\n",
       " ('walmart advanced system amp robotics fast growing venture mission reinvent retail robotics partnered walmart develop alphabot technology currently deployed store north america data engineer service amp support team play hand role development data pipeline setting data warehouse create dashboard visualization goal data accessible enable team organization effectively use data identify bottle neck decision optimize site performance ideal candidate posse strong aptitude data enjoys problem solving able multitasks effectively manage priority incoming data request work create data model report amp analysis support business need convert raw data meaningful insight interactive easy understand dashboard report translate team need data solution visualization tool like power bi tableau recommend technical approach data pipeline development understand correct schema future proof data pipeline develop query report internal external stakeholder review improve optimize existing etl sql query dashboard view procedure implementing best practice data extraction storage work implementation technology needed facilitate transfer data integration internal party application propose define review test data warehouse modification fill identified data gap improve report design efficiency including custom table view looking data warehousing data mining experience hand experience extracting data data source building data model star schema snowflake experience requirement analysis design prototyping experience resolving complex issue creative efficient effective way strong working knowledge sql relational database experience cloud data platform experience azure plus demonstrated proficiency transferring data cross platform application ability write clean readable maintainable code documentation java python experience required excellent written amp oral communication skill strong organizational amp planning skill b computer science data engineering related field m plus year experience data visualization tool like power bi tableau year experience machine learning computer vision artificial intelligence equivalent master level course work acceptable walmart advanced system amp robotics recent award include exclusive benefit time team member comprehensive health care option choose range health plan tailored extend benefit dependent unlimited pto salaried employee receive unlimited paid time vacation holiday personal day eye amp dental care got family covered notch vision dental insurance secure future advantage competitive k matching stock purchase plan equity opportunity peace mind comprehensive life disability insurance option parental leave embrace joy parenthood week fully paid maternity paternity leave exclusive discount shop save special walmart discount store online dine amp energize enjoy daily complimentary lunch beverage variety snack fueled day join experience best employee care benefit learn named best place work alertinnovation com career alert innovation proud equal employment opportunity employer celebrate diversity committed creating inclusive environment employee discriminate based race religion color national origin gender including pregnancy childbirth related medical condition sexual orientation gender identity gender expression age status protected veteran status individual disability applicable legally protected characteristic',\n",
       "  {'entities': [(242, 255, data pipeline),\n",
       "    (286, 295, dashboard),\n",
       "    (296, 309, visualization),\n",
       "    (483, 498, problem solving),\n",
       "    (694, 703, dashboard),\n",
       "    (745, 758, visualization),\n",
       "    (769, 777, power bi),\n",
       "    (815, 828, data pipeline),\n",
       "    (880, 893, data pipeline),\n",
       "    (982, 985, sql),\n",
       "    (992, 1001, dashboard),\n",
       "    (1044, 1059, data extraction),\n",
       "    (1126, 1142, data integration),\n",
       "    (1192, 1201, test data),\n",
       "    (1319, 1335, data warehousing),\n",
       "    (1336, 1347, data mining),\n",
       "    (1456, 1476, requirement analysis),\n",
       "    (1589, 1592, sql),\n",
       "    (1604, 1612, database),\n",
       "    (1797, 1801, java),\n",
       "    (1919, 1935, computer science),\n",
       "    (1936, 1952, data engineering),\n",
       "    (1990, 2008, data visualization),\n",
       "    (2019, 2027, power bi),\n",
       "    (2085, 2108, artificial intelligence)]}),\n",
       " ('data center industry deployment operation maintenance decommissioning salute mission critical industry leader delivering global data center service executed military precision salute employee team member culture significant driver success able deliver client team oriented culture defined transparent communication collaborative development deployment procedure best practice customer service mindset internally externally strong commitment safety responsibility job description data center facility engineer responsible operational integrity commissioning regulatory compliance electrical power mechanical monitoring control system process maximize customer uptime cost effective way highly available concurrently maintainable fault tolerant mission critical data center environment facility engineer responsible ensuring electrical mechanical fire life safety equipment data center operating peak efficiency utmost level reliability involves planned preventative maintenance equipment daily corrective work response emergency issue facility engineer line come hand electrical mechanical equipment troubleshooting serf expert highly capable technical resource reporting site manager interacting onsite facility technician party vendor expected singular focal point facility operation given data center data center equipment support mission critical server maintain better uptime equipment includes limited standby diesel generator related fuel system electrical switchgear upss pdus ahus related thermal management equipment system chemical treatment system applicable pump motor vfds building automation system responsibility place safety fail focus task activity given remain consistently focused safety operate monitor maintain respond abnormal condition facility system complete planned ad hoc preventive maintenance work assignment requiring skill direction electrical mechanical maintenance trade engineering discipline conduct periodic inspection building system e hvac electrical distribution maintain mechanical equipment room enclosure ensure clean functional safe condition rapid response investigation management indoor emergency situation air quality complaint including humidity complete working knowledge creating closing tracking work request wr initiate respond work order follow verify completion work monitor performance note deficiency service contractor recommend change necessary provide applied electrical mechanical integrated control water chemistry technical expertise entire data center directly interface management team contractor consultant phased data center construction commissioning including integration testing new system maintaining critical system online work facility technician stakeholder track complete preventive predicative maintenance schedule critical maintenance system commissioning ensure data center operates maximum operational efficiency including analyzing existing operating condition recommending new technology improving overall efficiency cost reduction responsible data center white space house electrical loading capacity including client installation system position require unusual hour work approximately week meet critical maintenance window scheduling directly manage contractor consultant daily operation critical maintenance ups system generator switchgear chiller maintenance provide applied electrical mechanical integrated control technical expertise entire data center directly manage supervise facility technician aspect day day operation maintenance electrical mechanical critical facility equipment respond emergency situation data center perform related duty responsibility assigned management data center facility engineer interacts internal department including executive interact client vendor desired skill amp experience strong background operating mechanical electrical critical system design review layout installation including ability read interpret electrical mechanical line ability conduct power thermal system capacity safety code compliance assessment hand experience installing maintaining troubleshooting large commercial industrial electrical system including medium voltage switchgear volt volt electrical generation distribution system diesel powered ac generator static ups system applicable dynamic rotary ups system applicable multi string flooded cell battery monitoring system static switch plc relay logic control power monitoring system data center power distribution management system current knowledge nfpa electrical fire life safety building code local regulatory equivalent location data center knowledge nec nfpa e nfpa nfpa compliance issue expert knowledge industrial safety best practice e lockout tag arc flash protection osha state regulation excellent understanding electrical mechanical system involved critical data center operation including system feeder transformer generator switchgear ups system at unit pdu unit chiller pump air handling unit crac unit work directly customer internal team installation engineer technical consultant implementation personnel coordinate customer turn ups process flow ensure smooth transition installation operation qualification year degree engineering discipline accredited university use sight hearing voice required motor skill ability lift kg lb bend stoop reach overhead stand long time period minimum year directly related experience operation installation maintenance data center critical equipment building system depth technical understanding knowledge hvac electrical plumbing fire life safety control system knowledge electrical mechanical system data center environment including feeder transformer generator switchgear ups system at sts unit pdu pmm unit chiller air handling unit crac unit good observation skill problem solving ability military experience advantage schedule rotation night professional growth compensation trajectory restricted way consider stepping role nationally recognized leader mission critical market working employer maintains corporate philosophy hire nation best invests unilaterally committed continuing enrichment employee take work dynamic fast paced environment welcome opportunity provide detail explore current situation interest military experience preferred',\n",
       "  {'entities': [(0, 11, data center),\n",
       "    (128, 139, data center),\n",
       "    (479, 490, data center),\n",
       "    (760, 771, data center),\n",
       "    (872, 883, data center),\n",
       "    (1291, 1302, data center),\n",
       "    (1303, 1314, data center),\n",
       "    (2503, 2514, data center),\n",
       "    (2579, 2590, data center),\n",
       "    (2839, 2850, data center),\n",
       "    (3024, 3035, data center),\n",
       "    (3428, 3439, data center),\n",
       "    (3601, 3612, data center),\n",
       "    (3669, 3680, data center),\n",
       "    (4438, 4449, data center),\n",
       "    (4589, 4600, data center),\n",
       "    (4826, 4837, data center),\n",
       "    (5429, 5440, data center),\n",
       "    (5612, 5623, data center),\n",
       "    (5781, 5796, problem solving)]}),\n",
       " ('new reach education company based tempe arizona staffed elearning expert ushering forward information renaissance financial literacy building new emboldened class entrepreneur mission simple unlock financial freedom practical actionable education simply goal change world education order life better student business believe people invest money education come winner loser currently supporting thousand student new reach education making powerful lasting impact community serf job description ready embark exciting data driven journey unique blend analytical prowess technical wizardry help build data savvy future join team data maestro help elevate data game quest boost data maturity need blend daily support technical prowess happen data magician looking reading key responsibility data analytics dive lead score modeling play crucial role quantifying lead quality ensuring sale team insight need collaborate data analysis marketing sale team unearth trend providing valuable insight success data science amp engineering charge data engineering support function keeping snowflake database warehouse running smoothly maintenance flex technical muscle machine learning modeling simulation data data twist science engineering maturity system data maturity champion sale portal custom built web app heart data operation payment future payment insight seamlessly connect stripe payment application customize maintain enhance sale portal ui provide sale finance team treasure trove data reporting guru crafting report galore team data appetite eye payment progress check box create dazzling sale performance graph dashboard beautiful data insight qualification experience python experience etl tool curious mind love explore data uncover hidden gem experience data engineering modeling simulation knack balancing business analysis technical wizardry dash creativity customize enhance sale portal join dynamic team driving data transformation journey shape future data analytics science capability fun collaborative work environment room growth creativity ready center stage data maestro passionate data analytics art making complex thing look simple apply let data magic benefit paid time holiday health package medical dental vision optional telehealth support k company provided matching paid time holiday maternity paternity leave servant leadership team mentorship support continued development leadership focus group positive culture value hard work relationship volunteer opportunity community company event new reach education support diverse workforce equal opportunity employer discriminate individual basis race gender color religion national origin age sexual orientation gender identity disability veteran status classification protected law drug free workplace female minority encouraged apply',\n",
       "  {'entities': [(791, 800, analytics),\n",
       "    (913, 926, data analysis),\n",
       "    (996, 1008, data science),\n",
       "    (1032, 1048, data engineering),\n",
       "    (1084, 1092, database),\n",
       "    (1612, 1621, dashboard),\n",
       "    (1758, 1774, data engineering),\n",
       "    (1811, 1828, business analysis),\n",
       "    (1920, 1939, data transformation),\n",
       "    (1966, 1975, analytics),\n",
       "    (2101, 2110, analytics)]}),\n",
       " ('department engineering location cary north carolina united state product epic online service company epic game requisition id r make epic core epic success talented passionate people epic pride creating collaborative welcoming creative environment building award winning game crafting engine technology enables visually stunning interactive experience innovating epic mean team continually strives right community user constantly innovating raise bar engine game development ecosec ecosec team provides safer experience epic user work multiple product service improve technology craft transparent policy player user positive experience platform responsible designing building maintaining data infrastructure ensure reliability efficiency data system ecosystem security team role include building maintaining data pipeline transform load data product managing aws infrastructure machine learning platform additionally work engineer product manager data scientist design implement robust scalable data service support ecosystem security mission ensuring user privacy work directly combat bad actor platform safe user role interact product team understand safety system interact data system design implement automated end end etl process prepare data machine learning ad hoc analysis including data anonymization manage scale tool technology use label data run aws devise database structure technology storing efficiently accessing large data set million record different type text image video etc use implement data extraction apis support data versioning strategy automated tool dvc support devising strategy labeling new data human looking strong analytical background bsc msc computer science software engineering related subject candidate degree welcome long proven extensive hand experience experience etl technical design automated data quality testing qa documentation data warehousing data modeling experience python interaction web service e g rest postman experience developing data apis experience aws snowflake comparable large scale analytics platform experience monitoring managing database use elasticsearch mongodb postgresql experience sql experience data versioning tool experience developing maintaining data infrastructure etl pipeline apache airflow role open multiple location north america europe excluding ny wa epic job epic benefit epic life intent cover thing medically necessary improve quality life pay premium dependent coverage includes medical dental vision hra long term disability life insurance amp k competitive match offer robust mental program modern health provides free therapy coaching employee amp dependent year celebrate employee event company wide paid break offer unlimited pto sick time recognize individual year employment paid sabbatical epic game span country studio employee globally year making award winning game engine technology empowers visually stunning game content bring environment life like epic award winning unreal engine technology provides game developer ability build high fidelity interactive experience pc console mobile vr tool embraced content creator variety industry medium entertainment automotive architectural design continue build engine technology develop remarkable game strive build team world class talent like hear come epic epic game deeply value diverse team inclusive work culture proud equal opportunity employer note recruitment agency epic accept unsolicited resume approach unauthorized party including recruitment placement agency e party negotiated validly executed agreement pay fee unauthorized party',\n",
       "  {'entities': [(688, 707, data infrastructure),\n",
       "    (738, 749, data system),\n",
       "    (808, 821, data pipeline),\n",
       "    (1176, 1187, data system),\n",
       "    (1265, 1280, ad hoc analysis),\n",
       "    (1369, 1377, database),\n",
       "    (1509, 1524, data extraction),\n",
       "    (1677, 1693, computer science),\n",
       "    (1836, 1848, data quality),\n",
       "    (1874, 1890, data warehousing),\n",
       "    (1891, 1904, data modeling),\n",
       "    (2044, 2053, analytics),\n",
       "    (2094, 2102, database),\n",
       "    (2129, 2139, postgresql),\n",
       "    (2151, 2154, sql),\n",
       "    (2221, 2240, data infrastructure),\n",
       "    (2254, 2268, apache airflow)]}),\n",
       " ('data engineer based newark nj newnan ga boiulder co irving tx swedesboro nj come cookin hellofresh hellofresh want revolutionize way eat making convenient exciting cook meal scratch office world deliver delicious meal million people industry leader meal kit subscription service growing time distinct meal kit service cater menu variety market allows reach incredibly wide population people hellofresh team diverse high performing international work environment inspiring space thrive result job description data engineer work fulfillment planning technology team help build generation suite internal tool enable planning operation team act quickly changing business condition data engineer build scalable data pipeline infrastructure tool power product service work analyst engineer planner design build maintain efficient scalable reliable data pipeline support business critical need data ingestion processing analysis develop maintain efficient scalable reliable code python sql collaborate team member troubleshoot perform root cause analysis optimize existing data pipeline tool work team member ensure effective tool integration system workflow minimum bachelor master degree computer science engineering related field year data engineering experience fulfillment logistics supply chain production related field working physical good strong proficiency python sql experience working distributed system building maintaining pipeline cloud technology aws snowflake experience containerization orchestration docker kubernetes airflow gcp startup experience plus competitive salary amp k company match vest immediately participation generous parental leave week amp pto policy paid holiday monthly premium flexible health plan amazing discount including hellofresh subscription flexible shift scheduling amp advancement opportunity emergency child adult care service snack amp monthly catered lunch collaborative dynamic work environment fast paced mission driven company policy hellofresh discriminate employee applicant employment race color religion sex sexual orientation gender identity national origin age marital status genetic information disability protected veteran colorado pay range usd illinois pay range usd texas pay range usd newark nj pay range usd georgia pay range usd',\n",
       "  {'entities': [(706, 719, data pipeline),\n",
       "    (842, 855, data pipeline),\n",
       "    (887, 901, data ingestion),\n",
       "    (979, 982, sql),\n",
       "    (1066, 1079, data pipeline),\n",
       "    (1183, 1199, computer science),\n",
       "    (1231, 1247, data engineering),\n",
       "    (1367, 1370, sql),\n",
       "    (1439, 1455, cloud technology),\n",
       "    (1530, 1537, airflow)]}),\n",
       " ('introduction career opportunity data engineer want current employer exciting opportunity join valify nation leading provider healthcare service hca healthcare benefit valify offer total reward package support health life career retirement colleague available plan program include comprehensive medical coverage cover common service cost low copay plan include prescription drug behavioral health coverage free telemedicine service free airmed medical transportation additional option dental vision benefit life disability coverage flexible spending account supplemental health protection plan accident critical illness hospital indemnity auto home insurance identity theft protection legal counseling long term care coverage moving assistance pet insurance free counseling service resource emotional physical financial wellbeing k plan match pay based year service employee stock purchase plan hca healthcare stock family support fertility family building benefit progyny adoption assistance referral service child elder pet care home auto repair event planning consumer discount abenity consumer discount retirement readiness rollover assistance service preferred banking partnership education assistance tuition student loan certification support dependent scholarship colleague recognition program time away work program paid time paid family leave long short term disability coverage leaf absence employee health assistance fund offer free employee coverage time time colleague based income learn employee benefit note eligibility benefit vary location team committed caring group colleague want work data engineer passion creating positive patient interaction valued dedicated caring opportunity want knowledge expertise job summary qualification data engineer responsible loading processing client data helping internal data workflow role responsible supporting build automated solution related data pipeline analyzing larger complex data set role assist initial validation new data set monitor monthly incoming data feed timeliness consistency completeness process monthly data feed established slas great attention detail assist fulfillment client custom reporting need question use internal business intelligence tool pull report build view data carefully document new process request build manage test automation data ingestion work account manager business analytics team ensure client data concern addressed promptly self check work accuracy raise concern identify problem configure aws data pipeline automate current monthly import process automate reporting python cron job design implement bi solution internal external end user including kpi dashboard project summary report qualification need bachelor degree information system management equivalent minimum year data mining analytical report creation experience year experience microsoft excel regularly healthtrust supply chain critical hca healthcare strategy focus improve performance reduce cost joining non clinical administrative function healthtrust supply chain best practice methodology develop apply monitor cost efficient initiative program hca healthcare improving facility efficiency medical professional focus mission patient care hca healthcare recognized world ethical company ethisphere institute time recent year hca healthcare spent estimated billion cost delivery charitable care uninsured discount uncompensated expense brick mortar hospital people dr thomas frist sr hca healthcare co founder looking opportunity provides satisfaction personal growth encourage apply data engineer opening promptly review application highly qualified candidate contacted interview unlock possibility apply today equal opportunity employer value diversity company discriminate basis race religion color national origin gender sexual orientation age marital status veteran status disability status',\n",
       "  {'entities': [(1901, 1914, data pipeline),\n",
       "    (2018, 2027, data feed),\n",
       "    (2080, 2089, data feed),\n",
       "    (2200, 2226, business intelligence tool),\n",
       "    (2323, 2337, data ingestion),\n",
       "    (2359, 2377, business analytics),\n",
       "    (2499, 2512, data pipeline),\n",
       "    (2658, 2667, dashboard),\n",
       "    (2780, 2791, data mining)]}),\n",
       " ('mayo clinic mayo clinic ranked specialty care provider according u news amp world report work need patient dedicated employee investing competitive compensation comprehensive benefit plan care family future continuing education advancement opportunity turn build long successful career mayo clinic thrive environment support innovation committed ending racism supporting diversity equity inclusion provides resource need succeed responsibility time remote position united state clinical specialty informatic unit section hospital surgical system departmental system division looking developer join medical informatic team data engineer medical informatic team mainly develops maintain analytical infrastructure enables function data world support anesthesiology operation room mayo clinic enterprise successful candidate collaborating engineer analyze integration transformation data set develop test deploy new data pipeline wide variety data source format premise cloud technology support analytics machine learning application solution assigned product team framework open source programming language meet desired design functionality product program candidate strong sql etl skill tableau experience able work collaboratively stakeholder create visualization represent needed analysis candidate responsible development test maintenance optimal data pipeline architecture large scale medical related database gcp big query state art system candidate involving creating maintaining data set process verification acquisition mining modeling clinical data micro service api ability perform root cause analysis external internal process data identify opportunity improvement answer question excellent analytic skill associated working structure unstructured datasets ability build process support data transformation workload management data structure dependency metadata gcp develop test large complex data set meet functional non functional business requirement continues build knowledge organization process customer performs range mainly straightforward assignment mayo clinic sponsor transfer visa position including f opt stem qualification required qualification bachelor degree computer science engineering accredited university college associate degree computer science engineering accredited university college year experience demonstrated ability analyze profile data mean address business problem leveraging advanced data modeling source system database data mining technique required provide consultative service department division committee demonstrated application problem solving methodology planning technique continuous improvement method analytical tool methodology e g data analysis data profiling modeling etc required incumbent ability manage varied workload project multiple priority stay current healthcare trend enterprise change interpersonal skill time management skill required requires strong analytical skill ability identify recommend solution advanced computer application skill commitment customer service experience data analysis quality profiling including data exploration tool including limited rapid sql aqt information analyzer informatics n preferred qualification working experience data engineering minimum year experience data engineering data science analytical modeling experience scripting language python javascript experience leveraging micro service high level language c c java data access analytics exemption status exempt compensation detail year schedule time hour pay period schedule detail pm monday friday weekend schedule n international assignment site description reputation spread minnesota root location today employee located major campus phoenix scottsdale arizona jacksonville florida rochester minnesota mayo clinic health system campus midwestern community international location mayo clinic location special place employee thrive work personal life learn unique mayo clinic campus offer best fit affirmative action equal opportunity employer affirmative action equal opportunity employer mayo clinic committed creating inclusive environment value diversity employee discriminate employee candidate woman minority veteran people lgbtq community people disability strongly encouraged apply join team reasonable accommodation access job opening apply job available recruiter ted keefe',\n",
       "  {'entities': [(912, 925, data pipeline),\n",
       "    (966, 982, cloud technology),\n",
       "    (991, 1000, analytics),\n",
       "    (1171, 1174, sql),\n",
       "    (1249, 1262, visualization),\n",
       "    (1348, 1361, data pipeline),\n",
       "    (1403, 1411, database),\n",
       "    (1774, 1787, build process),\n",
       "    (1796, 1815, data transformation),\n",
       "    (1836, 1850, data structure),\n",
       "    (1942, 1962, business requirement),\n",
       "    (2185, 2201, computer science),\n",
       "    (2261, 2277, computer science),\n",
       "    (2428, 2441, data modeling),\n",
       "    (2456, 2464, database),\n",
       "    (2465, 2476, data mining),\n",
       "    (2580, 2595, problem solving),\n",
       "    (2689, 2702, data analysis),\n",
       "    (2703, 2717, data profiling),\n",
       "    (3050, 3063, data analysis),\n",
       "    (3138, 3141, sql),\n",
       "    (3224, 3240, data engineering),\n",
       "    (3265, 3281, data engineering),\n",
       "    (3282, 3294, data science),\n",
       "    (3419, 3420, c),\n",
       "    (3421, 3422, c),\n",
       "    (3423, 3427, java),\n",
       "    (3428, 3439, data access),\n",
       "    (3440, 3449, analytics)]}),\n",
       " ('data engineer corporate headquarters uline drive pleasant prairie wi uncover potential collaborative environment design develop deliver custom solution big challenge proven industry leader run largest e commerce site u better apart position site looking good people share passion position responsibility uline data analytics engineering team work closely business customer development new enterprise analytics platform leverage industry leading integration analytics tool build modern data warehouse accurate reliable high performing easily accessible business department responsible life cycle development requirement gathering etl coding report dashboard design creation work business user establish reporting analytic requirement translate business requirement etl report specification provide technical business knowledge support team compile ad hoc data report request minimum requirement bachelor degree related major year related experience excellent knowledge data design sql data warehousing knowledge ibm cognos bi plus strong knowledge microsoft bi technology ssrs ssis ssa available travel uline domestic international branch benefit complete medical dental vision life insurance coverage flexible spending account wellness program k employer match paid holiday generous paid time tuition assistance program cover professional continuing education bonus program include annual performance sale goal profit sharing employee perk site caf executive chef seasonal dinner option class fitness center complimentary personal trainer mile beautifully maintained walking trail uline uline north america leading distributor shipping industrial packaging material family owned company known incredible service quality product day shipping huge stock inventory employee location time joined uline uline proud operate drug free workplace new hire complete pre employment hair follicle drug screening eeo aa employer vet disabled zr li mw corp ppit employee difference committed offering exceptional benefit perk explore http www uline job working blankuline job learn',\n",
       "  {'entities': [(315, 324, analytics),\n",
       "    (400, 409, analytics),\n",
       "    (457, 466, analytics),\n",
       "    (647, 656, dashboard),\n",
       "    (743, 763, business requirement),\n",
       "    (980, 983, sql),\n",
       "    (984, 1000, data warehousing),\n",
       "    (2020, 2024, http)]}),\n",
       " ('data engineer stanley seattle wa stanley havi company defined creativity building invention maker legendary bottle box driven purpose passion performance obsessed making difference keeping promise proud yesterday focused building team tomorrow position overview stanley havi company looking data engineer support current future data platform role responsible ensuring data available consumable business analyst key business user includes responsibility ingestion source data ci cd pipeline member enterprise data platform team contribute project initiative enterprise key responsibility develop end end solution ingestion storage prepping modeling data test audit maintain tune existing solution implement data structure data modeling elt etl process sql technology monitor daily data load remedy issue sla update extend system documentation regularly interact functional area ensure objective met exercise independent judgment method technique evaluation criterion obtaining result daily interaction cloud based data platform service solid understanding data architecture principle practice working knowledge data warehousing concept supporting performant access critical business data familiarity tabular modeling define data relationship navigation taxonomy experience coding language like sql python r education experience bachelor degree computer engineering computer science mathematics equivalent related specific experience year data engineer experience required preferably cloud environment experience database query analysis language e g sql pl sql r sa python data visualization tool e g powerbi tableau experience working data source e g sql oracle database flat file web api xml experience cloud warehouse analytics required azure data storage analytics e g data lake data factory synapse analysis service experience oracle eb preferred experience synapse ml preferred experience multi tenant infrastructure preferred understanding manufacturing supply chain inventory management sale operation data system preferred demonstrable ability communicate partner deliver solution business customer puget sound based hybrid role seattle office unable provide sponsorship role stanley committed diverse inclusive workplace stanley equal opportunity employer discriminate based race national origin gender gender identity sexual orientation protected veteran status disability age legally protected status individual disability like request accommodation reach recruiting pmi worldwide com base pay range position successful candidate state listed successful candidate actual pay based multiple factor work location job related knowledge skill qualification experience stanley total reward company includes reward base salary stanley time employee eligible annual bonus based company individual result addition offer variety employee benefit personalized time paid holiday dental vision k havi havi global privately owned company connects people idea data insight supply demand restaurant delivery ultimately people product love sourcing storing delivering product bring unmatched category expertise unrivaled operational excellence combined powerful digital analytics insight founded havi employ people serf customer country havi business unit include supply chain tm stanley portfolio business offer best class sourcing supply chain capability brand defining marketing promotion service innovative consumer product information visit havi com tmsw com stanley com',\n",
       "  {'entities': [(475, 480, ci cd),\n",
       "    (706, 720, data structure),\n",
       "    (721, 734, data modeling),\n",
       "    (751, 754, sql),\n",
       "    (1055, 1072, data architecture),\n",
       "    (1110, 1126, data warehousing),\n",
       "    (1293, 1296, sql),\n",
       "    (1343, 1363, computer engineering),\n",
       "    (1364, 1380, computer science),\n",
       "    (1511, 1525, database query),\n",
       "    (1548, 1551, sql),\n",
       "    (1552, 1558, pl sql),\n",
       "    (1571, 1589, data visualization),\n",
       "    (1650, 1653, sql),\n",
       "    (1654, 1669, oracle database),\n",
       "    (1719, 1728, analytics),\n",
       "    (1744, 1756, data storage),\n",
       "    (1757, 1766, analytics),\n",
       "    (1771, 1780, data lake),\n",
       "    (2008, 2019, data system),\n",
       "    (3164, 3173, analytics)]}),\n",
       " ('publicis sapient looking data engineer team notch technologist lead deliver technical solution large scale digital transformation project working latest data technology industry instrumental helping client evolve digital future daily duty amp impact combine technical expertise problem solving passion work closely client turning complex idea end end solution transform client business lead design develop deliver large scale data system data processing data transformation project delivers business value client automate data platform operation manage post production system process conduct technical feasibility assessment provide project estimate design development solution provide technical input agile process epic story task definition resolve issue remove barrier lifecycle client engagement creation maintenance infrastructure code cloud prem hybrid environment tool terraform cloudformation azure resource manager helm google cloud deployment manager mentor help grow junior team member qualification technical skill amp experience demonstrable experience data platform involving implementation end end data pipeline hand experience leading public cloud data platform azure aws google cloud implementation experience column oriented database technology e big query redshift vertica nosql database technology e dynamodb bigtable cosmos db etc traditional database system e sql server oracle mysql experience implementing data pipeline streaming batch integration tool framework like azure data factory glue etl lambda spark spark streaming etc ability handle module track level responsibility contributing task hand experience data modeling warehouse design fact dimension implementation experience working code repository continuous integration data modeling querying optimization relational nosql timeseries graph database data warehouse data lake data processing programming sql dbt python similar tool logical programming python spark pyspark java javascript scala data ingest validation enrichment pipeline design implementation cloud native data platform design focus streaming event driven architecture test programming automated testing framework data validation quality framework data lineage framework metadata definition management data catalog service catalog stewardship tool openmetadata datahub alation aws glue catalog google data catalog similar code review mentorship bachelor degree computer science engineering related field set apart developer certification cloud service like azure aws google cloud snowflake understanding development project methodology willingness travel additional information additional information pay range benefit working flexible vacation policy time limited allocated accrued paid holiday year generous parental leave new parent transition program tuition reimbursement corporate gift matching program additional information company description publicis sapient digital transformation partner helping established organization future digitally enabled state way work way serve customer help unlock value start mindset modern method fusing strategy consulting customer experience agile engineering problem solving creativity united core value purpose helping people thrive brave pursuit people office world combine experience truly value',\n",
       "  {'entities': [(278, 293, problem solving),\n",
       "    (426, 437, data system),\n",
       "    (438, 453, data processing),\n",
       "    (454, 473, data transformation),\n",
       "    (1113, 1126, data pipeline),\n",
       "    (1243, 1251, database),\n",
       "    (1292, 1297, nosql),\n",
       "    (1298, 1306, database),\n",
       "    (1329, 1337, bigtable),\n",
       "    (1364, 1379, database system),\n",
       "    (1382, 1385, sql),\n",
       "    (1400, 1405, mysql),\n",
       "    (1430, 1443, data pipeline),\n",
       "    (1492, 1510, azure data factory),\n",
       "    (1636, 1649, data modeling),\n",
       "    (1732, 1754, continuous integration),\n",
       "    (1755, 1768, data modeling),\n",
       "    (1802, 1807, nosql),\n",
       "    (1819, 1833, graph database),\n",
       "    (1849, 1858, data lake),\n",
       "    (1859, 1874, data processing),\n",
       "    (1887, 1890, sql),\n",
       "    (1948, 1955, pyspark),\n",
       "    (1956, 1960, java),\n",
       "    (2164, 2179, data validation),\n",
       "    (2327, 2335, aws glue),\n",
       "    (2372, 2383, code review),\n",
       "    (2411, 2427, computer science),\n",
       "    (3153, 3168, problem solving)]}),\n",
       " ('join leading fintech company democratizing finance robinhood founded simple idea financial market accessible customer heart decision robinhood lowering barrier providing greater access financial information building product service help create financial system participate continue build seeking curious growth minded thinker help shape vision structure system playing key role launch ambitious future invigorated mission value drive change world love apply team preferred location position robinhood office menlo park new york ny office work capability required management robinhood metric driven company data foundational key decision growth strategy product optimization day day operation looking data engineer build maintain foundational datasets allow reliably efficiently power decision making robinhood datasets include application event database snapshot derived datasets describe track robinhood key metric product partner closely engineer data scientist business team power analytics experimentation machine learning use case fast paced team fast growing company unique opportunity help lay foundation reliable impactful data driven decision company year come role located office location listed job description align office working environment connect recruiter information office philosophy expectation day day help define build key datasets robinhood product area lead evolution datasets use case grow build scalable data pipeline python spark airflow data different application data lake partner upstream engineering team enhance data generation pattern partner data consumer robinhood understand consumption pattern design intuitive data model ideate contribute shared data engineering tooling standard define promote data engineering best practice company year professional experience building end end data pipeline proven ability implement software engineering caliber code preferably python expert building maintaining large scale data pipeline open source framework spark flink etc strong sql presto spark sql etc skill experience solving problem data stack data infrastructure analytics visualization platform expert collaborator ability democratize data actionable insight solution bonus point passion working learning fast growing company expected salary range role based location work performed aligned compensation zone role eligible participate robinhood bonus plan robinhood equity plan zone zone zone base pay successful applicant depend variety job related factor include education training experience location business need market demand view comp zone office location table location listed compensation discussed recruiter interview process office location comp zone zone menlo park new york ny seattle wa washington c zone denver co westlake dallas tx chicago il zone lake mary fl click learn robinhood benefit robinhood promotes diversity provides equal opportunity applicant employee dedicated building company represents variety background perspective skill believe inclusive better work work environment additionally robinhood provides reasonable accommodation candidate request respect applicant privacy right review robinhood privacy policy visit robinhood applicant privacy policy applicant located uk eea visit robinhood uk eea applicant privacy policy',\n",
       "  {'entities': [(845, 853, database),\n",
       "    (984, 993, analytics),\n",
       "    (1430, 1443, data pipeline),\n",
       "    (1457, 1464, airflow),\n",
       "    (1480, 1496, application data),\n",
       "    (1684, 1700, data engineering),\n",
       "    (1733, 1749, data engineering),\n",
       "    (1818, 1831, data pipeline),\n",
       "    (1949, 1962, data pipeline),\n",
       "    (2008, 2011, sql),\n",
       "    (2025, 2028, sql),\n",
       "    (2077, 2096, data infrastructure),\n",
       "    (2097, 2106, analytics),\n",
       "    (2107, 2120, visualization),\n",
       "    (2747, 2748, c)]}),\n",
       " ('location designation hybrid join new york life joining company value career development collaboration innovation inclusiveness want employee feel proud company committed right thing opportunity grow career developing personally professionally resource program new york life relationship based company appreciates virtual person interaction support culture senior insight data engineer passionate digging data understanding story data telling equally passionate helping solve problem gain insight decision data work solidify enhance support data quality analytics dashboard report actively partner corporate data strategy governance corporate technology division design implement solution extraction integration data data warehouse data mart data lake purpose reporting decision support driving insight identify investigate resolve data discrepancy finding root cause issue work partner cross functional team prevent future occurrence proactively look opportunity optimize data loading structure develop new approach improve onboarding integrity data provide basic reporting respond inquiry asking insight data set stakeholder timely fashion data ensure data clean consistent synchronized platform oversee design implantation data cleansing procedure represent area project key player meeting level management act mentor team member foundational business technical issue experience proficient knowledge sql experience working redshift hadoop ability interpret data help strategic decision making excellent problem solving critical thinking skill required ability clearly articulate present idea writing verbally level nylife experience manipulating large data set leveraging tool python amp r knowledge etl process solid proficiency microsoft office application expert excel skill expertise root cause analysis resolving performance bottleneck professional positive demeanor li em li remote salary range overtime eligible exempt discretionary bonus eligible yes sale bonus eligible click learn benefit starting salary dependent factor including previous work experience specific industry experience skill required recognized fortune world admired company new york life committed improving local community culture employee giving volunteerism supported foundation proud mutuality operate best interest policy owner invite bring talent new york life continue help family business good life learn visit linkedin newsroom career page www newyorklife com job requisition id',\n",
       "  {'entities': [(540, 552, data quality),\n",
       "    (553, 562, analytics),\n",
       "    (563, 572, dashboard),\n",
       "    (607, 620, data strategy),\n",
       "    (731, 740, data mart),\n",
       "    (741, 750, data lake),\n",
       "    (1225, 1239, data cleansing),\n",
       "    (1402, 1405, sql),\n",
       "    (1505, 1520, problem solving)]}),\n",
       " ('new reach education company based tempe arizona staffed elearning expert ushering forward information renaissance financial literacy building new emboldened class entrepreneur mission simple unlock financial freedom practical actionable education simply goal change world education order life better student business believe people invest money education come winner loser currently supporting thousand student new reach education making powerful lasting impact community serf job description ready embark exciting data driven journey unique blend analytical prowess technical wizardry help build data savvy future join team data maestro help elevate data game quest boost data maturity need blend daily support technical prowess happen data magician looking reading key responsibility data analytics dive lead score modeling play crucial role quantifying lead quality ensuring sale team insight need collaborate data analysis marketing sale team unearth trend providing valuable insight success data science amp engineering charge data engineering support function keeping snowflake database warehouse running smoothly maintenance flex technical muscle machine learning modeling simulation data data twist science engineering maturity system data maturity champion sale portal custom built web app heart data operation payment future payment insight seamlessly connect stripe payment application customize maintain enhance sale portal ui provide sale finance team treasure trove data reporting guru crafting report galore team data appetite eye payment progress check box create dazzling sale performance graph dashboard beautiful data insight qualification experience python experience etl tool curious mind love explore data uncover hidden gem experience data engineering modeling simulation knack balancing business analysis technical wizardry dash creativity customize enhance sale portal join dynamic team driving data transformation journey shape future data analytics science capability fun collaborative work environment room growth creativity ready center stage data maestro passionate data analytics art making complex thing look simple apply let data magic benefit paid time holiday health package medical dental vision optional telehealth support k company provided matching paid time holiday maternity paternity leave servant leadership team mentorship support continued development leadership focus group positive culture value hard work relationship volunteer opportunity community company event new reach education support diverse workforce equal opportunity employer discriminate individual basis race gender color religion national origin age sexual orientation gender identity disability veteran status classification protected law drug free workplace female minority encouraged apply',\n",
       "  {'entities': [(791, 800, analytics),\n",
       "    (913, 926, data analysis),\n",
       "    (996, 1008, data science),\n",
       "    (1032, 1048, data engineering),\n",
       "    (1084, 1092, database),\n",
       "    (1612, 1621, dashboard),\n",
       "    (1758, 1774, data engineering),\n",
       "    (1811, 1828, business analysis),\n",
       "    (1920, 1939, data transformation),\n",
       "    (1966, 1975, analytics),\n",
       "    (2101, 2110, analytics)]}),\n",
       " ('description data engineer responsible creation ongoing maintenance internal external data pipeline service includes timely resolution data issue customer ad hoc data request assigned data architect leadership team collaborate product owner operation process owner identify opportunity define business requirement assist designing implementing solution assist reporting critical process system responsibility life cycle project analysis development delivery solution modify existing process improve operational efficiency entire data processing team perform data mining aggregation combining multiple datasets develop streamlined efficient solution internal external party perform data validation analysis ensure accuracy quality data handle data conversion data cleansing report generation scheduled data delivery standard ability deliver data feature based operation leadership requirement requirement required experience year data engineering experience relative analytical engineering experience year sql mssql relative sql syntax advanced trigger procedure managed statement understanding dimensional programming year python java experience experience power bi google data studio looker tableau experience version control git experience automated infrastructure infrastructure code e ansible terraform understanding sdlc secure programming skill strong understanding cloud platform azure amp gcp google cloud platform experience data warehouse environment azure synapse google bigquery aws redshift snowflake etc education bachelor degree related field equivalent experience required preferred experience skill ability experience machine learning infrastructure modeling experience training routine creation physical technical environment noise level work environment moderate ability maintain focus high level pressure multiple priority',\n",
       "  {'entities': [(85, 98, data pipeline),\n",
       "    (292, 312, business requirement),\n",
       "    (528, 543, data processing),\n",
       "    (557, 568, data mining),\n",
       "    (680, 695, data validation),\n",
       "    (741, 756, data conversion),\n",
       "    (757, 771, data cleansing),\n",
       "    (928, 944, data engineering),\n",
       "    (1004, 1007, sql),\n",
       "    (1023, 1026, sql),\n",
       "    (1129, 1133, java),\n",
       "    (1156, 1164, power bi),\n",
       "    (1481, 1489, bigquery)]}),\n",
       " ('american express know right backing people business power progress incredible way supporting customer financial confidence ahead taking commerce new height encouraging people explore world colleague constantly redefining possible proud step way join team amex diverse community colleague common goal deliver exceptional customer experience day learn grow champion meaningful career journey program benefit flexibility personally professionally colleague share company success win team striving uphold company value powerful backing promise customer community day integrity environment seen heard feel like truly belong join teamamex let lead way diverse tech team architect code ship software make essential customer digital life work alongside talented engineer open supportive inclusive environment voice valued decision tech use solve challenging problem amex offer range opportunity work latest technology encourages broader engineering community open source understand importance keeping skill fresh relevant dedicated time invest professional development find place technology teamamex data engineer responsible build core feature function card transaction system distributed platform deployed hybrid cloud senior engineer american express individual contributor role reporting director engineering responsibility responsible design build distributed data processing analytical system build high level design detailed design subsystem feature emphasis performant code build code feature working developer day day activity helping code sdlc task build pocs validate new concept new technology constantly purse learn industry leading innovative technology solution acutely aware enabling technology open source product build low latency distributed system lead culture innovation experimentation engage fun outcome oriented culture ready try new concept fear failure collaborate peer technology development team different location qualification year work experience software design implementation java scala experience data processing spark knowledge designing implementing operating nosql database cassandra elasticsearch preferred qualification experience distributed data processing analyzing cassandra elasticsearch spark experience distributed messaging system kafka experience building micro service service mesh plus experience cloud platform like docker kubernetes openshift plus experience continuous integration continuous delivery devops system experience architecting large scale distributed data system considering scalability reliability security performance flexibility clear understanding design pattern threading memory model supported language vm able mentor provide technical guidance engineer excellent written verbal communication skill create deliver effective presentation senior leadership salary range annually bonus benefit represents expected salary range job requisition ultimately determining pay consider location experience job related factor american express equal opportunity employer make employment decision regard race color religion sex sexual orientation gender identity national origin veteran status disability status age status protected law colleague support need thrive professionally personally amex flex enterprise working model provides greater flexibility colleague ensuring preserve important aspect unique person culture depending role business need colleague work onsite hybrid model combination office virtual day fully virtually job seeker employee click view eeo law poster supplement pay transparency policy statement link work copy paste following url new browser window http www dol gov agency ofccp poster access poster ghc shpe swe shpe afrotech',\n",
       "  {'entities': [(1357, 1372, data processing),\n",
       "    (1649, 1668, technology solution),\n",
       "    (2001, 2005, java),\n",
       "    (2023, 2038, data processing),\n",
       "    (2088, 2093, nosql),\n",
       "    (2094, 2102, database),\n",
       "    (2174, 2189, data processing),\n",
       "    (2403, 2425, continuous integration),\n",
       "    (2446, 2452, devops),\n",
       "    (2508, 2519, data system),\n",
       "    (3630, 3634, http)]}),\n",
       " ('crimson wine group guardian acre pristine vineyard iconic estate forest wildlife habitat west coast enviable portfolio brand includes pine ridge vineyard napa calif seghesio family vineyard healdsburg calif archery summit dayton ore chamisal vineyard san luis obispo calif double canyon west richland wash seven hill winery walla walla wash malene wine santa barbara county calif committed creating workplace culture celebrates diversity equity inclusion make little different lot better success informed wide range experience perspective team brings wine vine bottle encourage applicant vision creating equitable inclusive diverse wine industry apply join team information visit www crimsonwinegroup com position summary member team data engineer lead role designing implementing maintaining data system data related process support internal business team customer partner realm includes data warehouse database integration analytics reporting tool data wrangling remediation associated process right individual experience stage database project work requirement gathering logical physical design implementation testing deployment expertise technology needed implement robust data warehouse business intelligence solution seeking passionate answering question data ambition help team company level cwg regulated environment strong understanding compliance security control key essential duty amp responsibility following reflects management definition essential function job restrict task assigned management assign reassign duty responsibility job time reasonable accommodation reason collaborates business stakeholder design implement data warehouse reporting analytics integration solution align technology roadmap data architecture defines document socializes procedure standard ensure data handling meet objective data quality business process enablement compliance design build workflow tool integration data structure improve data accuracy availability usability identifies design implement internal process improvement automating manual process optimizing data delivery leverage integration platform service assemble complex data set meet end user business requirement build analytical tool provide actionable insight key business performance metric design implement maintains integration internal rd party system monitor ensures timely data flow related business facing solution crm erp sox application cwg environment establishes maintains procedure faq sop documentation required internal external audit end user support qualification bachelor degree business finance economics technology computer science statistic analytics data science mathematics preferred equivalent work experience considered year experience similar role ability organized perform minimal supervision remote work environment maintaining high attention detail essential demonstrated ability successfully manage multiple project workflow hand experience microsoft azure data storage integration service including azure data lake azure sql database ssis powerbi modern data warehouse platform e g azure snowflake integration platform e g mulesoft webmethods celigo informatica programming language e g python r sql web apis e g soap rest odata service oriented personality positive attitude passion data delivering creative solution excellent interpersonal communication skill ability work level personnel experience working regulated e g sox environment preferred applicant receive consideration employment regard federally state protected class require assistance participate application process contact hr crimsonwinegroup com projected base pay range position k k year addition annual bonus projected compensation position actual compensation offered vary based job related factor limited candidate qualification related experience education candidate work location market data crimson wine group reserve right modify pay range rate time future',\n",
       "  {'entities': [(793, 804, data system),\n",
       "    (904, 912, database),\n",
       "    (925, 934, analytics),\n",
       "    (950, 964, data wrangling),\n",
       "    (1030, 1038, database),\n",
       "    (1192, 1213, business intelligence),\n",
       "    (1663, 1672, analytics),\n",
       "    (1719, 1736, data architecture),\n",
       "    (1820, 1832, data quality),\n",
       "    (1911, 1925, data structure),\n",
       "    (2157, 2177, business requirement),\n",
       "    (2601, 2617, computer science),\n",
       "    (2628, 2637, analytics),\n",
       "    (2638, 2650, data science),\n",
       "    (2937, 2952, microsoft azure),\n",
       "    (2953, 2965, data storage),\n",
       "    (2996, 3011, azure data lake),\n",
       "    (3018, 3021, sql),\n",
       "    (3022, 3030, database),\n",
       "    (3193, 3196, sql)]}),\n",
       " ('job title data engineer job location san francisco mandatory skill python databricks pyspark aws sql job description year customer facing technical architecture consulting role expertise following technology developing modern data warehouse solution databricks aws azure stack drive technical discussion client architect team member knowledge databricks delta lake analytical data lake use case hand experience create mlops data pipeline creation aiml model develop train implement aiml use case knowledge banking domain card payment area relation database data classification data profiling mlops use case good experience offshore onsite coordination experience translating customer business need job type time salary year experience level year schedule hour shift work location person',\n",
       "  {'entities': [(74, 84, databricks),\n",
       "    (85, 92, pyspark),\n",
       "    (97, 100, sql),\n",
       "    (250, 260, databricks),\n",
       "    (343, 353, databricks),\n",
       "    (376, 385, data lake),\n",
       "    (424, 437, data pipeline),\n",
       "    (548, 556, database),\n",
       "    (557, 576, data classification),\n",
       "    (577, 591, data profiling)]}),\n",
       " ('ag grace looking talented data engineer support acquisition mission critical mission support data set preferred candidate background supporting cyber network related mission military space developer analyst engineer work performed customer site columbia md onsite essential job responsibility ideal candidate worked big data system complex structured unstructured data set supported government data acquisition analysis sharing effort past excel position candidate shall strong attention detail able understand technical complexity willingness learn adapt situation candidate work independently large team accomplish client objective',\n",
       "  {'entities': [(316, 324, big data), (394, 410, data acquisition)]}),\n",
       " ('overview overview today rapidly evolving technology landscape organization data important aspect achieving mission business goal data exploitation expert work client support mission business goal creating executing comprehensive data strategy best technology technique given challenge client data strategic asset looking fact based data driven customer focused organization help realize goal leveraging visual analytics platform analyze visualize share information steampunk design develop solution high impact complex data problem working best data practitioner data exploitation approach tightly integrated human centered design devsecops contribution contribution looking seasoned data engineer work team client develop enterprise grade data platform service pipeline looking data engineer technologist excellent communication customer service skill passion data problem solving lead architect migration data environment performance reliability ass understand etl job workflow bi tool report address technical inquiry concerning customization integration enterprise architecture general feature functionality data product experience crafting database data warehouse solution cloud preferably aws alternatively azure gcp key skill set python aws support agile software development lifecycle contribute growth data exploitation practice qualification qualification citizen ability hold position public trust government year industry experience coding commercial software passion solving complex problem year direct experience data engineering experience tool big data tool hadoop spark kafka etc relational sql nosql database including postgres cassandra data pipeline workflow management tool azkaban luigi airflow etc aws cloud service ec emr rds redshift azure equivalent data streaming system storm spark streaming etc search tool solr lucene elasticsearch object oriented object function scripting language python java c scala etc advanced working sql knowledge experience working relational database query authoring optimization sql working familiarity variety database experience message queuing stream processing highly scalable big data data store experience manipulating processing extracting value large disconnected datasets experience manipulating structured unstructured data analysis experience constructing complex query analyze result database data processing development environment experience data modeling tool process experience architecting data system transactional warehouse experience aggregating result compiling information reporting multiple datasets experience working agile environment experience supporting project team developer data scientist build web based interface dashboard report analytics machine learning model steampunk steampunk change agent federal contracting industry bringing new thinking client homeland federal civilian health dod sector human centered delivery methodology fundamentally changing expectation federal client true shared accountability solving toughest mission challenge employee owned company focus investing employee enable greatest work career rewarding outstanding contribution growth want learn story visit http www steampunk com equal opportunity employer qualified applicant receive consideration employment regard race color religion sex national origin disability status protected veteran status characteristic protected law steampunk participates e verify program',\n",
       "  {'entities': [(129, 146, data exploitation),\n",
       "    (229, 242, data strategy),\n",
       "    (403, 419, visual analytics),\n",
       "    (563, 580, data exploitation),\n",
       "    (866, 881, problem solving),\n",
       "    (1145, 1153, database),\n",
       "    (1256, 1282, agile software development),\n",
       "    (1311, 1328, data exploitation),\n",
       "    (1527, 1543, data engineering),\n",
       "    (1560, 1568, big data),\n",
       "    (1608, 1611, sql),\n",
       "    (1612, 1617, nosql),\n",
       "    (1618, 1626, database),\n",
       "    (1656, 1669, data pipeline),\n",
       "    (1709, 1716, airflow),\n",
       "    (1776, 1790, data streaming),\n",
       "    (1920, 1924, java),\n",
       "    (1925, 1926, c),\n",
       "    (1954, 1957, sql),\n",
       "    (1998, 2012, database query),\n",
       "    (2036, 2039, sql),\n",
       "    (2068, 2076, database),\n",
       "    (2138, 2146, big data),\n",
       "    (2147, 2157, data store),\n",
       "    (2286, 2299, data analysis),\n",
       "    (2353, 2361, database),\n",
       "    (2362, 2377, data processing),\n",
       "    (2413, 2426, data modeling),\n",
       "    (2464, 2475, data system),\n",
       "    (2703, 2712, dashboard),\n",
       "    (2720, 2729, analytics),\n",
       "    (3177, 3181, http)]}),\n",
       " ('join amazing company globally recognized brand dive intriguing project level skill work fun team onsite scottsdale arizona ready embark exciting challenge field data engineering posse genuine passion unraveling pattern crafting insightful solution data engineering envision delivering impactful outcome align company objective data driven solution opportunity ignite enthusiasm data engineering propel career new height mastermind com created partnership dean graziosi tony robbins online platform people looking market monetize knowledge base mastermind worldwide following touch life world seeking world class data engineer join team chance help company new height role build tool solution automate complex workflow create data pipeline maintain infrastructure architecture data generation storage processing build system collect manage convert raw data usable information role involves solving complex technical issue working collaboratively team report directly chief technology officer interview process include technical assessment peer code review ass technical proficiency data engineer position based phoenix arizona require work mastermind headquarters scottsdale az provide excellent compensation model based experience ranging k k opportunity available usa resident valid work authorization offer sponsorship relocation requirement year experience designing maintaining mysql experience cloud data warehouse preferably bigquery dbt python experience sql development experience experience spreadsheet manipulation sql able write custom query answer question data leaving sql environment experience querying manipulating large datasets bachelor degree technical field equivalent related work experience ability utilize fivetran stitch extracting loading data bigquery strong understanding database design experience web apis pulling data preferably python plus excellent problem solving communication skill experience querying manipulating large datasets experience data parsing scripting automation responsibility design create database object table stored procedure view conduct technical research data profiling data source utilizing technology like python apis sql innovatively propose technical data solution address business challenge perform dimensional data modeling optimize database object accessibility performance consistency collaborate business stakeholder gather understand data requirement communicate data concept report kpis technical subject business friendly language develop etl application sql python etc data extraction transformation loading document development standard kpi calculation business term table diagram relevant information related data report date emerging technology trend data engineering perform duty assigned perk amp benefit competitive salary compensation excellent medical benefit eoy profit sharing k administration matching program incredible opportunity growth development amazing office culture mission team making difference apply ready dive fascinating realm data engineering skill new height data engineer invite join exceptional team opportunity unleash power data shape future data driven decision making embark fulfilling career journey click apply button step exciting future wait review application explore endless possibility working mastermind com mastermind com created partnership dean graziosi tony robbins online platform people looking market monetize knowledge base redefining self education mean world mastermind software platform education entertainment implementation amp community mastermind serf people worldwide seek transformation fulfillment success outside traditional education path mastermind software empowers enables implement learn amp actually paid addition providing community surrounded like minded individual cheering level',\n",
       "  {'entities': [(161, 177, data engineering),\n",
       "    (248, 264, data engineering),\n",
       "    (378, 394, data engineering),\n",
       "    (672, 682, build tool),\n",
       "    (725, 738, data pipeline),\n",
       "    (1043, 1054, code review),\n",
       "    (1382, 1387, mysql),\n",
       "    (1431, 1439, bigquery),\n",
       "    (1462, 1465, sql),\n",
       "    (1525, 1528, sql),\n",
       "    (1582, 1585, sql),\n",
       "    (1769, 1777, bigquery),\n",
       "    (1799, 1814, database design),\n",
       "    (1881, 1896, problem solving),\n",
       "    (2039, 2047, database),\n",
       "    (2110, 2124, data profiling),\n",
       "    (2175, 2178, sql),\n",
       "    (2271, 2284, data modeling),\n",
       "    (2294, 2302, database),\n",
       "    (2522, 2525, sql),\n",
       "    (2537, 2552, data extraction),\n",
       "    (2722, 2738, data engineering),\n",
       "    (3020, 3036, data engineering),\n",
       "    (3141, 3168, data driven decision making)]}),\n",
       " ('company newmark newmark group inc nasdaq nmrk subsidiary newmark world leader commercial real estate seamlessly powering phase property life cycle newmark comprehensive suite service product uniquely tailored client owner occupier investor founder startup blue chip company combining platform global reach market intelligence established emerging property market newmark provides superior service client industry spectrum year ending december newmark generated revenue approximately billion march newmark company owned office business partner operate office approximately professional world learn visit nmrk comor follow newmark job description provide operational support technical service critical data center fast paced growing technology company essential job duty maintain repair mechanical electrical security fire protection system protect support business critical information system server mainframe etc identify able immediately recognize system fault respond operational emergency situation maintain strict compliance federal state local code regulation standard perform preventative test analyze data ensure proper functioning base building critical system equipment including electrical hvac fire life safety security emergency system determine cause problem malfunction corrective action required monitor operation customer approved adjustment chilled water system heat exchanger crac unit hvac electrical system emergency system hot water system pump valve filter mechanical electrical equipment record reading recommendation adjustment necessary ensure proper operation equipment prepare maintain maintenance log report accurately document condition system data center maintain knowledge preventive maintenance work order software system coordinate contractor management approval work order require use outside contractor maintain detailed log work performed read comprehend work complex technical information e blueprint sketch building plan schematic pertaining electrical mechanical system serve data center adhere standard operating procedure formal change control process implement change interact part organization respond variety service request perform work ensure safety building tenant continuous operation facility maintain organized building file regular scheduled hour skill education experience high school diploma general education degree ged minimum year previous critical data center operation engineering experience cfc certified certification handling refrigerant plus proficient computer email skill proficient building automation system electrical power monitoring system ability complete required safety class pertain specific job duty familiarity preventative maintenance work order software system excellent communication skill thrives fast paced environment work independently team perform duty assigned benefit perk industry leading parental leave policy week generous healthcare bright horizon care program generous paid time education reimbursement referral program opportunity network connect benefit perk listed vary depending nature employment newmark job location working condition normal working condition absence disagreeable element note statement intended describe general nature level work performed employee construed exhaustive list responsibility duty skill required personnel classified newmark equal opportunity affirmative action employer qualified applicant receive consideration employment regard race color religion sex including sexual orientation gender identity national origin disability protected veteran status characteristic protected applicable federal state local law job type time benefit k dental insurance health insurance life insurance paid time parental leave referral program vision insurance schedule hour shift work setting person work location person',\n",
       "  {'entities': [(700, 711, data center),\n",
       "    (1673, 1684, data center),\n",
       "    (2015, 2026, data center),\n",
       "    (2405, 2416, data center)]}),\n",
       " ('south burlington vt description beta technology apply intellectual curiosity passion aviation commitment sustainability shared mission revolutionizing electric aviation regardless position hold team member brings talent desire positively impact environment life refreshing vibrant inclusive culture looking data engineer join growing data platform team person position play pivotal role supporting beta growing data need crafting efficient data pipeline developing data model assisting development robust data platform ideal candidate enjoys optimizing data system posse keen understanding analytical process data infrastructure adept translating business need technical solution self directed comfortable supporting data need multiple team domain system product contribute revolutionizing electric aviation develop construct test maintain analytic database scalable data pipeline define performant data model power dashboard reporting maintaining coherence evolution time develop manage integration party apis service ensuring stable secure efficient data exchange work closely data producer ensure data available reliable ready analysis collaborate closely data analyst optimize data query visualization identify design implement internal process improvement automating manual process optimizing data delivery designing infrastructure greater scalability etc implement data quality process ensuring accuracy consistency data abreast emerging technology trend data engineering introducing innovation best practice team organization maintain documentation code algorithm data definition ensuring clarity team member stakeholder leverage cloud based solution appropriately ensuring scalability resilience cost effectiveness effectively communicate technical non technical stakeholder translating complex data concept understandable insight recommendation design write test deploy production ready code actively contribute collaborative team environment fostering open communication mutual respect unified vision achieve shared goal drive business value review team member work ensure quality consistency conformity best practice work data subject matter expert strive greater functionality data system minimum qualification seeking bachelor degree master computer science statistic software engineering relevant field year experience cloud big data engineering role familiarity cloud platform aws preferred experience architecting programming large scale software application python advanced working sql knowledge experience working relational database query authoring working familiarity variety database including columnar e g redshift non relational e g dynamodb aptitude rapidly learning integrating party apis experience building optimizing big data data pipeline architecture data set experience working message queuing stream processing highly scalable big data data store experience working git version control ci cd system strong project management organizational skill exceptional troubleshooting skill ability spot issue problem excellent communication skill written verbal experience supporting working cross functional team dynamic environment qualification distinguish experience developing infrastructure code iac aws cdk cloudformation terraform proficiency building restful apis web service experience apache big data tool airflow avro beam parquet etc familiarity production application alm application lifecycle management plm product lifecycle management manufacturing execution system me quality management system qms enterprise resource planning erp video interview aware invited complete video screening interview invite sent email user hireflix com highly recommend completing step application process following hour invited note sure check junk folder beta technology provides equal employment opportunity individual regardless race color religion age sex sexual orientation gender identity national origin ancestry place birth citizenship disability veteran military status health coverage status hiv status genetic information crime victim status pregnancy pregnancy related condition characteristic protected state federal local law company discriminate discriminate tolerate discrimination based characteristic application process employment offer employment beta technology contingent favorable result thorough background check beta technology e verify employer',\n",
       "  {'entities': [(440, 453, data pipeline),\n",
       "    (553, 564, data system),\n",
       "    (609, 628, data infrastructure),\n",
       "    (849, 857, database),\n",
       "    (867, 880, data pipeline),\n",
       "    (916, 925, dashboard),\n",
       "    (1052, 1065, data exchange),\n",
       "    (1192, 1205, visualization),\n",
       "    (1371, 1383, data quality),\n",
       "    (1461, 1477, data engineering),\n",
       "    (1561, 1570, algorithm),\n",
       "    (2189, 2200, data system),\n",
       "    (2254, 2270, computer science),\n",
       "    (2339, 2347, big data),\n",
       "    (2499, 2502, sql),\n",
       "    (2543, 2557, database query),\n",
       "    (2596, 2604, database),\n",
       "    (2745, 2753, big data),\n",
       "    (2754, 2767, data pipeline),\n",
       "    (2859, 2867, big data),\n",
       "    (2868, 2878, data store),\n",
       "    (2918, 2923, ci cd),\n",
       "    (2938, 2956, project management),\n",
       "    (3318, 3324, apache),\n",
       "    (3325, 3333, big data),\n",
       "    (3339, 3346, airflow),\n",
       "    (3408, 3440, application lifecycle management),\n",
       "    (3445, 3473, product lifecycle management)]}),\n",
       " ('hi job title data engineer py spark job location plano tx job type contract job description year professional work experience designing implementing data pipeline cloud environment required year experience migrating developing data solution aws cloud required year experience building implementing data pipeline databricks similar cloud database expert level knowledge sql write complex highly optimized query large volume data hand object oriented programming experience python required professional work experience building real time data stream spark experience spark knowledge experience architectural best practice building data lake job type time salary year experience level year year year schedule hour shift ability commute relocate plano tx reliably commute planning relocate starting work required experience datawarehouse year preferred python year preferred pyspark year preferred work location person',\n",
       "  {'entities': [(149, 162, data pipeline),\n",
       "    (298, 311, data pipeline),\n",
       "    (312, 322, databricks),\n",
       "    (331, 345, cloud database),\n",
       "    (369, 372, sql),\n",
       "    (629, 638, data lake),\n",
       "    (871, 878, pyspark)]}),\n",
       " ('direct hire opportunity client hybrid role located minneapolis mn candidate able work sponsorship seeking highly motivated detail oriented data engineer join dynamic team ideal candidate responsible developing maintaining data solution drive business objective forward passion working data technical background desire contribute project leverage advanced analytics role excellent opportunity meaningful impact work collaboratively team create innovative data driven solution focus data engineering cloud technology data analytics product advanced analytics ready career level difference data driven world encourage apply responsibility data solution development design develop implement data solution address business need leveraging understanding data modeling sql proficiency coding skill language python c javascript continuous improvement identify seize opportunity enhance existing data solution optimizing performance scalability efficiency data visualization collaborate cross functional team create engaging data visualization provide actionable insight stakeholder data engineering cloud keen interest data engineering cloud technology actively participating related project support data infrastructure architecture data analytics product contribute development maintenance data analytics product ensuring meet highest quality standard satisfy business requirement advanced analytics work project related advanced analytics including predictive modeling machine learning statistical analysis help drive informed decision making collaborative teamwork act effective team member participating collaborative project sharing knowledge supporting colleague achieving project goal project documentation assist creation high quality project documentation project artifact ensuring aspect solution documented future reference audit qualification bachelor degree technical field equivalent practical experience ability understand implement solution business need effectively bridging gap technical knowledge business objective excellent written oral communication skill convey complex technical concept non technical stakeholder exceptional attention detail note taking skill precise problem solving documentation proficiency sql data manipulation analysis experience coding language like python c javascript data driven mindset strong analytical orientation demonstrated interest data visualization data engineering cloud technology data analytics product advanced analytics willingness contribute collaborative team member project fostering culture knowledge sharing innovation vetting process emergent software work hard find data engineer right fit client step vetting process position application minute online assessment minute initial phone interview minute interview client job offer job type time pay year benefit k k matching dental insurance health insurance paid time vision insurance compensation package bonus opportunity schedule hour shift monday friday work location person',\n",
       "  {'entities': [(355, 364, analytics),\n",
       "    (481, 497, data engineering),\n",
       "    (498, 514, cloud technology),\n",
       "    (520, 529, analytics),\n",
       "    (547, 556, analytics),\n",
       "    (748, 761, data modeling),\n",
       "    (762, 765, sql),\n",
       "    (807, 808, c),\n",
       "    (947, 965, data visualization),\n",
       "    (1016, 1034, data visualization),\n",
       "    (1074, 1090, data engineering),\n",
       "    (1111, 1127, data engineering),\n",
       "    (1128, 1144, cloud technology),\n",
       "    (1192, 1211, data infrastructure),\n",
       "    (1230, 1239, analytics),\n",
       "    (1288, 1297, analytics),\n",
       "    (1353, 1373, business requirement),\n",
       "    (1383, 1392, analytics),\n",
       "    (1423, 1432, analytics),\n",
       "    (1443, 1462, predictive modeling),\n",
       "    (1480, 1500, statistical analysis),\n",
       "    (2184, 2199, problem solving),\n",
       "    (2226, 2229, sql),\n",
       "    (2230, 2247, data manipulation),\n",
       "    (2296, 2297, c),\n",
       "    (2381, 2399, data visualization),\n",
       "    (2400, 2416, data engineering),\n",
       "    (2417, 2433, cloud technology),\n",
       "    (2439, 2448, analytics),\n",
       "    (2466, 2475, analytics)]}),\n",
       " ('azure data engineer techstra solution play pivotal role designing implementing managing data architecture azure cloud platform collaborate cross functional team ensure data infrastructure meet business need adheres industry best practice strategic thinker strong technical expertise role offer exciting opportunity shape data landscape responsibility develop maintain data pipeline azure data factory relevant azure technology optimize etl process performance scalability leveraging azure service azure data factory mapping data flow azure data lake analytics azure synapse pipeline extract data source transform usable format load azure data storage solution azure sql database azure synapse analytics collaborate cross functional team understand data requirement design scalable efficient etl process azure service identify design implement etl solution extraction integration data data warehouse data mart purpose reporting decision support analysis lead design development deployment data solution azure cloud platform expert level understanding azure data factory azure synapse azure sql azure data lake azure app service required designing building data pipeline api ingestion streaming ingestion method knowledge dev ops process including ci cd infrastructure code essential knowledge azure databricks azure iot azure hdinsight spark azure stream analytics power bi desirable manage azure sql database azure data factory azure data warehouse azure based data store knowledge c hand powershell scripting desirable working knowledge python desirable qualification bachelor degree computer science information technology related field year experience azure data factory azure certification e g azure data engineer azure solution architect preferred strong expertise data modeling etl process database management proficiency sql azure sql azure data lake storage familiarity data warehousing concept tool excellent problem solving communication skill strong project management leadership ability ability work collaboratively cross functional team location position located pittsburgh pa techstra solution help company brand achieve business value digital talent transformation believe component successful business transformation business strategy technology talent coming discipline enable company advantage opportunity differentiates approach holistic encompassing consider picture guide client journey expert transformation business strategy technology innovation human capital management deliver expertise client consulting innovative staffing solution software development strategy implementation dedicated bringing client world class business talent solution fit strategic requirement importantly deliver result equal employment opportunity statement techstra solution maintains strong policy equal opportunity employment objective recruit hire retain qualified individual regard race color creed religion gender national origin age pregnancy marital status sexual orientation gender identity expression disability veteran status characteristic status protected applicable federal state local law equal employment philosophy applies aspect employment including recruitment compensation benefit training promotion transfer job benefit termination msphsqpqa',\n",
       "  {'entities': [(88, 105, data architecture),\n",
       "    (168, 187, data infrastructure),\n",
       "    (368, 381, data pipeline),\n",
       "    (382, 400, azure data factory),\n",
       "    (497, 515, azure data factory),\n",
       "    (534, 549, azure data lake),\n",
       "    (550, 559, analytics),\n",
       "    (638, 650, data storage),\n",
       "    (666, 669, sql),\n",
       "    (670, 678, database),\n",
       "    (693, 702, analytics),\n",
       "    (899, 908, data mart),\n",
       "    (1050, 1068, azure data factory),\n",
       "    (1089, 1092, sql),\n",
       "    (1093, 1108, azure data lake),\n",
       "    (1155, 1168, data pipeline),\n",
       "    (1246, 1251, ci cd),\n",
       "    (1292, 1308, azure databricks),\n",
       "    (1354, 1363, analytics),\n",
       "    (1364, 1372, power bi),\n",
       "    (1396, 1399, sql),\n",
       "    (1400, 1408, database),\n",
       "    (1409, 1427, azure data factory),\n",
       "    (1461, 1471, data store),\n",
       "    (1482, 1483, c),\n",
       "    (1585, 1601, computer science),\n",
       "    (1655, 1673, azure data factory),\n",
       "    (1770, 1783, data modeling),\n",
       "    (1796, 1815, database management),\n",
       "    (1828, 1837, sql azure),\n",
       "    (1838, 1841, sql),\n",
       "    (1842, 1857, azure data lake),\n",
       "    (1878, 1894, data warehousing),\n",
       "    (1918, 1933, problem solving),\n",
       "    (1961, 1979, project management)]}),\n",
       " ('technical lead aws data engineer python remote position description cgi immediate need technical lead aws data engineer join financial service team belton tx columbia sc exciting opportunity work fast paced team environment supporting largest leader secondary mortgage industry innovative approach supporting client working agile environment emerging technology partner bank globally banking client worked average year cgi member country k loyal client leveraging end end service globe seeking technical lead aws python data engineering experience large financial service client future duty responsibility role focused aws development architecture looking hand professional build good technical solution roll sleeve implement solution able lead team required qualification successful role year experience year aws development cloud architecture year experience python strong experience sql database oracle postgresql aurora year experience data engineering emr pyspark redshift glue strong experience data migration cloud migration etl strong experience aws lambda fargate sn sqs elastic beanstalk ec sagemaker cloudwatch experience enterprise data lake data warehouse data mart big data excellent communication skill ask question clarify requirement engage team stakeholder strong logic reasoning critical thinking skill solve problem arise adaptive change demonstrated ability problem solve fly tailor approach resource hand independent problem solver evaluate situation build solution option strong python strong working aws experience database oracle postgresql aurora data engineering emr pyspark redshift glue serverless experience lambda step function containerization ec fargate desired skillset sa knowledge devops knowledge jenkins bitbucket terraform ucd cloudformation testing automation education requirement bachelor degree computer science information system related field aws certification desired li tm dice cgi required law jurisdiction include reasonable estimate compensation range role determination range includes factor limited skill set level experience relevant training licensure certification support ability reward merit based performance cgi typically hire individual near range role compensation decision dependent fact circumstance case reasonable estimate current range role u cgi professional member reinforce join team owner empowered participate challenge reward come building world class company cgi benefit include competitive base salary eligibility participate attractive share purchase plan spp company match dollar dollar contribution eligible employee maximum job category k plan profit participation eligible member generous holiday vacation sick leave plan comprehensive insurance plan include benefit medical dental vision life disability county emergency coverage country employment childcare pet insurance member assistance program college saving program personal financial management tool lifestyle management program insight act technology heart client digital transformation understand people heart business success join cgi trusted advisor collaborating colleague client bring forward actionable insight deliver meaningful sustainable outcome employee member cgi shareholder owner owner enjoy working growing build company proud dream brought today world largest independent provider business consulting service cgi recognize richness diversity brings strive create work culture belong collaborate client building inclusive community equal opportunity employer want empower member succeed grow require accommodation point recruitment process let know happy assist ready success story join cgi idea action difference qualified applicant receive consideration employment regard race ethnicity ancestry color sex religion creed age national origin citizenship status disability pregnancy medical condition military veteran status marital status sexual orientation perceived sexual orientation gender gender identity gender expression familial status political affiliation genetic information legally protected status characteristic cgi provides reasonable accommodation qualified individual disability need accommodation apply job u email cgi u employment compliance mailbox u employment compliance cgi com need reference requisition number position interested message routed appropriate recruiter assist note email address individual need accommodation apply job email reason include requisition number returned easy translate military experience skill click directed site dedicated veteran transitioning service member cgi offer employment u contingent ability successfully complete background investigation background investigation component vary dependent specific assignment level government security clearance held cgi consider employment qualified applicant arrest conviction record accordance local regulation ordinance cgi discharge manner discriminate employee applicant inquired discussed disclosed pay pay employee applicant employee access compensation information employee applicant essential job function disclose pay employee applicant individual access compensation information disclosure response formal complaint charge b furtherance investigation proceeding hearing action including investigation conducted employer c consistent cgi legal duty furnish information',\n",
       "  {'entities': [(520, 536, data engineering),\n",
       "    (886, 889, sql),\n",
       "    (890, 898, database),\n",
       "    (906, 916, postgresql),\n",
       "    (940, 956, data engineering),\n",
       "    (961, 968, pyspark),\n",
       "    (1001, 1015, data migration),\n",
       "    (1016, 1031, cloud migration),\n",
       "    (1054, 1064, aws lambda),\n",
       "    (1144, 1153, data lake),\n",
       "    (1169, 1178, data mart),\n",
       "    (1179, 1187, big data),\n",
       "    (1539, 1547, database),\n",
       "    (1555, 1565, postgresql),\n",
       "    (1573, 1589, data engineering),\n",
       "    (1594, 1601, pyspark),\n",
       "    (1717, 1723, devops),\n",
       "    (1742, 1751, bitbucket),\n",
       "    (1838, 1854, computer science),\n",
       "    (5284, 5285, c)]}),\n",
       " ('care deliver purpose create community resident proud home currently manage unit north america continue grow bell partner http bellpartnersinc com overview national leader multi family housing mission apartment company choice creating value honoring commitment resident partner associate remote position cloud data engineer sql developer understands criticality database integrity term architecture security efficiency maintainability strong working knowledge relational database management system required work written specification pre established guideline perform function job maintains documentation process flow cloud data engineer sql developer understands criticality database integrity term architecture security efficiency maintainability strong working knowledge relational database management system required work written specification pre established guideline perform function job maintains documentation process flow essential function responsibility able perform duty pressure self motivated posse ability work independently minimum supervision participate scrum meeting properly plan report execute work work closely key member job function gather discus requirement investigates new technology add impactful value development process assist creation database management procedure develop documentation existing database architecture act mentor colleague code review technical discussion collaborate developer produce high quality software product delivers business value follows established design pattern develop code test debug application program varying degree complexity provide guidance assistance technical leadership junior level software engineer complex large project interacts internally externally needed basis exchange information experience database design modelling knowledge rdbms concept required work closely project manager assigned project team review ensure time delivery complex solution additional function responsibility ability travel business need dictate assist task project requested knowledge skill ability year hand experience m sql server writing complex sql query stored procedure function etc year etl development azure data factory experience ssis plus year experience building data pipeline analytics solution stream process datasets low latency year experience complex data modelling elt etl design large database business environment experience ingesting data source format viz parquet csv sql database sharepoint apis etc azure sql database year experience ssa sql analysis service tabular multi dimensional cube model experience writing dax expression plus expertise elt etl optimization designing coding tuning big data process azure data factory advanced experience sql server relational database management system extensive understanding knowledge data analytics reporting tsql ssrs power bi etc strong knowledge data warehousing framework methodology knowledge data engineering data operational excellence standard methodology knowledge amp experience ci cd process knowledge amp experience azure devops knowledge qa tool manual automated experience data mining profiling analysis experience usage source control team environment git tfs etc strong understanding database design best practice experience azure setting monitoring database user amp access right experience yardi system big plus education amp background advanced knowledge sql experience advanced financial modeling tool preferred candidate bachelor degree master degree related field equivalent work experience knowledge microsoft excel required general knowledge microsoft office application candidate strong communicator comfortable dealing level experience multifamily industry preferred bell partner inc bpi company equal employment opportunity employer bpi policy discriminate applicant employee based race color sex religion national origin age disability pregnancy military veteran status marital status genetic information gender identity sexual orientation basis protected applicable federal state local law bpi prohibits harassment applicant employee based protected category bpi policy comply applicable state federal law respecting consideration unemployment status making hiring decision note applicant smoking prohibited indoor area bell partner inc designated smoking area established particular location accordance applicable state local law',\n",
       "  {'entities': [(121, 125, http),\n",
       "    (323, 326, sql),\n",
       "    (361, 369, database),\n",
       "    (459, 496, relational database management system),\n",
       "    (637, 640, sql),\n",
       "    (675, 683, database),\n",
       "    (773, 810, relational database management system),\n",
       "    (1267, 1286, database management),\n",
       "    (1328, 1349, database architecture),\n",
       "    (1371, 1382, code review),\n",
       "    (1772, 1787, database design),\n",
       "    (2076, 2079, sql),\n",
       "    (2103, 2106, sql),\n",
       "    (2164, 2182, azure data factory),\n",
       "    (2229, 2242, data pipeline),\n",
       "    (2243, 2252, analytics),\n",
       "    (2358, 2366, database),\n",
       "    (2444, 2447, sql),\n",
       "    (2448, 2456, database),\n",
       "    (2483, 2486, sql),\n",
       "    (2487, 2495, database),\n",
       "    (2516, 2519, sql),\n",
       "    (2668, 2676, big data),\n",
       "    (2685, 2703, azure data factory),\n",
       "    (2724, 2727, sql),\n",
       "    (2735, 2772, relational database management system),\n",
       "    (2812, 2821, analytics),\n",
       "    (2842, 2850, power bi),\n",
       "    (2872, 2888, data warehousing),\n",
       "    (2921, 2937, data engineering),\n",
       "    (3012, 3017, ci cd),\n",
       "    (3051, 3063, azure devops),\n",
       "    (3110, 3121, data mining),\n",
       "    (3223, 3238, database design),\n",
       "    (3289, 3297, database),\n",
       "    (3397, 3400, sql)]}),\n",
       " ('senior data engineer hdai entrepreneurial company developing commercializing proprietary quantitative solution capture integrate interpret healthcare data improve decision outcome specialize transforming data actionable information client broad array industry application hdai located dedham near commuter rail minute bay south station job description sr data engineer develop maintain critical function data engineering ai operation including data acquisition etl dataset management experience inference orchestration observability plus strong understanding event driven cloud architecture serverless computing functional software design preferred knowledge healthcare data particularly hl fhir experience mirth equivalent interface engine preferred understanding basic cloud infrastructure ability collaborate team member important success role strong skill communication estimation planning needed responsibility build maintain data pipeline etl code support business requirement work collaboratively cross functional team deliver enterprise solution internal external client continuously maintain optimize etl pipeline contribute expertise insight design architecture discussion produce maintain high quality documentation ensure transparency understanding solution interface internal team build best viable solution prototype finished product build scalable solution leverage functional design principle event driven architecture maintain data warehouse product development team data scientist work clinician data governance accuracy minimum qualification minimum year professional experience data engineering strong hand experience data engineering etl dataset management proficiency python golang r java similar language experience deploying software cloud based solution aws azure similar experience terraform plus familiarity devops practice tool ci cd pipeline excellent communication skill preferred education bachelor degree computer science engineering equivalent education experience advanced degree preferred eo statement hdai committed creating diverse environment proud equal opportunity employer qualified applicant receive consideration employment regard race color religion gender gender identity expression sexual orientation national origin genetics disability age veteran status job type time pay year benefit k k matching dental insurance flexible spending account health insurance health saving account life insurance paid time tuition reimbursement vision insurance compensation package bonus opportunity stock option yearly pay experience level year schedule hour shift work location hybrid remote dedham',\n",
       "  {'entities': [(404, 420, data engineering),\n",
       "    (444, 460, data acquisition),\n",
       "    (465, 472, dataset),\n",
       "    (771, 791, cloud infrastructure),\n",
       "    (931, 944, data pipeline),\n",
       "    (962, 982, business requirement),\n",
       "    (1514, 1529, data governance),\n",
       "    (1598, 1614, data engineering),\n",
       "    (1638, 1654, data engineering),\n",
       "    (1659, 1666, dataset),\n",
       "    (1706, 1710, java),\n",
       "    (1835, 1841, devops),\n",
       "    (1856, 1861, ci cd),\n",
       "    (1937, 1953, computer science)]}),\n",
       " ('evaluate experiment novel data engineering tool advises information technology lead partner new capability determine optimal solution particular technical problem designated use case big data engineering skill year hand experience modern object oriented programming language java scala python including ability code programming language year hand experience applying principle best practice trade offs schema design different database system including relational oracle mssql postgres mysql nosql hbase cassandra mongodb year hand experience implementing batch real time data integration framework application private public cloud environment aws azure gcp etc technology hadoop spark impala etc including assessing performance debugging fine tuning system year hand experience developing enterprise level apis leveraging python web framework like flask deep understanding latest data science data engineering method process develop impactful reusable pattern abstraction enterprise level data asset year hand experience phase data modeling conceptualization database optimization demonstrated ability perform engineering necessary acquire ingest cleanse integrate structure massive volume data multiple source system enterprise analytics platform proven ability design optimize query build scalable modular efficient data pipeline ability work structured semi structured unstructured data extracting information identifying linkage disparate data set proven experience delivering production ready data engineering solution including requirement definition architecture selection prototype development debugging unit testing deployment support maintenance ability operate variety data engineering tool technology vendor agnostic candidate preferred domain industry knowledge strong collaboration communication skill work technology team business unit demonstrates curiosity interpersonal ability organizational skill necessary serve consulting partner includes ability uncover understand ass need business stakeholder experience problem discovery solution design insight delivery involves frequent interaction education engagement evangelism senior executive',\n",
       "  {'entities': [(26, 42, data engineering),\n",
       "    (183, 191, big data),\n",
       "    (238, 274, object oriented programming language),\n",
       "    (275, 279, java),\n",
       "    (426, 441, database system),\n",
       "    (485, 490, mysql),\n",
       "    (491, 496, nosql),\n",
       "    (571, 587, data integration),\n",
       "    (880, 892, data science),\n",
       "    (893, 909, data engineering),\n",
       "    (1027, 1040, data modeling),\n",
       "    (1059, 1067, database),\n",
       "    (1229, 1238, analytics),\n",
       "    (1318, 1331, data pipeline),\n",
       "    (1498, 1514, data engineering),\n",
       "    (1612, 1624, unit testing),\n",
       "    (1680, 1696, data engineering)]}),\n",
       " ('ag grace looking talented data engineer support acquisition mission critical mission support data set preferred candidate background supporting cyber network related mission military space developer analyst engineer work performed customer site columbia md onsite essential job responsibility ideal candidate worked big data system complex structured unstructured data set supported government data acquisition analysis sharing effort past excel position candidate shall strong attention detail able understand technical complexity willingness learn adapt situation candidate work independently large team accomplish client objective',\n",
       "  {'entities': [(316, 324, big data), (394, 410, data acquisition)]}),\n",
       " ('data engineerjob descriptionthe data engineer responsible building implementing supporting stakeholder business line including accounting finance marketing operation business intelligence team right individual experience scaling data pipeline ensuring data arrives promptly reliably integral role building metric self serve data capability unlock phase growth great role individual passionate solving problem working data system ideal candidate great communicator help coordinate multiple internal external team take pride building end end project position hybrid position located vancouver wa dallas fort worth tx time offer sponsorship work location hybrid relocation assistance available work visa sponsorship available responsibility develop maintain robust scalable etl elt process pipeline handling large volume data format destination identify design implement process architecture improvement enhance infrastructure scalability data delivery automated manual process proficiently write complex sql query python powershell demonstrate strong command relational database system showcasing exceptional analytical problem solving ability represent complex algorithm software showcasing deep understanding database technology management system data structure algorithm expertise database architecture testing methodology offer engineering support team issue bug conducting research implementing effective fix maintain high quality data solution requirement year experience delivering data solution including data warehousing data integration data lake reporting analytics database software development experience sql sql python powershell experience m sql server azure snowflake aws experience api integration python powershell self starting mindset strong communication collaboration skill familiarity business intelligent implementation methodology understanding tableau desktop tableau server experience api integration python powershell excellent analytical amp troubleshooting remediation capability experience solving complex problem data experience translating complex problem technical specification strong ability multi task balance multiple priority fast paced environment detail oriented individual ability organize task work efficiently bachelor degree economics business mathematics statistic equivalent combination education experience benefit k k matching dental insurance employee assistance program employee discount flexible spending account health insurance health saving account life insurance paid time professional development assistance referral program vision insurance schedule hour shift hybrid day office day remote ability commute relocate vancouver wa amp dallas fort worth tx reliably commute planning relocate starting work required education bachelor required pay scale yearindustry restaurant employment type time job type time pay year benefit k matching employee assistance program employee discount life insurance paid time professional development assistance vision insurance experience level year schedule hour shift ability commute relocate vancouver wa reliably commute planning relocate starting work preferred experience sql year preferred data warehouse year preferred cloud computing year preferred work location hybrid remote vancouver wa',\n",
       "  {'entities': [(166, 187, business intelligence),\n",
       "    (229, 242, data pipeline),\n",
       "    (417, 428, data system),\n",
       "    (1002, 1005, sql),\n",
       "    (1068, 1083, database system),\n",
       "    (1118, 1133, problem solving),\n",
       "    (1160, 1169, algorithm),\n",
       "    (1209, 1217, database),\n",
       "    (1247, 1261, data structure),\n",
       "    (1262, 1271, algorithm),\n",
       "    (1282, 1303, database architecture),\n",
       "    (1511, 1527, data warehousing),\n",
       "    (1528, 1544, data integration),\n",
       "    (1545, 1554, data lake),\n",
       "    (1565, 1574, analytics),\n",
       "    (1575, 1592, database software),\n",
       "    (1616, 1619, sql),\n",
       "    (1620, 1623, sql),\n",
       "    (1655, 1658, sql),\n",
       "    (3166, 3169, sql),\n",
       "    (3215, 3230, cloud computing)]}),\n",
       " ('overview responsible design development testing maintenance data architecture including large scale database data pipeline process data delivery solution maintains solid understanding amtrust mission vision value upholds standard amtrust organization responsibility assemble large complex set data meet reporting analytics need enterprise analyze raw data source develop test implement optimized data pipeline solution control design develop test implement query report cube dashboard support business analytic need continuous focus improving data quality efficiency interface business analyst discus timeline clarify requirement pertains new project enhancement bug fix adherence devops sdlc change management practice keep current market trend demand performs functionally related duty assigned qualification knowledge sql sql pl sql ability write complex highly optimized query large volume data thorough understanding data pipeline construction etl process proficiency ssis ssa ssrs expertise database design methodology excellent analytic skill year data focused technical experience preferred excellent oral written communication skill ability communicate complicated technical information non technical audience efficient simple method curiosity passion data visualization solving problem highly creative order determine best solution real world problem quantitative data enjoy collaborating team atmosphere eagerness learn fast paced environment b degree computer science math statistic related technical field equivalent professional experience job description designed provide general overview requirement job entail comprehensive listing activity duty responsibility required position amtrust right revise job description time li gd li hybrid offer amtrust financial service offer competitive compensation package excellent career advancement opportunity benefit include medical amp dental plan life insurance including eligible spouse amp child health care flexible spending dependent care k saving plan paid time amtrust strives create diverse inclusive culture thought idea employee appreciated respected concept encompasses limited human difference regard race ethnicity gender sexual orientation culture religion disability amtrust value excellence recognizes embracing diverse background skill perspective workforce sustain competitive advantage remain employer choice diversity business imperative enabling attract retain develop best talent available diversity policy practice integral company operate future',\n",
       "  {'entities': [(60, 77, data architecture),\n",
       "    (100, 108, database),\n",
       "    (109, 122, data pipeline),\n",
       "    (313, 322, analytics),\n",
       "    (396, 409, data pipeline),\n",
       "    (475, 484, dashboard),\n",
       "    (543, 555, data quality),\n",
       "    (681, 687, devops),\n",
       "    (821, 824, sql),\n",
       "    (825, 828, sql),\n",
       "    (829, 835, pl sql),\n",
       "    (922, 935, data pipeline),\n",
       "    (997, 1012, database design),\n",
       "    (1261, 1279, data visualization),\n",
       "    (1463, 1479, computer science)]}),\n",
       " ('maximize opportunity rewarding work future leadership potential career growth join industry leader octapharma plasma offer professional opportunity meaningful difference enhance life patient need life saving medicine reward donor provide plasma collect inspire growth development team donation center office lab invite role data engineer charlotte north carolina seeking data engineer join growing biopharmaceutical company reporting business intelligence manager development position responsible analyzing designing developing implementing maintaining supporting ssis script stored procedure related etl structure meet company need additional responsibility data engineer role include consulting data management team big picture idea company data storage need presenting company warehousing option based storage need designing coding data warehousing system desired company specification assist development compliance departmental programming standard analyze data model interface design specification completeness conformity standard write code compliance programming standard prepare test data test program conducting preliminary testing warehousing environment data extracted extracting company data transferring new warehousing environment testing new storage system data transferred troubleshooting issue arise providing maintenance support help determine level effort time required solution delivery create document artifact promote sustainable knowledge management organization natural leader display strong character integrity excellent communicator team player person committed excellent customer service day day excellent technical conceptual interpersonal written verbal communication skill work pressure demanding deadline demonstrates ability adapt changing priority respond problem creatively decision maker ability positively influence people take bachelor degree computer science information system science equivalent experience minimum year experience design programming database design communication technique minimum year experience extracting transforming data business intelligence environment year experience microsoft sql server including sql agent job ssis package year experience sql programming object including stored procedure user defined function common table expression year experience sharepoint performancepoint year experience database tuning optimization year comprehensive data warehouse analysis design experience knowledge data warehouse methodology data modeling working knowledge general business quality regulatory requirement manufacturing system demonstrate proficiency technical conceptual interpersonal written verbal communication skill demonstrate ability adapt changing priority respond problem creatively participate rotation assist hour issue required h b sponsorship available octapharma plasma inc donation center team member u octapharma plasma inc collect plasma create life saving medicine patient worldwide growing impressive pace positive impact work community relies teamwork compassion expertise thing right way making meaningful difference life touch interested learn online apply octapharmahiring com know great fit octapharma plasma inc forward posting inner satisfaction outstanding impact',\n",
       "  {'entities': [(434, 455, business intelligence),\n",
       "    (697, 712, data management),\n",
       "    (743, 755, data storage),\n",
       "    (835, 851, data warehousing),\n",
       "    (1087, 1096, test data),\n",
       "    (1880, 1896, computer science),\n",
       "    (1989, 2004, database design),\n",
       "    (2082, 2103, business intelligence),\n",
       "    (2142, 2145, sql),\n",
       "    (2163, 2166, sql),\n",
       "    (2206, 2209, sql),\n",
       "    (2362, 2377, database tuning),\n",
       "    (2489, 2502, data modeling)]}),\n",
       " ('description markful currently seeking talented data engineer year proven sql server experience join growing data management team ideal candidate position strong background developing maintaining world class data solution microsoft sql server technology candidate advanced sql skill strong ability work independently wide range business enhancing data project equally important specific technology skill excellent written verbal communication skill team external team ability multi task ability translate business requirement database requirement ability adapt quickly new environment creativity solve difficult problem key member software development team essential responsibility characteristic position include following develop organize sql server object stored procedure function view table ssis package etc support automation effort co develop sql based solution e commerce environment generate deliver solution provide business intelligence stakeholder collaborate database developer software development team complex data project exhibit understanding fundamental database design principle ability prioritize manage multiple concurrent task project ability collaborate small productive development team motivated self starter ability learn adapt new technology passion working technology excitement creating high quality product rigorous attention detail focus quality deliverable excellent verbal written communication skill dedication appetite learning new technology furthering professional growth solid analytical problem solving skill minimum required qualification year worth experience deep knowledge remote role able work office located el cajon mssql server higher advanced sql skillset knowledge m sql server performance tuning design implementation high availability database architecture etl ssis ssrs ssms visual studio additional preferred qualification experience tableau experience reporting business intelligence system exposure understanding agile methodology proficient aspect software development lifecycle excellence debugging practice proficiency excel familiarity m development technology c asp net web technology html javascript cs xml etc highly desirable requirement year worth experience deep knowledge remote role able work office located el cajon mssql server higher advanced sql skillset knowledge m sql server performance tuning design implementation high availability database architecture etl ssis ssrs ssms visual studio',\n",
       "  {'entities': [(73, 76, sql),\n",
       "    (108, 123, data management),\n",
       "    (231, 234, sql),\n",
       "    (272, 275, sql),\n",
       "    (504, 524, business requirement),\n",
       "    (525, 533, database),\n",
       "    (740, 743, sql),\n",
       "    (849, 852, sql),\n",
       "    (925, 946, business intelligence),\n",
       "    (971, 979, database),\n",
       "    (1071, 1086, database design),\n",
       "    (1525, 1540, problem solving),\n",
       "    (1690, 1693, sql),\n",
       "    (1715, 1718, sql),\n",
       "    (1785, 1806, database architecture),\n",
       "    (1915, 1936, business intelligence),\n",
       "    (1967, 1984, agile methodology),\n",
       "    (2119, 2120, c),\n",
       "    (2121, 2128, asp net),\n",
       "    (2312, 2315, sql),\n",
       "    (2337, 2340, sql),\n",
       "    (2407, 2428, database architecture)]}),\n",
       " ('interested sport job new orleans saint pelican right place mission new orleans saint new orleans pelican global brand fan pride growth success entire gulf coast region committed leveraging resource goodwill foster meaningful change greater good society community celebrate individuality value identity experience value perspective team member team member integral success stand success hardship priority team member feel included opinion heard win want team value community integrity excellence organization people come celebrated love industry sport excited potential working major league nfl nba understand appreciate new orleans unique city offer want vibrant community want team best passionate driven ready work hard fun position summary exciting work sport exciting decision influencer sport data engineering technique tool method imagine learn accomplish saint pelican organization stop thinking learning creating testing innovating saint pelican business intelligence consumer insight department looking talented data engineer passionate designing crafting operating automated analytical solution drive scientific data driven decision making direct measurable positive impact new orleans saint new orleans pelican ultimately customer fan data engineer use python sql snowflake dbt tool build automated process model solution data operation company role data engineering process business related data support entire company marketing sale amp retention game presentation facility operation ticketing corporate partnership etc engage collaborate business analyst find opportunity understand requirement translate requirement technical data science solution design engineer maintain analytical tool model statistical data science approach apply tried true technique develop custom algorithm needed business problem collaborate data scientist business analyst design test implement robust automated batch machine learning solution build maintain data source etl elt process business operation social medium data digital text message data live traffic data weather data help ensure database integrity building maintaining resilient data process assist architecting data storage solution efficiently store retrieve information collaborate business analyst ensure operational business metric health monitoring key decision point provide actionable insight instantaneously business analyst key business decision maker execute data mining investigate adversarial trend identify behavior pattern respond agile logic change communicate result analysis business analyst manager executive research new technology method data science data engineering data visualization improve technical capability team project duty assigned bring bachelor degree computer science data engineering machine learning statistic applied mathematics economics related field focus computer science data science programming strong microsoft sql programming administration skill understanding database design data collection methodology including underlying relational dimensional data model experience building modern etl elt process preferably python centralize organize data strong understanding xml json file structure understanding utilize cloud based solution offered snowflake microsoftazure year experience applying data science real business problem minimum year practical experience ability implement data science pipeline application general programming language python minimum year practical experience knowledge relational database sql minimum year practical experience knowledge data visualization tool tableau knowledge modern data engineering framework dbt knowledge passion finding newest innovative engineering technique type programmer love refactoring reading new framework understanding algorithm classification regression clustering anomaly detection ability handle high pressure situation attentive detail excellent work product required high level interpersonal skill handle sensitive confidential situation information ability comprehend debug complex system integration ability extract meaningful business insight data identify story pattern excellent communication organization skill distilling complex analysis concept concise business focused takeaway creativity engineer novel feature signal push current tool approach highly analytical person strong problem solving self directed learning skill exceptional attention detail highly self motivated individual excellent teamwork spirit experience sport industry required plus submitting business case study past working experience highly recommended working condition typical office environment able sit work computer day working schedule hour ebb flow nfl nba schedule include working late hour holiday night weekend offer medical dental vision insurance option k employer contribution paid parental leave pto paid holiday wellness program application submitted online mail email resume saint pelican qualified candidate contacted phone email contact saint pelican human resource hiring manager check status application applying sure updated contact information provided new orleans saint new orleans pelican team committed providing equal employment opportunity candidate employee regardless membership protected classification team discriminate violation law basis race color age national origin sex including sexual orientation gender identity transgender status pregnancy religion physical mental disability genetic information marital status veteran status familial status status victim domestic violence legally protected class applicable federal state local law anti discrimination policy applies respect employment decision including limited hiring promotion discipline discharge team committed equal opportunity person disability compliance american disability act state law feel need accommodation disability inform request accommodation evaluated case case basis request accommodation essential participate fully interactive process',\n",
       "  {'entities': [(798, 814, data engineering),\n",
       "    (954, 975, business intelligence),\n",
       "    (1122, 1149, data driven decision making),\n",
       "    (1271, 1274, sql),\n",
       "    (1361, 1377, data engineering),\n",
       "    (1641, 1653, data science),\n",
       "    (1722, 1734, data science),\n",
       "    (1786, 1795, algorithm),\n",
       "    (2085, 2093, database),\n",
       "    (2168, 2180, data storage),\n",
       "    (2426, 2437, data mining),\n",
       "    (2615, 2627, data science),\n",
       "    (2628, 2644, data engineering),\n",
       "    (2645, 2663, data visualization),\n",
       "    (2742, 2758, computer science),\n",
       "    (2759, 2775, data engineering),\n",
       "    (2853, 2869, computer science),\n",
       "    (2870, 2882, data science),\n",
       "    (2912, 2915, sql),\n",
       "    (2963, 2978, database design),\n",
       "    (2979, 2994, data collection),\n",
       "    (3294, 3306, data science),\n",
       "    (3381, 3393, data science),\n",
       "    (3506, 3514, database),\n",
       "    (3515, 3518, sql),\n",
       "    (3563, 3581, data visualization),\n",
       "    (3612, 3628, data engineering),\n",
       "    (3778, 3787, algorithm),\n",
       "    (4351, 4366, problem solving)]}),\n",
       " ('responsibility peraton seeking software engineer data chantilly va support department defense dod highly talented highly motivated high performing team engineering team support exciting program involves design development test deployment exciting complex system agile construct contribute mission success variety stakeholder join generation innovator blaze trail forward profession company transform customer requirement technologically advanced high performing software architecture program technical schedule cost constraint supporting dod customer work emerging service distributed computing technology satellite data processing distribution member software development system integration team supporting dod customer work high performance environment agile software development process support system design architecture application deployment application maintenance perform backlog grooming sprint planning developing reviewing code updating technical document generate unit test ensuring code meet requirement unit testing resolving discrepancy report develop install configure software component algorithm cloud based environment framework work scrum master product owner scrum team member ensure successful integration software component qualification required qualification active secret clearance sci eligibility ability obtain maintain polygraph required bachelor degree computer science computer engineering related field year experience related field master degree computer science computer engineering related field equivalent experience substituted degree year experience software design development experience service oriented architecture include web service jms publish subscribe messaging micro service distributed computing technology experience modern java development including java common java framework spring boot apache camel knowledge python x bash x language experience developing software linux system typically redhat enterprise linux higher experience collaborating external developer integrating component larger framework experience continuous integration continuous delivery ci cd pipeline experience working team environment agile devsecops method tool include jira confluence jenkins git maven gitlab experience implementing cybersecurity hardened solution familiarity tl pki experience safe agile methodology use jira backlog perform work willing work person willing obtain security certification month hire desired qualification experience developing container deploying kubernetes helm x familiarity rancher x openshift x experience network architecture including ipv ipv network switch firewall tcp ip socket application higher level protocol http http grpc etc think creatively operate collaborative fast paced process oriented deadline driven ambiguous environment little supervision express confidently concisely accurately written oral communication benefit peraton benefit designed help best work daily fully committed growth employee fully comprehensive medical plan tuition reimbursement tuition assistance fertility treatment support way sismgs li py target salary range represents typical salary range position based experience factor',\n",
       "  {'entities': [(573, 594, distributed computing),\n",
       "    (616, 631, data processing),\n",
       "    (755, 781, agile software development),\n",
       "    (825, 847, application deployment),\n",
       "    (1017, 1029, unit testing),\n",
       "    (1104, 1113, algorithm),\n",
       "    (1383, 1399, computer science),\n",
       "    (1400, 1420, computer engineering),\n",
       "    (1479, 1495, computer science),\n",
       "    (1496, 1516, computer engineering),\n",
       "    (1723, 1744, distributed computing),\n",
       "    (1774, 1778, java),\n",
       "    (1801, 1805, java),\n",
       "    (1813, 1817, java),\n",
       "    (1840, 1852, apache camel),\n",
       "    (2067, 2089, continuous integration),\n",
       "    (2110, 2115, ci cd),\n",
       "    (2329, 2346, agile methodology),\n",
       "    (2684, 2688, http),\n",
       "    (2689, 2693, http)]}),\n",
       " ('chs inc leading global agribusiness owned farmer rancher cooperative united state provides grain food energy resource business consumer world serve agriculture customer consumer united state world employee united state today employee country chs creating connection empower agriculture chs inc data engineer expert location inver grove height mn job description data engineer expert position work chs information technology division business intelligence team apply knowledge enterprise resource planning integration bi technology including life cycle large complex enterprise platform upgrade implementation implementing solution according agile methodology managing task delivery multiple concurrent project reporting tool metadata creation specific duty include following designing developing testing information model hana bw hana platform interacting project united functional analyst enterprise information management enterprise solution architect ea business analyst ba gather high level business requirement translating functional specification technical specification conforming enterprise standard industry best practice creating database schema virtual model index compose view developing script working stakeholder identify establish data feed hana instance participating planning coordination release production ensure project timeline release schedule followed deadline ultimately met ensuring promotion content hana landscape sap solution manager participating user acceptance testing uat associated troubleshooting issue collaborating sap basis security team design implement analytic privilege secure limit access hana artifact assisting hana performance tuning query optimization effort creating extending semantically rich model cd view creating query advanced sql bw hana query position report worksite location incumbent work home united state job requirement qualified candidate bachelor degree foreign equivalent degree computer information system computer science computer engineering electrical engineering closely related field qualified candidate year month progressive post bachelor experience working sap bw sap hana qualified candidate year month experience following participating blueprint workshop session business process owner capture advanced sap bw hana modeling real time modeling requirement b debugging bw hana data model high performance stand point accommodate real time reporting scenario avoid data latency data redundancy data footprint c interacting team solution design live phase accurate integration team like system analyst business analyst end user group responsible end end testing application planning designing evaluating test approach test plan test procedure process unit integration performance user acceptance testing e translating functional specification technical specification conforming enterprise standard industry best practice f creating database schema physical model index compose view develop script g developing hana bw hana solution virtual persistent data modelling analytics cd view like basic composite consumption view table function amdp composite provider advanced dso open od view process chain development bw hana cockpit h performing work following tool technology sap hana sap netweaver bw sap bo sap ecc position report worksite location incumbent work home united state experience gained concurrently',\n",
       "  {'entities': [(433, 454, business intelligence),\n",
       "    (641, 658, agile methodology),\n",
       "    (995, 1015, business requirement),\n",
       "    (1140, 1155, database schema),\n",
       "    (1246, 1255, data feed),\n",
       "    (1481, 1499, acceptance testing),\n",
       "    (1780, 1783, sql),\n",
       "    (1971, 1987, computer science),\n",
       "    (1988, 2008, computer engineering),\n",
       "    (2451, 2466, data redundancy),\n",
       "    (2482, 2483, c),\n",
       "    (2634, 2654, application planning),\n",
       "    (2757, 2775, acceptance testing),\n",
       "    (2904, 2919, database schema),\n",
       "    (3038, 3047, analytics)]}),\n",
       " ('role bi data engineer skill sql dax azure spark sql python databricks synapse power bi location preferred pst job description bi data engineering sql dax azure spark sql python databricks synapse power bi job type permanent time pay hour expected hour week benefit k k matching dental insurance health insurance compensation package contract performance bonus yearly pay experience level year schedule day shift work location person',\n",
       "  {'entities': [(28, 31, sql),\n",
       "    (48, 51, sql),\n",
       "    (59, 69, databricks),\n",
       "    (78, 86, power bi),\n",
       "    (129, 145, data engineering),\n",
       "    (146, 149, sql),\n",
       "    (166, 169, sql),\n",
       "    (177, 187, databricks),\n",
       "    (196, 204, power bi)]}),\n",
       " ('data center industry deployment operation maintenance decommissioning salute mission critical industry leader delivering global data center service executed military precision salute employee team member culture significant driver success able deliver client team oriented culture defined transparent communication collaborative development deployment procedure best practice customer service mindset internally externally strong commitment safety responsibility job description data center facility engineer responsible operational integrity commissioning regulatory compliance electrical power mechanical monitoring control system process maximize customer uptime cost effective way highly available concurrently maintainable fault tolerant mission critical data center environment facility engineer responsible ensuring electrical mechanical fire life safety equipment data center operating peak efficiency utmost level reliability involves planned preventative maintenance equipment daily corrective work response emergency issue facility engineer line come hand electrical mechanical equipment troubleshooting serf expert highly capable technical resource reporting site manager interacting onsite facility technician party vendor expected singular focal point facility operation given data center data center equipment support mission critical server maintain better uptime equipment includes limited standby diesel generator related fuel system electrical switchgear upss pdus ahus related thermal management equipment system chemical treatment system applicable pump motor vfds building automation system responsibility place safety fail focus task activity given remain consistently focused safety operate monitor maintain respond abnormal condition facility system complete planned ad hoc preventive maintenance work assignment requiring skill direction electrical mechanical maintenance trade engineering discipline conduct periodic inspection building system e hvac electrical distribution maintain mechanical equipment room enclosure ensure clean functional safe condition rapid response investigation management indoor emergency situation air quality complaint including humidity complete working knowledge creating closing tracking work request wr initiate respond work order follow verify completion work monitor performance note deficiency service contractor recommend change necessary provide applied electrical mechanical integrated control water chemistry technical expertise entire data center directly interface management team contractor consultant phased data center construction commissioning including integration testing new system maintaining critical system online work facility technician stakeholder track complete preventive predicative maintenance schedule critical maintenance system commissioning ensure data center operates maximum operational efficiency including analyzing existing operating condition recommending new technology improving overall efficiency cost reduction responsible data center white space house electrical loading capacity including client installation system position require unusual hour work approximately week meet critical maintenance window scheduling directly manage contractor consultant daily operation critical maintenance ups system generator switchgear chiller maintenance provide applied electrical mechanical integrated control technical expertise entire data center directly manage supervise facility technician aspect day day operation maintenance electrical mechanical critical facility equipment respond emergency situation data center perform related duty responsibility assigned management data center facility engineer interacts internal department including executive interact client vendor desired skill amp experience strong background operating mechanical electrical critical system design review layout installation including ability read interpret electrical mechanical line ability conduct power thermal system capacity safety code compliance assessment hand experience installing maintaining troubleshooting large commercial industrial electrical system including medium voltage switchgear volt volt electrical generation distribution system diesel powered ac generator static ups system applicable dynamic rotary ups system applicable multi string flooded cell battery monitoring system static switch plc relay logic control power monitoring system data center power distribution management system current knowledge nfpa electrical fire life safety building code local regulatory equivalent location data center knowledge nec nfpa e nfpa nfpa compliance issue expert knowledge industrial safety best practice e lockout tag arc flash protection osha state regulation excellent understanding electrical mechanical system involved critical data center operation including system feeder transformer generator switchgear ups system at unit pdu unit chiller pump air handling unit crac unit work directly customer internal team installation engineer technical consultant implementation personnel coordinate customer turn ups process flow ensure smooth transition installation operation qualification year degree engineering discipline accredited university use sight hearing voice required motor skill ability lift kg lb bend stoop reach overhead stand long time period minimum year directly related experience operation installation maintenance data center critical equipment building system depth technical understanding knowledge hvac electrical plumbing fire life safety control system knowledge electrical mechanical system data center environment including feeder transformer generator switchgear ups system at sts unit pdu pmm unit chiller air handling unit crac unit good observation skill problem solving ability military experience advantage schedule rotation night professional growth compensation trajectory restricted way consider stepping role nationally recognized leader mission critical market working employer maintains corporate philosophy hire nation best invests unilaterally committed continuing enrichment employee take work dynamic fast paced environment welcome opportunity provide detail explore current situation interest military experience preferred',\n",
       "  {'entities': [(0, 11, data center),\n",
       "    (128, 139, data center),\n",
       "    (479, 490, data center),\n",
       "    (760, 771, data center),\n",
       "    (872, 883, data center),\n",
       "    (1291, 1302, data center),\n",
       "    (1303, 1314, data center),\n",
       "    (2503, 2514, data center),\n",
       "    (2579, 2590, data center),\n",
       "    (2839, 2850, data center),\n",
       "    (3024, 3035, data center),\n",
       "    (3428, 3439, data center),\n",
       "    (3601, 3612, data center),\n",
       "    (3669, 3680, data center),\n",
       "    (4438, 4449, data center),\n",
       "    (4589, 4600, data center),\n",
       "    (4826, 4837, data center),\n",
       "    (5429, 5440, data center),\n",
       "    (5612, 5623, data center),\n",
       "    (5781, 5796, problem solving)]}),\n",
       " ('job summary data engineer time remote based new england dedham alexander technology group searching data engineer join healthcare saas client targeting remote candidate new england area review job description requirement believe fit role reach chris mcmillan cmcmillan alexandertg com updated resume data engineer role responsible expanding optimizing data data pipeline architecture analyze adjust existing data data structure conceptualize data architecture multiple large scale project write tune complex query job design build launch maintain data model optimal data pipeline architecture infrastructure convert manual step automated process experience year experience data engineering position hand experience building functional data warehouse advanced sql knowledge year experience working azure cloud environment year experience working data tool azure datafactory strong experience building optimizing data pipeline architecture excellent communication skill ability work different team reach chris mcmillan cmcmillan alexandertg com updated resume atg monatg li cm',\n",
       "  {'entities': [(357, 370, data pipeline),\n",
       "    (413, 427, data structure),\n",
       "    (442, 459, data architecture),\n",
       "    (566, 579, data pipeline),\n",
       "    (673, 689, data engineering),\n",
       "    (759, 762, sql),\n",
       "    (911, 924, data pipeline)]}),\n",
       " ('w monroe lasalle chicago illinois continued effort harness power data seeking seasoned senior cloud data engineer deep expertise aws service successful candidate pioneer risk finance data hub development data ingestion pipeline creation specifically tailored finance risk sector oversees development method practice policy tool process collect store access use data securely efficiently cost effectively support lifecycle data asset assures quality data collection including adequacy accuracy legitimacy data identifies way manage data diverse unified data tier plan support implementation cloud data management platform tool policy procedure giving organization control business data cloud application hybrid cloud premise set ups manages data integration applying relevant data governance management practice advanced level proficiency creates maintains optimal data pipeline architecture assembles large complex data set meet functional non functional business requirement build data infrastructure required optimal extraction transformation loading data wide variety data source build analytics tool utilize data pipeline provide actionable insight customer operational efficiency key business performance metric work different stakeholder team assist data related technical issue support data infrastructure need work expert data strategy data governance suggest data quality enhancement operates group enterprise wide level serf senior specialist resource bmo influence team group work applies expertise think creatively address unique ambiguous situation find solution multiple interdependent complex problem communicates abstract concept simple term foster strong internal external network work multiple team achieve business objective anticipates trend responds implementing appropriate change broader work accountability assigned needed key responsibility architect implement aws based data hub catering unique requirement finance risk sector design develop optimize data ingestion pipeline aws glue python related technology manage fine tune aws redshift postgres database ensure fast accurate financial data retrieval collaborate sql developer analyst data mapper ensure accurate integration finance risk data provide aws cloud data architectural solution emphasizing security compliance scalability write maintain python script sql query pertinent financial data processing risk analysis monitor optimize data pipeline ensure integrity accuracy compliance mentor provide technical guidance team member ensuring best practice financial data management required qualification bachelor master degree computer science finance financial engineering related field minimum year experience working aws strong focus data solution finance risk sector proficiency python sql keen understanding financial data structure risk metric expertise aws redshift postgres experience finance related database administration solid understanding unix shell scripting demonstrable knowledge financial regulation risk management principle aws cloud data architecture best practice experience financial data mapping transformation compliance process preferred qualification master degree year experience data application development python sql scala java year experience public cloud aws microsoft azure google cloud year experience distributed data computing tool emr kafka spark mysql year experience working real time data streaming application year experience nosql implementation mongo cassandra year data warehousing experience redshift snowflake year experience unix linux including basic command shell scripting year experience agile engineering practice desired skill relevant aws certification aws certified data analytics specialty aws certified solution architect associate prior experience large financial institution financial technology familiarity global financial regulation risk management standard strong analytical communication collaboration skill proven track record project management delivering large scale data solution continuous learner staying updated aws documentation participating aws training attending aws conference compensation benefit base salary represents bmo financial group hiring range position actual salary vary based factor location skill experience qualification role salary time role pro rated based number hour regularly worked base salary component bmo financial group total compensation package employee include performance based incentive commission discretionary bonus perk reward bmo offer health insurance tuition reimbursement accident life insurance retirement saving plan view detail benefit visit www bmousbenefits com help bmo driven shared purpose boldly grow good business life call create lasting positive change customer community people working innovating pushing boundary transform life business power economic growth world member bmo team valued respected heard way grow impact strive help impact day customer support tool resource need reach new milestone help customer reach depth training coaching manager support network building opportunity help gain valuable experience broaden skillset find visit http job bmo com u en bmo committed inclusive equitable accessible workplace learning difference gain strength people perspective bmo equal opportunity affirmative action employer qualified applicant receive consideration employment regard sex gender identity sexual orientation race color religion national origin disability protected veteran status age characteristic protected law accommodation available request candidate taking aspect selection process request accommodation contact recruiter note recruiter bmo accept unsolicited resume source directly candidate unsolicited resume sent bmo directly indirectly considered bmo property bmo pay fee placement resulting receipt unsolicited resume recruiting agency valid written fully executed agency agreement contract service submit resume',\n",
       "  {'entities': [(183, 191, data hub),\n",
       "    (204, 218, data ingestion),\n",
       "    (449, 464, data collection),\n",
       "    (596, 620, data management platform),\n",
       "    (685, 702, cloud application),\n",
       "    (740, 756, data integration),\n",
       "    (775, 790, data governance),\n",
       "    (864, 877, data pipeline),\n",
       "    (955, 975, business requirement),\n",
       "    (982, 1001, data infrastructure),\n",
       "    (1089, 1098, analytics),\n",
       "    (1112, 1125, data pipeline),\n",
       "    (1293, 1312, data infrastructure),\n",
       "    (1330, 1343, data strategy),\n",
       "    (1344, 1359, data governance),\n",
       "    (1368, 1380, data quality),\n",
       "    (1896, 1904, data hub),\n",
       "    (1977, 1991, data ingestion),\n",
       "    (2001, 2009, aws glue),\n",
       "    (2075, 2083, database),\n",
       "    (2115, 2129, data retrieval),\n",
       "    (2142, 2145, sql),\n",
       "    (2341, 2344, sql),\n",
       "    (2371, 2386, data processing),\n",
       "    (2387, 2400, risk analysis),\n",
       "    (2418, 2431, data pipeline),\n",
       "    (2538, 2563, financial data management),\n",
       "    (2610, 2626, computer science),\n",
       "    (2773, 2776, sql),\n",
       "    (2806, 2820, data structure),\n",
       "    (2892, 2915, database administration),\n",
       "    (3037, 3054, data architecture),\n",
       "    (3090, 3102, data mapping),\n",
       "    (3196, 3219, application development),\n",
       "    (3227, 3230, sql),\n",
       "    (3237, 3241, java),\n",
       "    (3275, 3290, microsoft azure),\n",
       "    (3368, 3373, mysql),\n",
       "    (3408, 3422, data streaming),\n",
       "    (3451, 3456, nosql),\n",
       "    (3493, 3509, data warehousing),\n",
       "    (3710, 3719, analytics),\n",
       "    (3976, 3994, project management),\n",
       "    (5156, 5160, http)]}),\n",
       " ('join verizon verizon world leading provider technology communication service transforming way connect world human network reach globe work scene anticipate lead believe listening learning begin crisis celebration come lifting community striving impact world forward fueled purpose powered persistence explore career discover rigor take difference fulfillment come living networklife role artificial intelligence ai amp organization focus delivering power data ai drive marketplace leadership innovative solution create value customer shareholder v teamers society responsibly verizon multi year journey industrialize data practice ai capability simply mean ai data fuel decision business process company understanding code development compliant meet high standard quality delivers desired functionality cutting edge technology programming component developing feature framework working independently contributing immediate team cross functional team contributing design discussion taking ownership delegated task helping team member demonstrating initiative explore alternate technology approach solving problem following agile based approach deliver data product looking looking junior data engineer good fundamental understanding google cloud platform data ingesting data curation concept depending need product looking help build gcp platform prem hadoop teradata need bachelor degree year work experience good understanding data warehousing data lake big data platform strong leadership communication persuasion teamwork skill empathy positive attitude better following bachelor master degree computer science information science engineering related field year relevant work experience year programming experience gcp data proc cloud shell sdk cloud composer gc cloud function amp big query year experience designing deployment hadoop cluster different big data analytical tool including hdfs pig hive sqoop spark oozie hand experience designing building data pipeline nifi airflow apache beam gcp data proc big query etl related job knowledge google data catalog google cloud apis monitoring querying billing related analysis big query usage experience working nosql database hbase cassandra couchbase relational database oracle mysql teradata verizon role sound like fit encourage apply meet better qualification listed role eligible considered department defense skillbridge program working hybrid role defined work location includes work home assigned office day set manager scheduled weekly hour equal employment opportunity proud equal opportunity employer celebrate employee difference including race color religion sex sexual orientation gender identity national origin age disability veteran status verizon know diversity make stronger committed collaborative inclusive environment encourages authenticity foster sense belonging strive feel valued connected empowered reach potential contribute best check diversity inclusion page learn',\n",
       "  {'entities': [(388, 411, artificial intelligence),\n",
       "    (1269, 1282, data curation),\n",
       "    (1428, 1444, data warehousing),\n",
       "    (1445, 1454, data lake),\n",
       "    (1455, 1463, big data),\n",
       "    (1597, 1613, computer science),\n",
       "    (1857, 1865, big data),\n",
       "    (1959, 1972, data pipeline),\n",
       "    (1978, 1985, airflow),\n",
       "    (1986, 1997, apache beam),\n",
       "    (2166, 2171, nosql),\n",
       "    (2172, 2180, database),\n",
       "    (2218, 2226, database),\n",
       "    (2234, 2239, mysql)]}),\n",
       " ('company newmark newmark group inc nasdaq nmrk subsidiary newmark world leader commercial real estate seamlessly powering phase property life cycle newmark comprehensive suite service product uniquely tailored client owner occupier investor founder startup blue chip company combining platform global reach market intelligence established emerging property market newmark provides superior service client industry spectrum year ending december newmark generated revenue approximately billion march newmark company owned office business partner operate office approximately professional world learn visit nmrk comor follow newmark job description provide operational support technical service critical data center fast paced growing technology company essential job duty maintain repair mechanical electrical security fire protection system protect support business critical information system server mainframe etc identify able immediately recognize system fault respond operational emergency situation maintain strict compliance federal state local code regulation standard perform preventative test analyze data ensure proper functioning base building critical system equipment including electrical hvac fire life safety security emergency system determine cause problem malfunction corrective action required monitor operation customer approved adjustment chilled water system heat exchanger crac unit hvac electrical system emergency system hot water system pump valve filter mechanical electrical equipment record reading recommendation adjustment necessary ensure proper operation equipment prepare maintain maintenance log report accurately document condition system data center maintain knowledge preventive maintenance work order software system coordinate contractor management approval work order require use outside contractor maintain detailed log work performed read comprehend work complex technical information e blueprint sketch building plan schematic pertaining electrical mechanical system serve data center adhere standard operating procedure formal change control process implement change interact part organization respond variety service request perform work ensure safety building tenant continuous operation facility maintain organized building file regular scheduled hour skill education experience high school diploma general education degree ged minimum year previous critical data center operation engineering experience cfc certified certification handling refrigerant plus proficient computer email skill proficient building automation system electrical power monitoring system ability complete required safety class pertain specific job duty familiarity preventative maintenance work order software system excellent communication skill thrives fast paced environment work independently team perform duty assigned benefit perk industry leading parental leave policy week generous healthcare bright horizon care program generous paid time education reimbursement referral program opportunity network connect benefit perk listed vary depending nature employment newmark job location working condition normal working condition absence disagreeable element note statement intended describe general nature level work performed employee construed exhaustive list responsibility duty skill required personnel classified newmark equal opportunity affirmative action employer qualified applicant receive consideration employment regard race color religion sex including sexual orientation gender identity national origin disability protected veteran status characteristic protected applicable federal state local law job type time benefit k dental insurance health insurance life insurance paid time parental leave referral program vision insurance schedule hour shift work setting person work location person',\n",
       "  {'entities': [(700, 711, data center),\n",
       "    (1673, 1684, data center),\n",
       "    (2015, 2026, data center),\n",
       "    (2405, 2416, data center)]}),\n",
       " ('company overview world class team professional deliver generation technology product robotic autonomous platform ground soldier maritime system location world wide work contributes innovative research field sensor science signal processing data fusion artificial intelligence ai machine learning ml augmented reality ar qinetiq dedicated expert defense aerospace security related field work explore new way protecting american warfighter security force ally qinetiq mean central safety security world partnering customer help save life reduce risk society maintain global infrastructure depend join qinetiq courage wide variety complex challenge experience unique working environment innovative team blend different perspective discipline technology discover new way solving complex problem diverse inclusive environment authentic feel valued respected realize potential qinetiq support workplace flexibility commitment health family provide opportunity work purpose committed supporting success professional personal life position overview far ridgeleine engagement qinetiq company seeking data engineer support united state army special operation command usasoc core responsibility provide trained ready force prepared meet geographic combatant command gcc theater special operation command tsoc requirement world considering breadth hybrid dynamic threat group associated gcc respective area responsibility providing tailored special operation force package proper mix skill equipment time prevent address conflict difficult task conventional force generation deployment process usasocs transition truly data driven organization best posture current operational environment operational environment responsibility implement continuous process improvement process tool technology facilitate usasoc transition data driven organization support ongoing ai ml enterprise enable modernization effort support arsof mission actively support artificial intelligence division ai div utilizeagile best practice execute tenant devops required qualification dod secret security clearance required willing able obtain t sci meet dod iat level ii iam level bachelor degree year experience ability assemble large complex set data meet non functional functional business requirement identify design implement internal process improvement including designing infrastructure greater scalability optimizing data delivery automating manual process develops maintains scalable data pipeline build new api integration support continuing increase data volume complexity work stakeholder including data design product executive team assist data related technical issue able migrate workload different cloud computing service model able identify data consolidation opportunity database system including data sharing access business line provide recommendation new database technology architecture monitor maintain database ensure optimal performance implement data mining data warehousing application writes unit integration test contributes engineering wiki document work design data integration data quality framework design evaluates open source vendor tool data lineage high level proficiency sql data modeling etl elt process implementation technical documentation skill familiar multiple database knowledge programming language e g java python preferred qualification experience aws open source data engineering solution big data familiar data engineer tool like apache airflow nifi experience non sql database data engineering problem set ie nosql vector graph effectively communicate data science team software development team company eeo statement accessibility accommodation medical condition disability need reasonable accommodation employment process send e mail staffing u qinetiq com opt let know nature request contact information qinetiq equal opportunity affirmative action employer qualified applicant receive equal consideration employment regard race age color religion creed sex sexual orientation gender identity national origin disability protected veteran status',\n",
       "  {'entities': [(240, 251, data fusion),\n",
       "    (252, 275, artificial intelligence),\n",
       "    (1935, 1958, artificial intelligence),\n",
       "    (2017, 2023, devops),\n",
       "    (2247, 2267, business requirement),\n",
       "    (2457, 2470, data pipeline),\n",
       "    (2678, 2693, cloud computing),\n",
       "    (2753, 2768, database system),\n",
       "    (2779, 2791, data sharing),\n",
       "    (2840, 2848, database),\n",
       "    (2890, 2898, database),\n",
       "    (2936, 2947, data mining),\n",
       "    (2948, 2964, data warehousing),\n",
       "    (3056, 3072, data integration),\n",
       "    (3073, 3085, data quality),\n",
       "    (3173, 3176, sql),\n",
       "    (3177, 3190, data modeling),\n",
       "    (3270, 3278, database),\n",
       "    (3314, 3318, java),\n",
       "    (3377, 3393, data engineering),\n",
       "    (3403, 3411, big data),\n",
       "    (3445, 3459, apache airflow),\n",
       "    (3480, 3483, sql),\n",
       "    (3484, 3492, database),\n",
       "    (3493, 3509, data engineering),\n",
       "    (3525, 3530, nosql),\n",
       "    (3568, 3580, data science)]}),\n",
       " ('data engineer summary seeking data engineer augment capacity client platform working control compliance department ensure meet financial regulatory obligation client base timely fashion required qualification year software design development experience including ci cd source code control testing quality management highly proficient python sql database design experience working data warehouse like snowflake generate complex solution scale self starter sense urgency eagerness learn explore new technology appropriate solve business problem strong communicator interact clear concise manner non technical business stakeholder product manager engineer ability troubleshoot logically ass problem determine solution experience working post trade automation space designing architecting system deliver solution complex data problem experience generating complex financial industry regulatory report like cat financial report like c duty amp collaboration engineer collaborate task include following account master migration cat cais account master migration service anti money laundering aml exception management tech debt work onboarding process client engineer collaborate client team participate sprint planning process work client code base tech stack python sql snowflake retool docker kubernetes argo metaplane rest apis long term contract remote positionaccepting candidate north america latin america amp europe job type contract benefit health insurance compensation package weekly pay experience level year schedule hour shift monday friday experience python year required sql year required argo year required work location remote',\n",
       "  {'entities': [(263, 268, ci cd),\n",
       "    (341, 344, sql),\n",
       "    (345, 360, database design),\n",
       "    (928, 929, c),\n",
       "    (1261, 1264, sql),\n",
       "    (1581, 1584, sql)]}),\n",
       " ('description w role month contract project resource working business system team client enterprise capability domain scope project large scale migration prem sql cloud business data warehouse team utilizing azure data brick month project possibility extension core technical skill azuredatabrickssqlpyspark education bachelor degree job type contract pay hour experience level year work location remote',\n",
       "  {'entities': [(157, 160, sql)]}),\n",
       " ('professional agile professional goal ensure employee know valued number individual member datahaven team small company know employee turn know work environment includes modern office setting food provided work home day growth opportunity responsibility ideal candidate worked big data system complex structured unstructured data set supported government data acquisition analysis sharing effort past excel position candidate shall strong attention detail able understand technical complexity willingness learn adapt situation candidate work independently large team accomplish client objective qualification security clearance current secret level security clearance required candidate u citizen year experience developer analyst engineer bachelor related field year relevant experience master related field high school diploma equivalent year relevant experience experience programming language python java proficiency acquisition understanding network data associated metadata fluency data extraction translation loading including data prep labeling enable data analytics experience kibana elasticsearch familiarity log format json xml experience data flow management storage solution e kafka nifi aws sqs solution ability decompose technical problem troubleshoot system dataflow issue able work site hybrid role great team highly analytical individual passion working large datasets implementing scalable solution love hear join team data engineer contribute success organization note job description intended inclusive employee perform related duty negotiated meet ongoing need organization job type time pay year benefit k k matching dental insurance health insurance life insurance paid time referral program vision insurance compensation package bonus opportunity signing bonus yearly pay experience level year schedule hour shift monday friday ability commute relocate fort gordon ga reliably commute planning relocate starting work required application question fluent data extraction translation loading including data prep labeling enable data analytics education bachelor required experience nifi year required security clearance secret required work location hybrid remote fort gordon ga',\n",
       "  {'entities': [(276, 284, big data),\n",
       "    (354, 370, data acquisition),\n",
       "    (903, 907, java),\n",
       "    (987, 1002, data extraction),\n",
       "    (1064, 1073, analytics),\n",
       "    (1273, 1281, dataflow),\n",
       "    (1978, 1993, data extraction),\n",
       "    (2055, 2064, analytics)]}),\n",
       " ('walmart advanced system amp robotics fast growing venture mission reinvent retail robotics partnered walmart develop alphabot technology currently deployed store north america data engineer service amp support team play hand role development data pipeline setting data warehouse create dashboard visualization goal data accessible enable team organization effectively use data identify bottle neck decision optimize site performance ideal candidate posse strong aptitude data enjoys problem solving able multitasks effectively manage priority incoming data request work create data model report amp analysis support business need convert raw data meaningful insight interactive easy understand dashboard report translate team need data solution visualization tool like power bi tableau recommend technical approach data pipeline development understand correct schema future proof data pipeline develop query report internal external stakeholder review improve optimize existing etl sql query dashboard view procedure implementing best practice data extraction storage work implementation technology needed facilitate transfer data integration internal party application propose define review test data warehouse modification fill identified data gap improve report design efficiency including custom table view looking data warehousing data mining experience hand experience extracting data data source building data model star schema snowflake experience requirement analysis design prototyping experience resolving complex issue creative efficient effective way strong working knowledge sql relational database experience cloud data platform experience azure plus demonstrated proficiency transferring data cross platform application ability write clean readable maintainable code documentation java python experience required excellent written amp oral communication skill strong organizational amp planning skill b computer science data engineering related field m plus year experience data visualization tool like power bi tableau year experience machine learning computer vision artificial intelligence equivalent master level course work acceptable walmart advanced system amp robotics recent award include exclusive benefit time team member comprehensive health care option choose range health plan tailored extend benefit dependent unlimited pto salaried employee receive unlimited paid time vacation holiday personal day eye amp dental care got family covered notch vision dental insurance secure future advantage competitive k matching stock purchase plan equity opportunity peace mind comprehensive life disability insurance option parental leave embrace joy parenthood week fully paid maternity paternity leave exclusive discount shop save special walmart discount store online dine amp energize enjoy daily complimentary lunch beverage variety snack fueled day join experience best employee care benefit learn named best place work alertinnovation com career alert innovation proud equal employment opportunity employer celebrate diversity committed creating inclusive environment employee discriminate based race religion color national origin gender including pregnancy childbirth related medical condition sexual orientation gender identity gender expression age status protected veteran status individual disability applicable legally protected characteristic',\n",
       "  {'entities': [(242, 255, data pipeline),\n",
       "    (286, 295, dashboard),\n",
       "    (296, 309, visualization),\n",
       "    (483, 498, problem solving),\n",
       "    (694, 703, dashboard),\n",
       "    (745, 758, visualization),\n",
       "    (769, 777, power bi),\n",
       "    (815, 828, data pipeline),\n",
       "    (880, 893, data pipeline),\n",
       "    (982, 985, sql),\n",
       "    (992, 1001, dashboard),\n",
       "    (1044, 1059, data extraction),\n",
       "    (1126, 1142, data integration),\n",
       "    (1192, 1201, test data),\n",
       "    (1319, 1335, data warehousing),\n",
       "    (1336, 1347, data mining),\n",
       "    (1456, 1476, requirement analysis),\n",
       "    (1589, 1592, sql),\n",
       "    (1604, 1612, database),\n",
       "    (1797, 1801, java),\n",
       "    (1919, 1935, computer science),\n",
       "    (1936, 1952, data engineering),\n",
       "    (1990, 2008, data visualization),\n",
       "    (2019, 2027, power bi),\n",
       "    (2085, 2108, artificial intelligence)]}),\n",
       " ('job summary data engineer job family form software engineering field support track function data engineer responsible managing optimizing monitoring data retrieval storage appropriate distribution organization contribute achievement business objective identifying opportunity effective use technology developing implementing supporting analytical solution time budget pgw auto glass llc working hybrid environment team member expected commute cranberry township pa headquarters day week sponsorship assistance provided essential job duty conduct research leveraging big data technology surface actionable insight influence analytical solution roadmap process unstructured data form suitable analysis utilize critical thinking skill ass ai capability best applied complex business situation work closely software engineering team integrate idea innovation algorithm production system query database structured un structured data perform statistical analysis collaborate cross functional team understand data requirement translate scalable data pipeline design develop test maintain etl process ensure timely accurate movement data source data warehouse work closely business team understand best way deliver data enable data driven decision making promote culture data driven decision making organization enabling access clean accurate trustworthy data posse strong communication skill variety stakeholder group level organization develop deep understanding auto glass industry internal business operation strategic goal proactively identify opportunity data driven insight actively participate continuous improvement initiative contributing new idea feedback challenge proposed solution stay date industry trend emerging technology e tool methodology best practice data engineering analytics identify opportunity automate manual process improve efficiency accuracy existing data process apply quantitative analysis data mining expertise presenting data visualize number underlying trend use analysis process automation assumes duty assigned basic qualification education amp experience bachelor degree computer science related discipline equivalent experience year experience working data engineering similar role knowledge skill ability understanding basic concept terminology practical application current emerging technology strong analytical ability technical skill evaluate alternative approach recommend feasible economical solution proficiency architecture analytical design technique database technology programming language work academic experience building application following strong experience working sql server oracle relational database programming year experience working programming language like python java scala year experience business intelligence tool cognos tableau power bi experience working ssis ssrs experience azure data factory adf plus exposure cloud environment azure preferred ability lead effectively participate team strong verbal written communication skill ability express complex technical concept audience appropriate term ability work close supervision',\n",
       "  {'entities': [(149, 163, data retrieval),\n",
       "    (566, 574, big data),\n",
       "    (855, 864, algorithm),\n",
       "    (889, 897, database),\n",
       "    (936, 956, statistical analysis),\n",
       "    (1038, 1051, data pipeline),\n",
       "    (1219, 1246, data driven decision making),\n",
       "    (1263, 1290, data driven decision making),\n",
       "    (1765, 1781, data engineering),\n",
       "    (1782, 1791, analytics),\n",
       "    (1893, 1914, quantitative analysis),\n",
       "    (1915, 1926, data mining),\n",
       "    (2102, 2118, computer science),\n",
       "    (2184, 2200, data engineering),\n",
       "    (2492, 2500, database),\n",
       "    (2615, 2618, sql),\n",
       "    (2644, 2664, database programming),\n",
       "    (2722, 2726, java),\n",
       "    (2749, 2775, business intelligence tool),\n",
       "    (2791, 2799, power bi),\n",
       "    (2840, 2858, azure data factory)]}),\n",
       " ('location requirement role hybrid office naperville il remote candidate considered role team looking enthusiastic experienced highly driven data engineer play instrumental role creating evolving complex data centric solution improve decision making company client internal staff data engineering team responsible building maintaining scalable data system pipeline team manages acquisition storage processing data internal external data source multiple analytical product includes data mapping geocoding validation metadata management automation process team us python scripting implement automate data workflow relational columnar database collaborate stakeholder development team provide decision making solution idea design deployment operation validation working exciting fast paced agile environment role report data team lead work closely development product management strategic partner deliver solution simplify daily work life line education expect create optimize maintain improve data integration pipeline create implement complex transformation regular large datasets ensure data quality reliability scalability integrity prepare data prescriptive predictive modeling automate improve performance efficiency data management process collaborate engineering product development team architecting building deploying data analysis system provide feedback work data engineer communicate clarity organization influence decision appropriate bring role ability willingness learn new language technology fast learner detail oriented capable working fast paced work environment excellent oral written communication skill role requires year progressive experience data intensive system application proficiency sql data modeling experience database technology m sql postgresql snowflake excellent command python scripting hand experience etl elt tool experience bi tool including tableau experience cloud based environment aws preferred familiarity devops linux window familiarity agile development methodology frontline education pioneer school administration software purpose built k district provide innovative connected solution student special program business operation human capital management powerful data analytics empower educator administrator earn trust k leader u serving consistently high performing forthright partner school district dimension company group unique talented individual love lucky land job rapidly growing tech company support appreciative friendly customer base work hard customer happy like good time process company strives think term instead believe philosophy servant leadership putting value balance family work frontline embrace diversity equity inclusivity intentionally building workplace respect support value identity employee believe foundational developing strong community company frontline education equal opportunity employer discriminate basis race religion color national origin gender sexual orientation age marital status veteran status disability status perk frontliner frontline offer competitive compensation package including base salary rewarding bonus structure k match unlimited pto company growth created promising environment career advancement rewarding challenge offer tuition reimbursement program eligible college credit coursework available employee depending status length employment',\n",
       "  {'entities': [(278, 294, data engineering),\n",
       "    (342, 353, data system),\n",
       "    (479, 491, data mapping),\n",
       "    (630, 638, database),\n",
       "    (989, 1005, data integration),\n",
       "    (1085, 1097, data quality),\n",
       "    (1158, 1177, predictive modeling),\n",
       "    (1218, 1233, data management),\n",
       "    (1323, 1336, data analysis),\n",
       "    (1709, 1712, sql),\n",
       "    (1713, 1726, data modeling),\n",
       "    (1738, 1746, database),\n",
       "    (1760, 1763, sql),\n",
       "    (1764, 1774, postgresql),\n",
       "    (1947, 1953, devops),\n",
       "    (2213, 2222, analytics)]}),\n",
       " ('performs data engineer task planning technical leadership management team provides client technical expertise project program able manage technology project lead technology solutioning provides highly technical specialized guidance solution complex problem performs elaborate analysis study evaluates recommends executes new technology update existing infrastructure ensure optimal performance efficiency develops strategy ensure system meet existing future requirement based need regulation work variety environment excellent verbal non verbal communication skill work effectively independently bring qualification u citizen obtain clearance min year experience b related field experience developing logical data model knowledge industry standard data migration tool experience designing data migration solution pipeline ability work oracle postgresql sql server aws service experience data migration etl tool experience python java node j computer language preferred oneglobe oneglobe llc founded provide quality information technology solution exceed expectation focus system modernization agile software development practice devsecops deliver intuitive maintainable system help customer improve process capability provide service solution skill identify plan perform cost saving step system lifecycle enhance system efficiency optimizing value deliver customer provide highly competitive benefit package include extensive medical dental vision annual salary k paid time pto k annually ongoing education training monthly social tech event additional position http www oneglobeit com career oneglobe proud equal opportunity employer drug free eeo employer committed diverse workforce consider qualified candidate regardless race color national origin sex age marital status personal appearance sexual orientation gender identity family responsibility disability political affiliation veteran status additional position http www oneglobeit com career job type time benefit k k matching dental insurance flexible schedule health insurance health saving account life insurance paid time professional development assistance referral program retirement plan tuition reimbursement vision insurance schedule hour shift education bachelor preferred experience aws year required postgresql year required python year required data migration year required work location person',\n",
       "  {'entities': [(748, 762, data migration),\n",
       "    (789, 803, data migration),\n",
       "    (842, 852, postgresql),\n",
       "    (853, 856, sql),\n",
       "    (887, 901, data migration),\n",
       "    (929, 933, java),\n",
       "    (1027, 1046, technology solution),\n",
       "    (1093, 1119, agile software development),\n",
       "    (1288, 1304, system lifecycle),\n",
       "    (1562, 1566, http),\n",
       "    (1921, 1925, http),\n",
       "    (2272, 2282, postgresql),\n",
       "    (2318, 2332, data migration)]}),\n",
       " ('introduction career opportunity data engineer want current employer exciting opportunity join valify nation leading provider healthcare service hca healthcare benefit valify offer total reward package support health life career retirement colleague available plan program include comprehensive medical coverage cover common service cost low copay plan include prescription drug behavioral health coverage free telemedicine service free airmed medical transportation additional option dental vision benefit life disability coverage flexible spending account supplemental health protection plan accident critical illness hospital indemnity auto home insurance identity theft protection legal counseling long term care coverage moving assistance pet insurance free counseling service resource emotional physical financial wellbeing k plan match pay based year service employee stock purchase plan hca healthcare stock family support fertility family building benefit progyny adoption assistance referral service child elder pet care home auto repair event planning consumer discount abenity consumer discount retirement readiness rollover assistance service preferred banking partnership education assistance tuition student loan certification support dependent scholarship colleague recognition program time away work program paid time paid family leave long short term disability coverage leaf absence employee health assistance fund offer free employee coverage time time colleague based income learn employee benefit note eligibility benefit vary location team committed caring group colleague want work data engineer passion creating positive patient interaction valued dedicated caring opportunity want knowledge expertise job summary qualification data engineer responsible loading processing client data helping internal data workflow role responsible supporting build automated solution related data pipeline analyzing larger complex data set role assist initial validation new data set monitor monthly incoming data feed timeliness consistency completeness process monthly data feed established slas great attention detail assist fulfillment client custom reporting need question use internal business intelligence tool pull report build view data carefully document new process request build manage test automation data ingestion work account manager business analytics team ensure client data concern addressed promptly self check work accuracy raise concern identify problem configure aws data pipeline automate current monthly import process automate reporting python cron job design implement bi solution internal external end user including kpi dashboard project summary report qualification need bachelor degree information system management equivalent minimum year data mining analytical report creation experience year experience microsoft excel regularly healthtrust supply chain critical hca healthcare strategy focus improve performance reduce cost joining non clinical administrative function healthtrust supply chain best practice methodology develop apply monitor cost efficient initiative program hca healthcare improving facility efficiency medical professional focus mission patient care hca healthcare recognized world ethical company ethisphere institute time recent year hca healthcare spent estimated billion cost delivery charitable care uninsured discount uncompensated expense brick mortar hospital people dr thomas frist sr hca healthcare co founder looking opportunity provides satisfaction personal growth encourage apply data engineer opening promptly review application highly qualified candidate contacted interview unlock possibility apply today equal opportunity employer value diversity company discriminate basis race religion color national origin gender sexual orientation age marital status veteran status disability status',\n",
       "  {'entities': [(1901, 1914, data pipeline),\n",
       "    (2018, 2027, data feed),\n",
       "    (2080, 2089, data feed),\n",
       "    (2200, 2226, business intelligence tool),\n",
       "    (2323, 2337, data ingestion),\n",
       "    (2359, 2377, business analytics),\n",
       "    (2499, 2512, data pipeline),\n",
       "    (2658, 2667, dashboard),\n",
       "    (2780, 2791, data mining)]}),\n",
       " ('summary junior data engineer build operate data pipeline solar product manufacturing line conduct end end analysis data gathering data processing analysis visualization responsibility define extract high volume manufacturing data multiple source integrate data target database application file efficient programming process design develop data visualization convey information engineer technician develop implement script etl process maintenance monitoring performance tuning collaborate cross functional team resolve data quality operational issue maintain existing data pipeline visualization enhancement request required qualification bachelor degree quantitative discipline e g computer science statistic industrial engineering management information system related field year prior experience data engineer similar role extensive experience working data source oracle sql database experience web application tool java spring boot etc deep understanding object oriented programming concept excellent analytical problem solving written verbal communication skill ability work independently collaboratively team environment preferred qualification master degree quantitative discipline e g computer science statistic industrial engineering management information system related field year prior experience data engineer similar role experience manufacturing industry solar pv semiconductor proficiency data visualization tool spotfire tableau powerbi etc experience statistical package numpy scipy scikit learn web framework django flask python experience big data technology hadoop spark kafka etc hanwha q cell provides equal employment opportunity eeo employee applicant employment regard race color religion sex national origin age disability genetics',\n",
       "  {'entities': [(43, 56, data pipeline),\n",
       "    (130, 145, data processing),\n",
       "    (155, 168, visualization),\n",
       "    (268, 288, database application),\n",
       "    (339, 357, data visualization),\n",
       "    (518, 530, data quality),\n",
       "    (567, 580, data pipeline),\n",
       "    (581, 594, visualization),\n",
       "    (682, 698, computer science),\n",
       "    (873, 876, sql),\n",
       "    (877, 885, database),\n",
       "    (918, 922, java),\n",
       "    (974, 993, programming concept),\n",
       "    (1015, 1030, problem solving),\n",
       "    (1192, 1208, computer science),\n",
       "    (1404, 1422, data visualization),\n",
       "    (1558, 1566, big data)]}),\n",
       " ('integrated technology strategy provider information technology consulting digital transformation improving business performance developing strategy enhancing value core integrated technology strategy looking hire data engineer consulting team participate effectively contribute design development implementation complex application new technology provide technical expertise system design individual initiative education amp experience bachelor degree engineering bachelor degree technology recognized university candidate recent work experience based customer minimum year relevant experience required technical skill requirement hand experience cognos qlik view tableau power bi amp linux linux default o expert level knowledge linux needed data engineer experience business intelligence mandatory linux tableau cognos qlik view power bi amp db deep understanding cognos qlikview tableau important client looking evaluate use moving forward migrating data aws cloud cognos stay premise self starter demonstrates personal initiative excellent communication organization skill',\n",
       "  {'entities': [(672, 680, power bi),\n",
       "    (768, 789, business intelligence),\n",
       "    (831, 839, power bi)]}),\n",
       " ('people passion purpose p health partner promise guide community better health unburden clinician align incentive engage patient physician led organization relentless mission overcome obstacle positively disrupting business health care transforming sickness care wellness guidance looking data engineer passionate work eager fun motivated fast growing organization la vega nevada consider joining team data engineer overall purpose data engineer provides value added analysis population healthcare medical pharmacy data incumbent demonstrates business data understanding provides advanced comprehensive reporting healthcare data statistical analysis including predictive analytics data manipulation interpretation presentation recommendation specific course action data engineer responsible design develop report analyze data measure clinical outcome network performance methodology lever investigate key business problem quantitative analysis utilization health care cost data data engineer responsible building deploying medicare risk adjustment cm hierarchal condition code model scoring tool report education experience bachelor degree business finance statistic health care related field required master degree strongly preferred minimum year experience healthcare analytic role demonstrating ability provide value add analysis compiled data minimum year experience medical knowledge skill ability year experience business intelligence tool preferred year experience creating stored procedure function preferred experience risk adjustment methodology cm hccs similar strongly preferred knowledge experience working sql query database scripting procedure function job physical data model creation education required bachelor better equal opportunity employer protected veteran individual disability contractor discharge manner discriminate employee applicant inquired discussed disclosed pay pay employee applicant employee access compensation information employee applicant essential job function disclose pay employee applicant individual access compensation information disclosure response formal complaint charge b furtherance investigation proceeding hearing action including investigation conducted employer c consistent contractor legal duty furnish information cfr c',\n",
       "  {'entities': [(628, 648, statistical analysis),\n",
       "    (659, 679, predictive analytics),\n",
       "    (680, 697, data manipulation),\n",
       "    (921, 942, quantitative analysis),\n",
       "    (1418, 1444, business intelligence tool),\n",
       "    (1619, 1622, sql),\n",
       "    (1629, 1637, database),\n",
       "    (2217, 2218, c),\n",
       "    (2276, 2277, c)]}),\n",
       " ('bluesnap rapid growth international fintech company headquartered waltham office israel ireland uk thrilled fastest growing company payment industry team work collaboratively building world class payment orchestration platform provides client extensive capability technology service accept payment optimized fashion bluesnap looking enthusiastic data engineer work member global business intelligence team position located waltham description develop maintain bluesnap business intelligence safeguarding solution explore way enhance data quality reliability performance analyze organize raw structured data different source build algorithm prototype develop analytical tool program collaborate bi engineer finance team member project responsibility required requirement technical expertise data model data mining segmentation technique experience etl data integration tool experience java sql experience groovy plus good analytic skill ability independently troubleshoot strong team player skill ability work harmoniously diverse employee good oral written communication skill including ability communicate complex idea simple way experience oracle bi epm product suite advantage database maintenance skill advantage qualification bachelor degree information system business intelligence related discipline year professional experience data engineer bi developer eligible time bluesnap team member receive competitive salary excellent benefit package include bluecross blueshield medical dental insurance fsa hra vision life disability opportunity save retirement k plan find best team member employee referral provide opportunity earn significant referral bonus addition provide team member flexible time plan help enjoy nice work life balance great benefit offer look forward telling interview process bluesnap bluesnap equal opportunity employer celebrate difference background perspective applicant considered employment attention race color religion sex sexual orientation gender identity national origin veteran disability status support equality treatment employment committed procedure determine equal pay employee discriminate free bias',\n",
       "  {'entities': [(379, 400, business intelligence),\n",
       "    (469, 490, business intelligence),\n",
       "    (533, 545, data quality),\n",
       "    (630, 639, algorithm),\n",
       "    (801, 812, data mining),\n",
       "    (851, 867, data integration),\n",
       "    (884, 888, java),\n",
       "    (889, 892, sql),\n",
       "    (1180, 1188, database),\n",
       "    (1266, 1287, business intelligence)]}),\n",
       " ('data engineer data migration remote description responsibility collaboration huge regularly partner engineer product manager passion learning exploring new technology participate contribute project development stage inception release design develop maintain reference implementation data pipeline associated infrastructure support existing application develop clean readable structured code solution provide desired functionality adhere specification studying information need contribute technical documentation use clear code comment ability mentor junior member team company education experience requirement bachelor degree year professional experience industry leading data engineering tool sql jenkins airflow snowflake git knowledge version control system work git similar experience acceptable knowledge aws platform experience unit integration testing familiar cicd technology workflow familiarity visualization tool tableau valued excellent problem solving skill dbt including jinja amp macro excellent verbal written communication skill proactive willing learn adapt new pattern practice technology ability thrive culture quality personal accountability experience sap hana plus team player willing work internal cross functional team interested send updated resume sejal dot parashar technokraftserve dot com thanks amp regard sejal job type time contract salary hour expected hour week compensation package performance bonus experience level year schedule hour shift experience sql year preferred data migration year preferred airflow year preferred jenkins year preferred snowflake year preferred work location remote',\n",
       "  {'entities': [(14, 28, data migration),\n",
       "    (283, 296, data pipeline),\n",
       "    (672, 688, data engineering),\n",
       "    (694, 697, sql),\n",
       "    (706, 713, airflow),\n",
       "    (905, 918, visualization),\n",
       "    (949, 964, problem solving),\n",
       "    (1489, 1492, sql),\n",
       "    (1508, 1522, data migration),\n",
       "    (1538, 1545, airflow)]}),\n",
       " ('job title data engineer year candidate required job location remote job description linkedin mandatory description bachelor degree area computer science engineering information system business equivalent field study required year experience working data solution year experience coding python scala similar scripting language year experience developing data pipeline aws cloud platform preferred azure snowflake scale year experience designing implementing data ingestion real time data streaming tool like kafka kinesis similar tool sap client cloud integration preferred year experience working mpp database snowflake preferred redshift similar mpp database year experience working serverless etl process lambda aws glue matillion similar year experience big data technology like emr hadoop spark cassandra mongodb open source big data tool knowledge professional software engineering best practice software development life cycle including coding standard code review source control management build process testing operation experience designing documenting defending design key component large distributed computing system demonstrated ability learn new technology quickly independently demonstrated ability achieve stretch goal innovative fast paced environment ability handle multiple competing priority fast paced environment excellent verbal written communication skill especially technical communication strong interpersonal skill desire work collaboratively experience participating agile software development team e g scrum job responsibility responsible building deployment maintenance critical scalable data pipeline assemble large complex set data meet non functional functional business requirement work closely smes data modeler architect analyst team member requirement build scalable real time near real time batch data solution contributes design code configuration documentation component manage data ingestion real time streaming batch processing data extraction transformation loading data lake cloud data warehouse mpp snowflake redshift similar technology owns key component infrastructure work continually improve identifying gap improving platform quality robustness maintainability speed cross train team member technology developed continuously learning new technology team member interacts technical team ensures solution meet customer requirement term functionality performance availability scalability reliability performs development qa dev ops role needed ensure total end end responsibility solution current trend big data analytics evaluate tool pace innovation mentor junior engineer create necessary documentation run book able deliver goal',\n",
       "  {'entities': [(136, 152, computer science),\n",
       "    (353, 366, data pipeline),\n",
       "    (457, 471, data ingestion),\n",
       "    (482, 496, data streaming),\n",
       "    (601, 609, database),\n",
       "    (651, 659, database),\n",
       "    (714, 722, aws glue),\n",
       "    (757, 765, big data),\n",
       "    (829, 837, big data),\n",
       "    (959, 970, code review),\n",
       "    (997, 1010, build process),\n",
       "    (1099, 1120, distributed computing),\n",
       "    (1494, 1520, agile software development),\n",
       "    (1617, 1630, data pipeline),\n",
       "    (1694, 1714, business requirement),\n",
       "    (1917, 1931, data ingestion),\n",
       "    (1952, 1968, batch processing),\n",
       "    (1969, 1984, data extraction),\n",
       "    (2008, 2017, data lake),\n",
       "    (2549, 2567, big data analytics)]}),\n",
       " ('description western national insurance group private mutual insurance company year experience serving customer property casualty insurance need midwestern northwestern southwestern united state known relationship company define success measure relationship built time know delivering friendly helpful interaction make better experience involved power nice western national nice work bring person organization partner serve opportunity interest western national seeking data engineer join team individual role opportunity develop maintain implement test deploy data management application agile team responsibility opportunity role collaborates troubleshooting application issue participates work refinement estimation process product team prepares executes unit testing new modified code team product organization standard work product team change control team ensure quality delivery creates maintains documentation product team company defined important ensures timely delivery project task communicates delay roadblock team leadership needed flex non core responsibility needed ensure timely work delivery customer review contributes technical specification flow chart data flow diagram effectively communicate application design delivery update solicits code review team product organization standard identifies opportunity process improvement share process improvement conversation product team level assist troubleshooting resolution communication urgent production issue performs special project duty assigned requirement qualification candidate good mathematical analytical problem solving skill ability work independently carry assignment completion strong attention detail quality ability communicate clearly effectively verbally writing technical non technical audience beginner intermediate knowledge use office computer equipment software package fundamental effectiveness required computer language code versioning deployment software tool relevant education training experience lieu education acceptable ideal candidate demonstrated basic organization prioritization skill ability onboard rapidly multiple potentially new responsibility accomplish multiple task specified timeframes demonstrated basic knowledge relevant agile methodology product team level passion data understanding value business internship work experience including data management data analytics reporting experience relational database including ability write tune sql compensation overview targeted hiring range role annually base pay offered vary depending job related knowledge skill credential experience candidate factor scope location role candidate looking compensation outside posted range encouraged apply considered based individual qualification considered position culture total reward western national long known relationship company caring employee relationship commitment value connectiveness empowerment accountability believe employee biggest asset currently ranked st largest private company revenue minnesota minneapolis st paul business journal western national earned accolade year year employer choice garnered multiple award wellness workplace western national named star tribune workplace addition group consistently recognized ward property casualty insurance company outstanding financial result western national offer time employee significant total reward package including medical insurance plan option standard employee benefit including dental insurance vision benefit life insurance disability insurance health saving account hsa flexible spending account fsa k plan plus company match wellbeing program including onsite fitness studio paid time including holiday vacation volunteer company paid tuition reimbursement approved job relevant coursework access institute risk insurance education paid parental leave bonus opportunity western national belief supporting balance work life providing flexible work environment includes variety hybrid remote work arrangement designed balance individual job department company need western national provides employment opportunity employee applicant employment prohibits discrimination harassment type regard race color religion age sex national origin disability status genetics protected veteran status sexual orientation gender identity expression characteristic protected federal state local law',\n",
       "  {'entities': [(560, 575, data management),\n",
       "    (757, 769, unit testing),\n",
       "    (1172, 1189, data flow diagram),\n",
       "    (1214, 1232, application design),\n",
       "    (1258, 1269, code review),\n",
       "    (1582, 1597, problem solving),\n",
       "    (2236, 2253, agile methodology),\n",
       "    (2352, 2367, data management),\n",
       "    (2373, 2382, analytics),\n",
       "    (2415, 2423, database),\n",
       "    (2453, 2456, sql)]}),\n",
       " ('western welding academy software amp data engineer integration amp maintenance job summary seeking talented versatile software data engineer expertise integration maintenance join team software data engineer play critical role integrating maintaining software application ensuring seamless functionality optimizing performance work closely cross functional team implement new feature troubleshoot issue provide technical support passion software engineering data management knack problem solving love hear key responsibility collaborate cross functional team understand software integration requirement design effective solution implement manage software integration system apis data flow ensure smooth communication platform application conduct comprehensive testing integrated software system identifying resolving issue bug monitor software performance identify potential bottleneck optimize system efficiency investigate troubleshoot software data related issue working closely stakeholder provide timely resolution develop maintain technical documentation including integration specification system diagram user guide provide technical support user addressing inquiry resolving software related issue stay date latest software engineering trend technology best practice collaborate external vendor partner integrate party software apis assist software development project needed contributing code review testing deployment process implementing ai software process staying date ai trend technology qualification bachelor degree computer science software engineering data engineering related field year professional experience software engineering data engineering similar role strong proficiency programming language java script python php typescript sql perl c sound knowledge software integration methodology apis data flow management experience database system data modeling schema design familiarity software maintenance support process excellent problem solving skill attention detail strong interpersonal communication skill collaborate effectively cross functional team stakeholder ability work independently manage multiple task concurrently familiarity agile development methodology plus experience cloud platform service aws azure plus knowledge scripting language automation tool devops practice plus perk amp benefit competitive salary attractive benefit package opportunity career growth professional development dynamic industry collaborative inclusive work environment flexible working hour occasional remote work option regular training skill building opportunity exposure cutting edge technology tool software engineering data management apply apply position software data engineer integration maintenance submit resume cover letter outlining relevant experience explaining interested joining team include example previous integration maintenance project provide link portfolio showcasing skill area application emailed humanresources pipearcsolutions com submitted online application system appreciate application shortlisted candidate contacted interview appreciate interest company look forward reviewing application western welding academy policy provide equal employment opportunity applicant employee regard race color religion sexual orientation gender identity age disability protected veteran status protected characteristic job type time pay year benefit k k matching dental insurance health insurance health saving account life insurance paid time retirement plan vision insurance experience level year work location person',\n",
       "  {'entities': [(458, 473, data management),\n",
       "    (480, 495, problem solving),\n",
       "    (1397, 1408, code review),\n",
       "    (1532, 1548, computer science),\n",
       "    (1570, 1586, data engineering),\n",
       "    (1651, 1667, data engineering),\n",
       "    (1721, 1725, java),\n",
       "    (1755, 1758, sql),\n",
       "    (1764, 1765, c),\n",
       "    (1852, 1867, database system),\n",
       "    (1868, 1881, data modeling),\n",
       "    (1955, 1970, problem solving),\n",
       "    (2295, 2301, devops),\n",
       "    (2643, 2658, data management)]}),\n",
       " ('inclusively partnering professional sport team hire data engineer inclusively inclusively digital tech platform connects candidate disability benefit workplace accommodation inclusive employer includes disability ada including mental health condition e g anxiety depression ptsd chronic illness e g diabetes long covid neurodivergence e g autism adhd applicant condition encouraged apply inclusively require applicant disclose specific disability engage collaborate business analyst find opportunity understand requirement translate requirement technical data science solution design engineer maintain analytical tool model statistical data science approach apply tried true technique develop custom algorithm needed business problem collaborate data scientist business analyst design test implement robust automated batch machine learning solution build maintain data source etl elt process business operation social medium data digital text message data live traffic data weather data help ensure database integrity building maintaining resilient data process assist architecting data storage solution efficiently store retrieve information collaborate business analyst ensure operational business metric health monitoring key decision point provide actionable insight instantaneously business analyst key business decision maker execute data mining investigate adversarial trend identify behavior pattern respond agile logic change communicate result analysis business analyst manager executive research new technology method data science data engineering data visualization improve technical capability team project duty assigned bring bachelor degree computer science data engineering machine learning statistic applied mathematics economics related field focus computer science data science programming strong microsoft sql programming administration skill understanding database design data collection methodology including underlying relational dimensional data model experience building modern etl elt process preferably python centralize organize data strong understanding xml json file structure understanding utilize cloud based solution offered snowflake microsoftazure year experience applying data science real business problem minimum year practical experience ability implement data science pipeline application general programming language python minimum year practical experience knowledge relational database sql minimum year practical experience knowledge data visualization tool tableau knowledge modern data engineering framework dbt knowledge passion finding newest innovative engineering technique type programmer love refactoring reading new framework understanding algorithm classification regression clustering anomaly detection ability handle high pressure situation attentive detail excellent work product required high level interpersonal skill handle sensitive confidential situation information ability comprehend debug complex system integration ability extract meaningful business insight data identify story pattern excellent communication organization skill distilling complex analysis concept concise business focused takeaway creativity engineer novel feature signal push current tool approach highly analytical person strong problem solving self directed learning skill exceptional attention detail highly self motivated individual excellent teamwork spirit experience sport industry required plus submitting business case study past working experience highly recommended working condition typical office environment able sit work computer day working schedule hour ebb flow nfl nba schedule include working late hour holiday night weekend offer medical dental vision insurance option k employer contribution paid parental leave pto paid holiday wellness program job type time benefit k dental insurance health insurance paid time vision insurance schedule monday friday work location person',\n",
       "  {'entities': [(555, 567, data science),\n",
       "    (636, 648, data science),\n",
       "    (700, 709, algorithm),\n",
       "    (999, 1007, database),\n",
       "    (1082, 1094, data storage),\n",
       "    (1340, 1351, data mining),\n",
       "    (1529, 1541, data science),\n",
       "    (1542, 1558, data engineering),\n",
       "    (1559, 1577, data visualization),\n",
       "    (1656, 1672, computer science),\n",
       "    (1673, 1689, data engineering),\n",
       "    (1767, 1783, computer science),\n",
       "    (1784, 1796, data science),\n",
       "    (1826, 1829, sql),\n",
       "    (1877, 1892, database design),\n",
       "    (1893, 1908, data collection),\n",
       "    (2208, 2220, data science),\n",
       "    (2295, 2307, data science),\n",
       "    (2420, 2428, database),\n",
       "    (2429, 2432, sql),\n",
       "    (2477, 2495, data visualization),\n",
       "    (2526, 2542, data engineering),\n",
       "    (2692, 2701, algorithm),\n",
       "    (3265, 3280, problem solving)]}),\n",
       " ('description experience operating facility associated enterprise class data center passionate solving power cooling problem working new technology structured cabling looking like join team apl seeking experienced data center facility engineer work dynamic fast paced team operation multiple enterprise data center includes handling daily operation planning future enhancement contributing facility project role collaborate team outstanding data center storage server network virtualization engineer enable design implementation service solution work impactful enable groundbreaking research innovation application solution supporting laboratory mission data center facility manager provide input data center design layout implementation data center equipment physically mount hardware rack connect cable resolve data center configuration issue lead coordinate activity include installation de installation move add change mac server storage network equipment fiber optic copper cabling connectivity cable management lead coordinate activity networking group data center network resource update renewal perform physical inventory assessment update data center information management dcim application collaborate facility engineer apl facility department develop execute comprehensive preventative maintenance plan supporting enterprise data center operation lead coordinate preventative maintenance task associated data center hvac system ups system battery plant electrical generator provide technical support administrative duty associated management itsd collateral technical facility participate management data center performance issue outage minimize recovery time failure qualification meet minimum qualification job bachelor science degree equivalent year related professional work experience year experience enterprise data center operation enterprise setting experience data center mechanical electrical plumbing mep element pertains data center operation demonstrated communication effectiveness ability communicate complex technical information diverse audience ability independently resolve complex problem current job knowledge research data external resource able obtain secret level security clearance selected subject government security clearance investigation meet requirement access classified information eligibility requirement include u citizenship minimum requirement building industry consulting service international bicsi registered communication distribution designer rcdd certification knowledge tia standard applicable standard data center management experience data center layout configuration managing enterprise class data center experience establishing new data center configured latest standard work apl john hopkins university applied physic laboratory apl brings world class expertise nation critical defense security space science challenge dedicated solving complex challenge pioneering new technology make truly outstanding culture offer vibrant welcoming atmosphere bring authentic self work continue grow build strong connection inspiring teammate apl celebrate difference encourage creativity bold new idea employee enjoy generous benefit including robust education assistance program unparalleled retirement contribution healthy work life balance apl campus located baltimore washington metro area learn career opportunity http www jhuapl edu career apl equal opportunity affirmative action employer qualified applicant receive consideration employment regard race creed color religion sex gender identity expression sexual orientation national origin age physical mental disability genetic information veteran status occupation marital familial status political opinion personal appearance characteristic protected applicable law apl committed promoting innovative environment embrace diversity encourages creativity support inclusion new idea committed providing reasonable accommodation individual ability including disability require reasonable accommodation participate hiring process contact accommodation jhuapl edu ensuring voice heard empowered bold great thing world better place',\n",
       "  {'entities': [(70, 81, data center),\n",
       "    (212, 223, data center),\n",
       "    (301, 312, data center),\n",
       "    (439, 450, data center),\n",
       "    (652, 663, data center),\n",
       "    (695, 706, data center),\n",
       "    (736, 747, data center),\n",
       "    (811, 822, data center),\n",
       "    (1057, 1068, data center),\n",
       "    (1146, 1157, data center),\n",
       "    (1334, 1345, data center),\n",
       "    (1413, 1424, data center),\n",
       "    (1609, 1620, data center),\n",
       "    (1826, 1837, data center),\n",
       "    (1878, 1889, data center),\n",
       "    (1942, 1953, data center),\n",
       "    (2556, 2567, data center),\n",
       "    (2590, 2601, data center),\n",
       "    (2649, 2660, data center),\n",
       "    (2689, 2700, data center),\n",
       "    (3365, 3369, http)]}),\n",
       " ('position data engineer location suitland md clearance t sci salary highly competitive job description willcor seeking data engineer successful candidate perform following requirement task research opportunity data acquisition new us existing data collaborate respective government pocs frame open ended analytical question identify weakness develop solution determine trend opportunity design construct install test maintain highly scalable data management system include recommendation improving data reliability efficiency quality context meeting customer expectation delivery timeline integrate new data management technology software engineering tool existing structure architecture including limited cloud based data processing storage collaborate oni information technology enterprise command information office cio implementing updating disaster recovery procedure geoint data develop data set process data modeling mining production proof concept initiative employ variety scripting language develop integrated system data management accessibility warehousing contribute design new product feature articulating current anticipated customer need weighting design trade offs accompany oni personnel community geoint related conference meeting order gain insight understanding latest military civil sector geoint technique technology development required contractor travel ashore afloat naval joint service command geoint related trade show conference familiar special project exercise analytic method contractor permitted represent oni community conference meeting oni personnel available contractor shall act advisory role geoint technology technique application methodology gather fact meeting conference qualification bachelor degree accredited institution quantitative technical field study statistic mathematics computer science physical science engineering geographic information system gi year practical work experience data technology include limited exact similar equivalent software following spark scala hive hadoop year practical experience developing script apls program limited exact similar equivalent software following java shell perl python c year practical work experience building data pipeline stream processing year practical work experience geospatial data imagery data year experience writing technical analysis report demonstrated problem solving skill demonstrated presentation skill including limited presenting professional academic conference',\n",
       "  {'entities': [(209, 225, data acquisition),\n",
       "    (441, 456, data management),\n",
       "    (602, 617, data management),\n",
       "    (717, 732, data processing),\n",
       "    (909, 922, data modeling),\n",
       "    (1026, 1041, data management),\n",
       "    (1823, 1839, computer science),\n",
       "    (2142, 2146, java),\n",
       "    (2165, 2166, c),\n",
       "    (2207, 2220, data pipeline),\n",
       "    (2323, 2341, technical analysis),\n",
       "    (2362, 2377, problem solving)]}),\n",
       " ('description interested helping develop field nation generation capability isr amp intelligence surveillance reconnaissance amp targeting counter isr amp excited tackling difficult multi faceted problem bringing different system achieve greater capability significant experience isr amp related system capability great opportunity amd multi domain surveillance tracking branch looking experienced technical specialist help develop improvement today isr amp fusion algorithm pave way generation data fusion target tracking development believe include focusing science researching developing approach fusing dissimilar sensor data e g rf amp ir ranging raw sensor data track fusion envisioned data fusion solution incorporate relevant isr amp contextual information including sensor data source intelligence looking engineer mathematician physicist computer scientist collectively provide needed experience skill data fusion target tracking machine learning signal processing achieve goal sensor data fusion engineer contribute development advanced data fusion target tracking algorithm physic based knowledge sensor prototyping algorithm possible help integrate sensor fusion intelligent sensor control algorithm designed provide demand level quality service support targeting apply state art machine learning algorithm exploit information possible data source apply advanced computer science technique signal image processing enable processing edge investigate new sensing modality applicability isr amp data fusion interact collaborate service community involved multi domain isr achieve breakthrough improvement capability qualification meet minimum qualification position bachelor science degree engineering mathematics physic computer science year relevant professional work experience related isr amp activity described domain knowledge isr amp architecture data fusion sensor platform experience matlab python c c java ability work effectively technical team solve complex problem excellent communication skill written verbal strong interpersonal skill hold active secret security clearance ultimately obtain t sci level clearance selected subject government security clearance investigation meet requirement access classified information eligibility requirement include u citizenship willingness travel time needed minimum requirement m phd engineering mathematics physic computer science year relevant professional experience field experience advanced data fusion algorithm involving dissimilar sensor deep experience sensor signal image processing algorithm processing platform active t sci clearance work apl apl celebrate difference encourage creativity bold new idea employee enjoy generous benefit including robust education assistance program unparalleled retirement contribution healthy work life balance apl campus located baltimore washington metro area apl equal opportunity affirmative action employer qualified applicant receive consideration employment regard race color religion sex gender identity sexual orientation national origin disability status veteran status characteristic protected applicable law li kw jst apl equal opportunity affirmative action employer qualified applicant receive consideration employment regard race creed color religion sex gender identity expression sexual orientation national origin age physical mental disability genetic information veteran status occupation marital familial status political opinion personal appearance characteristic protected applicable law apl committed promoting innovative environment embrace diversity encourages creativity support inclusion new idea committed providing reasonable accommodation individual ability including disability require reasonable accommodation participate hiring process contact accommodation jhuapl edu ensuring voice heard empowered bold great thing world better place',\n",
       "  {'entities': [(463, 472, algorithm),\n",
       "    (493, 504, data fusion),\n",
       "    (690, 701, data fusion),\n",
       "    (910, 921, data fusion),\n",
       "    (993, 1004, data fusion),\n",
       "    (1046, 1057, data fusion),\n",
       "    (1074, 1083, algorithm),\n",
       "    (1126, 1135, algorithm),\n",
       "    (1201, 1210, algorithm),\n",
       "    (1291, 1317, machine learning algorithm),\n",
       "    (1374, 1390, computer science),\n",
       "    (1503, 1514, data fusion),\n",
       "    (1729, 1745, computer science),\n",
       "    (1862, 1873, data fusion),\n",
       "    (1915, 1916, c),\n",
       "    (1917, 1918, c),\n",
       "    (1919, 1923, java),\n",
       "    (2378, 2394, computer science),\n",
       "    (2459, 2470, data fusion),\n",
       "    (2471, 2480, algorithm),\n",
       "    (2556, 2565, algorithm)]}),\n",
       " ('bluesnap rapid growth international fintech company headquartered waltham office israel ireland uk thrilled fastest growing company payment industry team work collaboratively building world class payment orchestration platform provides client extensive capability technology service accept payment optimized fashion bluesnap looking enthusiastic data engineer work member global business intelligence team position located waltham description develop maintain bluesnap business intelligence safeguarding solution explore way enhance data quality reliability performance analyze organize raw structured data different source build algorithm prototype develop analytical tool program collaborate bi engineer finance team member project responsibility required requirement technical expertise data model data mining segmentation technique experience etl data integration tool experience java sql experience groovy plus good analytic skill ability independently troubleshoot strong team player skill ability work harmoniously diverse employee good oral written communication skill including ability communicate complex idea simple way experience oracle bi epm product suite advantage database maintenance skill advantage qualification bachelor degree information system business intelligence related discipline year professional experience data engineer bi developer eligible time bluesnap team member receive competitive salary excellent benefit package include bluecross blueshield medical dental insurance fsa hra vision life disability opportunity save retirement k plan find best team member employee referral provide opportunity earn significant referral bonus addition provide team member flexible time plan help enjoy nice work life balance great benefit offer look forward telling interview process bluesnap bluesnap equal opportunity employer celebrate difference background perspective applicant considered employment attention race color religion sex sexual orientation gender identity national origin veteran disability status support equality treatment employment committed procedure determine equal pay employee discriminate free bias',\n",
       "  {'entities': [(379, 400, business intelligence),\n",
       "    (469, 490, business intelligence),\n",
       "    (533, 545, data quality),\n",
       "    (630, 639, algorithm),\n",
       "    (801, 812, data mining),\n",
       "    (851, 867, data integration),\n",
       "    (884, 888, java),\n",
       "    (889, 892, sql),\n",
       "    (1180, 1188, database),\n",
       "    (1266, 1287, business intelligence)]}),\n",
       " ('category information technology location houston texas job id req welcome conocophillips innovation excellence create platform opportunity growth come realize potential world largest independent exploration production company based proved reserve production liquid natural gas operation activity country explore develop produce crude oil natural gas globally challenged important job safely find deliver energy world employee critical success power civilization grounded spirit value safety people integrity responsibility innovation teamwork value position deliver strong performance dynamic business cost believe set apart strive significant difference community live operate create inclusive environment value voice opinion different background experience idea perspective employee drive success job summary description data engineer role global well conocophillips responsible building data pipeline source system analytics environment role monitor optimize support development test production environment working closely system administrator architect support team ensure system availability role proficient integrating preparing large varied datasets including time series data designing specialized database table information link environment person work closely global engineer well team develop optimize environment providing clean reliable data analytical solution role requires strong current technology focus practical analytical experience excellent written communication skill job description responsibility include collect explore validate condition data collaborate business subject matter expert data scientist create data set engineer feature analytical model productionize operationalize analytical model adhere conocophillips governance compliance policy data process monitor model performance basic required legally authorized work united state time basis current employer bachelor degree higher computer science math engineering statistic information system information science related field foreign equivalent year experience data engineering role year practical programming skill year experience rdbms including strong sql skill year experience data visualization analytics tool excel year experience cloud devops infrastructure code iac azure aws year previous experience handling large operational data creating etl elt pipeline intermediate knowledge python intermediate level knowledge r sql python script programming database query preferred year experience oil gas industry year experience r c java sql year experience developing data visualization tool spotfire power bi year experience oracle snowflake m sql experience connecting enterprise system year experience informatica powercenter iics year experience m azure year experience well operation time series data feed combination historical unstructured data excellent verbal written presentation skill ability communicate clearly persuasively excellent social skill area teamwork communication delivers result realistic planning accomplish goal build effective solution based available information make timely decision safe ethical build positive relationship based trust seek collaboration organizational boundary achieve goal apply october sponsorship conocophillips sponsorship employment authorization u available position eeo conocophillips equal opportunity affirmative action employer qualified applicant receive consideration employment regard race color religion sex sexual orientation national origin age disability veteran status gender identity expression genetic information legally protected status',\n",
       "  {'entities': [(890, 903, data pipeline),\n",
       "    (918, 927, analytics),\n",
       "    (1206, 1214, database),\n",
       "    (1917, 1933, computer science),\n",
       "    (2049, 2065, data engineering),\n",
       "    (2143, 2146, sql),\n",
       "    (2169, 2187, data visualization),\n",
       "    (2188, 2197, analytics),\n",
       "    (2231, 2237, devops),\n",
       "    (2416, 2419, sql),\n",
       "    (2446, 2460, database query),\n",
       "    (2522, 2523, c),\n",
       "    (2524, 2528, java),\n",
       "    (2529, 2532, sql),\n",
       "    (2560, 2578, data visualization),\n",
       "    (2593, 2601, power bi),\n",
       "    (2637, 2640, sql),\n",
       "    (2793, 2802, data feed)]}),\n",
       " ('join amazing company globally recognized brand dive intriguing project level skill work fun team onsite scottsdale arizona ready embark exciting challenge field data engineering posse genuine passion unraveling pattern crafting insightful solution data engineering envision delivering impactful outcome align company objective data driven solution opportunity ignite enthusiasm data engineering propel career new height mastermind com created partnership dean graziosi tony robbins online platform people looking market monetize knowledge base mastermind worldwide following touch life world seeking world class data engineer join team chance help company new height role build tool solution automate complex workflow create data pipeline maintain infrastructure architecture data generation storage processing build system collect manage convert raw data usable information role involves solving complex technical issue working collaboratively team report directly chief technology officer interview process include technical assessment peer code review ass technical proficiency data engineer position based phoenix arizona require work mastermind headquarters scottsdale az provide excellent compensation model based experience ranging k k opportunity available usa resident valid work authorization offer sponsorship relocation requirement year experience designing maintaining mysql experience cloud data warehouse preferably bigquery dbt python experience sql development experience experience spreadsheet manipulation sql able write custom query answer question data leaving sql environment experience querying manipulating large datasets bachelor degree technical field equivalent related work experience ability utilize fivetran stitch extracting loading data bigquery strong understanding database design experience web apis pulling data preferably python plus excellent problem solving communication skill experience querying manipulating large datasets experience data parsing scripting automation responsibility design create database object table stored procedure view conduct technical research data profiling data source utilizing technology like python apis sql innovatively propose technical data solution address business challenge perform dimensional data modeling optimize database object accessibility performance consistency collaborate business stakeholder gather understand data requirement communicate data concept report kpis technical subject business friendly language develop etl application sql python etc data extraction transformation loading document development standard kpi calculation business term table diagram relevant information related data report date emerging technology trend data engineering perform duty assigned perk amp benefit competitive salary compensation excellent medical benefit eoy profit sharing k administration matching program incredible opportunity growth development amazing office culture mission team making difference apply ready dive fascinating realm data engineering skill new height data engineer invite join exceptional team opportunity unleash power data shape future data driven decision making embark fulfilling career journey click apply button step exciting future wait review application explore endless possibility working mastermind com mastermind com created partnership dean graziosi tony robbins online platform people looking market monetize knowledge base redefining self education mean world mastermind software platform education entertainment implementation amp community mastermind serf people worldwide seek transformation fulfillment success outside traditional education path mastermind software empowers enables implement learn amp actually paid addition providing community surrounded like minded individual cheering level',\n",
       "  {'entities': [(161, 177, data engineering),\n",
       "    (248, 264, data engineering),\n",
       "    (378, 394, data engineering),\n",
       "    (672, 682, build tool),\n",
       "    (725, 738, data pipeline),\n",
       "    (1043, 1054, code review),\n",
       "    (1382, 1387, mysql),\n",
       "    (1431, 1439, bigquery),\n",
       "    (1462, 1465, sql),\n",
       "    (1525, 1528, sql),\n",
       "    (1582, 1585, sql),\n",
       "    (1769, 1777, bigquery),\n",
       "    (1799, 1814, database design),\n",
       "    (1881, 1896, problem solving),\n",
       "    (2039, 2047, database),\n",
       "    (2110, 2124, data profiling),\n",
       "    (2175, 2178, sql),\n",
       "    (2271, 2284, data modeling),\n",
       "    (2294, 2302, database),\n",
       "    (2522, 2525, sql),\n",
       "    (2537, 2552, data extraction),\n",
       "    (2722, 2738, data engineering),\n",
       "    (3020, 3036, data engineering),\n",
       "    (3141, 3168, data driven decision making)]}),\n",
       " ('overview avalonbay community inc equity reit long term track record developing redeveloping acquiring managing distinctive apartment home best u market delivering outsized risk adjusted return shareholder equal part experience vision established leadership position rooted purpose creating better way live focused building value long term creating better way live purpose bind avalonbay associate purpose seriously expect focusing collaboration innovation taking ownership choice action act way focus creating value customer investor associate positive professional consistent personal interaction avalonbay great place work role avalonbay building industry advanced data analytics capability join green field opportunity responsibility developing data lakehouse aws largest reit country need strong data engineering capability particularly python sql aws tool including glue lambda apache spark role includes developing comprehensive data acquisition solution meta data data lakehouse support business intelligence data science solution working closely digital development team support customer experience application development working closely data science team implement machine learning forecasting simulation model delivering relational solution snowflake support business intelligence implementing data governance best practice implementing automated quality assurance best practice qualification required bachelor degree computer science technical field year experience developing data pipeline aws deep knowledge python sql glue lambda apache spark experience infrastructure code iac tool aws cloudformation aws cdk preferred understanding data warehousing methodology data lake concept experience snowflake strong plus experience developing etl process leveraging tool talend ability explain complex technical material non technical audience avalonbay support know team beating heart success committed showing appreciation offer comprehensive benefit health dental amp vision k company match paid vacation holiday tuition reimbursement employee stock purchase plan growth based achievement promotion associate recognition company wide recognition program celebrates associate effort success contributing overall success organization including destination award avalonbay best recognition program discount incredible apartment home culture built purpose core value commitment integrity spirit caring focus continuous improvement additional info avalonbay proud equal opportunity employer committed inclusive diverse work environment free discrimination harassment believe order achieve purpose creating better way live recruit develop retain associate wide range background experience perspective create environment encourages voice heard understood appreciated know great thing avalonbay make employment decision regard person race ethnicity color religion sex national origin sexual orientation gender identity pregnancy including childbirth lactation related medical condition age physical mental disability genetic information including characteristic testing citizenship status military veteran status status protected law avalonbay consider employment qualified applicant criminal history manner consistent requirement law li hybrid california resident elect apply avalonbay accept avalonbay california personnel privacy notice',\n",
       "  {'entities': [(672, 681, analytics),\n",
       "    (800, 816, data engineering),\n",
       "    (848, 851, sql),\n",
       "    (883, 895, apache spark),\n",
       "    (935, 951, data acquisition),\n",
       "    (994, 1015, business intelligence),\n",
       "    (1016, 1028, data science),\n",
       "    (1107, 1130, application development),\n",
       "    (1147, 1159, data science),\n",
       "    (1270, 1291, business intelligence),\n",
       "    (1305, 1320, data governance),\n",
       "    (1429, 1445, computer science),\n",
       "    (1489, 1502, data pipeline),\n",
       "    (1529, 1532, sql),\n",
       "    (1545, 1557, apache spark),\n",
       "    (1598, 1616, aws cloudformation),\n",
       "    (1649, 1665, data warehousing),\n",
       "    (1678, 1687, data lake)]}),\n",
       " ('playstation playstation best place play best place work today recognized global leader entertainment producing playstation family product service including playstation playstation playstation vr playstation plus acclaimed playstation software title playstation studio playstation strives create inclusive environment empowers employee embrace diversity welcome encourage passion curiosity innovation technology play explore open position join growing global team playstation brand fall sony interactive entertainment wholly owned subsidiary sony corporation fully remote east coast time zone w c c sony interactive entertainment future technology group ftg team leading cloud gaming revolution putting console quality video game device data engineer future technology data engineering team play key role design development server application shaping managing transforming data large geographically distributed infrastructure sony interactive entertainment sie future technology group ftg bring passion expertise continuously improve value data platform provides data producer consumer organization providing tooling standard methodology state art technology empower data driven decision company great teammate think worth drop help colleague people happy open minded diligence like opinion ready listen understand change idea make sense speak mind openly sharing idea question concern able eager learn pick new thing quickly enjoy working fast paced environment chasing latest innovative technology strong communication documentation skill strongly value proactive idea exchange technical conversation colleague able work independently remotely time collaborating colleague located timezones prefer accountability decisional space approach problem working according vision business objective value contribution open source community blog post article source code artifact value resonate innovation inclusion iteration customer centricity extreme ownership collaboration result drive embracing failure work life balance know degree computer science related quantitative field year relevant experience consistent track record extensive amount data drive product innovation fluent jvm language scala java kotlin python experience sql knowledge software paradigm like functional programming object oriented programming ddd tdd knowledge web technology including rest json grpc websockets expertise data structure json avro protobuf unstructured semistructured data experience etl elt pipeline experience stream processing real time pipeline batch processing experience apache kafka flink spark familiarity software development tool process like git ci cd skilled designing microservices strong understanding concept concurrency parallelism event driven architecture following requirement plus experience kubernetes experience aws general design complete part data pipeline taking scale performance security usability consideration liaise product domain team help turn data information information business value design implement maintain evolve processing pipeline support data product shape solution ensure privacy security player data contribute building data catalog making wisdom accessible organization implement measure monitor reliability service provide li tp refer candidate privacy notice information process personal information data protection right sie working partner consider factor setting role base pay range including competitive benchmarking data market geographic location note individual base pay range vary based job related factor include knowledge skill experience location addition role eligible benefit offering include medical dental vision click learn flexible role remote varying pay range based geographic location example based seattle estimated base pay range role listed hourly rate usd equal opportunity statement sony equal opportunity employer person receive consideration employment regard gender including gender identity gender expression gender reassignment race including colour nationality ethnic national origin religion belief marital civil partnership status disability age sexual orientation pregnancy maternity trade union membership membership legally protected category strive create inclusive environment empower employee embrace diversity encourage respond playstation fair chance employer qualified applicant arrest conviction record considered employment',\n",
       "  {'entities': [(594, 595, c),\n",
       "    (596, 597, c),\n",
       "    (768, 784, data engineering),\n",
       "    (2031, 2047, computer science),\n",
       "    (2197, 2201, java),\n",
       "    (2227, 2230, sql),\n",
       "    (2394, 2408, data structure),\n",
       "    (2537, 2553, batch processing),\n",
       "    (2565, 2577, apache kafka),\n",
       "    (2645, 2650, ci cd),\n",
       "    (2855, 2868, data pipeline)]}),\n",
       " ('join leading fintech company democratizing finance robinhood founded simple idea financial market accessible customer heart decision robinhood lowering barrier providing greater access financial information building product service help create financial system participate continue build seeking curious growth minded thinker help shape vision structure system playing key role launch ambitious future invigorated mission value drive change world love apply team preferred location position robinhood office menlo park new york ny office work capability required management robinhood metric driven company data foundational key decision growth strategy product optimization day day operation looking data engineer build maintain foundational datasets allow reliably efficiently power decision making robinhood datasets include application event database snapshot derived datasets describe track robinhood key metric product partner closely engineer data scientist business team power analytics experimentation machine learning use case fast paced team fast growing company unique opportunity help lay foundation reliable impactful data driven decision company year come role located office location listed job description align office working environment connect recruiter information office philosophy expectation day day help define build key datasets robinhood product area lead evolution datasets use case grow build scalable data pipeline python spark airflow data different application data lake partner upstream engineering team enhance data generation pattern partner data consumer robinhood understand consumption pattern design intuitive data model ideate contribute shared data engineering tooling standard define promote data engineering best practice company year professional experience building end end data pipeline proven ability implement software engineering caliber code preferably python expert building maintaining large scale data pipeline open source framework spark flink etc strong sql presto spark sql etc skill experience solving problem data stack data infrastructure analytics visualization platform expert collaborator ability democratize data actionable insight solution bonus point passion working learning fast growing company expected salary range role based location work performed aligned compensation zone role eligible participate robinhood bonus plan robinhood equity plan zone zone zone base pay successful applicant depend variety job related factor include education training experience location business need market demand view comp zone office location table location listed compensation discussed recruiter interview process office location comp zone zone menlo park new york ny seattle wa washington c zone denver co westlake dallas tx chicago il zone lake mary fl click learn robinhood benefit robinhood promotes diversity provides equal opportunity applicant employee dedicated building company represents variety background perspective skill believe inclusive better work work environment additionally robinhood provides reasonable accommodation candidate request respect applicant privacy right review robinhood privacy policy visit robinhood applicant privacy policy applicant located uk eea visit robinhood uk eea applicant privacy policy',\n",
       "  {'entities': [(845, 853, database),\n",
       "    (984, 993, analytics),\n",
       "    (1430, 1443, data pipeline),\n",
       "    (1457, 1464, airflow),\n",
       "    (1480, 1496, application data),\n",
       "    (1684, 1700, data engineering),\n",
       "    (1733, 1749, data engineering),\n",
       "    (1818, 1831, data pipeline),\n",
       "    (1949, 1962, data pipeline),\n",
       "    (2008, 2011, sql),\n",
       "    (2025, 2028, sql),\n",
       "    (2077, 2096, data infrastructure),\n",
       "    (2097, 2106, analytics),\n",
       "    (2107, 2120, visualization),\n",
       "    (2747, 2748, c)]}),\n",
       " ('fanduel group fanduel group innovative sport tech entertainment company changing way consumer engage favorite sport team league premier gaming destination united state fanduel group consists portfolio leading brand gaming sport betting daily fantasy sport advance deposit wagering tv medium fanduel group presence state approximately million customer retail location company based new york office california new jersey florida oregon georgia portugal romania scotland network fanduel tv fanduel broadly distributed linear cable television relationship leading direct consumer ott platform fanduel group subsidiary flutter entertainment plc world largest sport betting gaming operator portfolio globally recognized brand constituent ftse index london stock exchange roster fanduel group fan new innovative way interact favorite game sport team dedicated building winning team pride able moment mean especially come career winning look like fanduel recognition hard earned result culture brings best work roster talented coworkers mistake win believe winning right mean compromise come looking teammate creatives professional cutting edge technology innovator fanduel offer wide range career opportunity best class benefit tool explore grow best self fanduel principle team run office globe expect exciting company opportunity grow successful position roster opening role partner engineering team building online application data scientist mining data training model analyst data mean help organize model present data coherent product offer stakeholder providing common information framework allows fanduel intelligently react happening field marketplace looking data engineer skill level experienced engineer discipline looking big data environment describes read want hear game plan team play mastery data data quality check pipeline support data analytics analyst engineering practice business intelligence tool tableau data security privacy e g gdpr cpp data governance data testing framework e g alation dbt continuous integration delivery production data product inclusive culture expects excellence priority growth engineer person advance career defined skill based track individual contributor manager providing equal opportunity compensation advancement apply experience intellect autonomous team end end ownership key component data architecture serve mentor junior engineer cultivating craftsmanship achieving operational excellence system reliability automation data quality cost efficiency stats looking teammate modeling querying data sql jinja templating building orchestrating etl elt pipeline dbt databricks airflow batch processing sql sftp spark real time streaming kafka kinesis data pipeline monitoring support tool dbt databand amp astronomer query optimization large datasets redshift databricks designing maintaining production database system liaising data customer requirement leverage knowledge different type data store use case trade offs performance limitation implementing data lake warehouse lakehouse architecture leveraging knowledge data archival retrieval solution managing access v cost leadership teaching cross functional team embodying willingness grow grow actively seek continued learning opportunity familiarity python player contract treat team right opportunity professional development generous insurance paid leave policy committed making sure employee fanduel ask competitive compensation beginning team expect exciting fun environment committed driving real growth opportunity build cool product fan love mentorship professional development resource help refine game flexible vacation allowance let refuel hall fame benefit program platform fanduel group equal opportunity employer believe principal state team committed equal employment opportunity regardless race color ethnicity ancestry religion creed sex national origin sexual orientation age citizenship status marital status disability gender identity gender expression veteran status believe fanduel strongest best able compete employee feel valued respected included want team include diverse individual diversity thought diversity perspective diversity experience lead better performance diverse inclusive workforce core value believe make company stronger competitive team applicable salary range position dependent variety factor including relevant experience location business need market demand role offer following benefit medical vision dental insurance life insurance disability insurance k matching program employee benefit role eligible short term long term incentive compensation including limited cash bonus stock program participation role includes flexible time including unlimited paid time time employee paid company holiday fanduel offer paid sick time accordance applicable state federal law li hybrid',\n",
       "  {'entities': [(1411, 1427, application data),\n",
       "    (1727, 1735, big data),\n",
       "    (1806, 1818, data quality),\n",
       "    (1847, 1856, analytics),\n",
       "    (1886, 1912, business intelligence tool),\n",
       "    (1921, 1934, data security),\n",
       "    (1956, 1971, data governance),\n",
       "    (2011, 2033, continuous integration),\n",
       "    (2336, 2353, data architecture),\n",
       "    (2472, 2484, data quality),\n",
       "    (2547, 2550, sql),\n",
       "    (2612, 2622, databricks),\n",
       "    (2623, 2630, airflow),\n",
       "    (2631, 2647, batch processing),\n",
       "    (2648, 2651, sql),\n",
       "    (2697, 2710, data pipeline),\n",
       "    (2806, 2816, databricks),\n",
       "    (2850, 2865, database system),\n",
       "    (2935, 2945, data store),\n",
       "    (3002, 3011, data lake)]}),\n",
       " ('amazon field engineer provide life cycle support aws data center design inception site improvement maintenance engineering resource region technical advice needed use subject matter expertise engage diverse team troubleshoot conduct root cause analysis rca create corrective action documentation site equipment failure directly support operational issue ad hoc training complex operating procedure review including critical equipment event support conceptual design existing data center upgrade design solution add capacity improve availability increase efficiency interface internal data center design engineering team server hardware team environmental health safety team promote standard maintain consistency reliability service delivered work concurrent project multiple geographical region initiate lead engineering audit including site visit amazon owned colo data center produce report outlining risk recommended mitigation remediation act resident engineer new construction project support construction commissioning turnover day life amazon vision world customer centric company role key vision field engineer leading project fit data center meet evolving customer need continue expanding fleet hyper scale ideal candidate posse strong engineering judgement able provide recommendation despite uncertainty detail data oriented experience managing engineering project consultant build trust relationship different stakeholder e g operation commissioning construction design adaptable inclined field thing close day interact different team responsible aspect data center prioritize activity support data center capacity availability safety focusing action impactful opportunity work project locally globally immediate opening field engineer northern virginia meet qualification exude passion enjoy challenge innovative project hyper scale job team aws aws amazon web service aws world comprehensive broadly adopted cloud platform pioneered cloud computing stopped innovating customer successful startup global company trust robust suite product service power business inclusive team culture aws nature learn curious employee led affinity group foster culture inclusion empower celebrate difference ongoing event learning experience including conversation race ethnicity core amazecon gender diversity conference inspire stop embracing uniqueness mentorship amp career growth career path matter stage start continuously raising performance bar strive earth best employer find endless knowledge sharing mentorship career advancing resource help develop better rounded professional open hiring candidate work following location herndon va usa basic qualification bachelor degree mechanical engineering equivalent experience cumulative year experience industrial commercial engineering mission critical facility including limited data center power generation oil gas facility experienced engineer preferred qualification organized ability set priority meet deadline budget posse leadership problem solving skill experience variety web based software tool calculation data processing direct experience design construction operation maintenance mission critical facility especially data center experience resident engineer hand field design consultant knowledge building code regulation region experience reading interpreting creating construction drawing specification submittal document ability carry design concept exploration development deployment mass production posse excellent communication writing skill attention detail maintain high quality standard basic understanding mechanical instrumentation electrical equipment design related data center including limited uninterruptable power source diesel generator electrical switchgear power distribution unit variable frequency drive automatic static transfer switch chiller air cooled water cooled pump cooling tower heat exchanger crahs air economizer etc epms scada bm control system experience software hardware registered professional engineer advanced degree engineering business related field fluent knowledge continuous operating redundant electrical system cooling system air flow containment system building management system including limited uninterruptable power source ac dc conversion p amp id loop diesel generator system complex arrangement direct evaporative cooling system etc ability develop solution execute plan complex project previous ownership fast track design build project multiple significant upgrade project meet exceeds amazon leadership principle requirement role meet exceeds amazon functional technical depth complexity role amazon committed diverse inclusive workplace amazon equal opportunity employer discriminate basis race national origin gender gender identity sexual orientation protected veteran status disability age legally protected status individual disability like request accommodation visit http www amazon job en disability u',\n",
       "  {'entities': [(0, 6, amazon),\n",
       "    (53, 64, data center),\n",
       "    (475, 486, data center),\n",
       "    (584, 595, data center),\n",
       "    (848, 854, amazon),\n",
       "    (866, 877, data center),\n",
       "    (1043, 1049, amazon),\n",
       "    (1139, 1150, data center),\n",
       "    (1566, 1577, data center),\n",
       "    (1606, 1617, data center),\n",
       "    (1863, 1881, amazon web service),\n",
       "    (1947, 1962, cloud computing),\n",
       "    (2833, 2844, data center),\n",
       "    (2993, 3008, problem solving),\n",
       "    (3070, 3085, data processing),\n",
       "    (3183, 3194, data center),\n",
       "    (3645, 3656, data center),\n",
       "    (4509, 4515, amazon),\n",
       "    (4567, 4573, amazon),\n",
       "    (4617, 4623, amazon),\n",
       "    (4662, 4668, amazon),\n",
       "    (4898, 4902, http),\n",
       "    (4907, 4913, amazon)]}),\n",
       " ('department engineering location cary north carolina united state product epic online service company epic game requisition id r make epic core epic success talented passionate people epic pride creating collaborative welcoming creative environment building award winning game crafting engine technology enables visually stunning interactive experience innovating epic mean team continually strives right community user constantly innovating raise bar engine game development ecosec ecosec team provides safer experience epic user work multiple product service improve technology craft transparent policy player user positive experience platform responsible designing building maintaining data infrastructure ensure reliability efficiency data system ecosystem security team role include building maintaining data pipeline transform load data product managing aws infrastructure machine learning platform additionally work engineer product manager data scientist design implement robust scalable data service support ecosystem security mission ensuring user privacy work directly combat bad actor platform safe user role interact product team understand safety system interact data system design implement automated end end etl process prepare data machine learning ad hoc analysis including data anonymization manage scale tool technology use label data run aws devise database structure technology storing efficiently accessing large data set million record different type text image video etc use implement data extraction apis support data versioning strategy automated tool dvc support devising strategy labeling new data human looking strong analytical background bsc msc computer science software engineering related subject candidate degree welcome long proven extensive hand experience experience etl technical design automated data quality testing qa documentation data warehousing data modeling experience python interaction web service e g rest postman experience developing data apis experience aws snowflake comparable large scale analytics platform experience monitoring managing database use elasticsearch mongodb postgresql experience sql experience data versioning tool experience developing maintaining data infrastructure etl pipeline apache airflow role open multiple location north america europe excluding ny wa epic job epic benefit epic life intent cover thing medically necessary improve quality life pay premium dependent coverage includes medical dental vision hra long term disability life insurance amp k competitive match offer robust mental program modern health provides free therapy coaching employee amp dependent year celebrate employee event company wide paid break offer unlimited pto sick time recognize individual year employment paid sabbatical epic game span country studio employee globally year making award winning game engine technology empowers visually stunning game content bring environment life like epic award winning unreal engine technology provides game developer ability build high fidelity interactive experience pc console mobile vr tool embraced content creator variety industry medium entertainment automotive architectural design continue build engine technology develop remarkable game strive build team world class talent like hear come epic epic game deeply value diverse team inclusive work culture proud equal opportunity employer note recruitment agency epic accept unsolicited resume approach unauthorized party including recruitment placement agency e party negotiated validly executed agreement pay fee unauthorized party',\n",
       "  {'entities': [(688, 707, data infrastructure),\n",
       "    (738, 749, data system),\n",
       "    (808, 821, data pipeline),\n",
       "    (1176, 1187, data system),\n",
       "    (1265, 1280, ad hoc analysis),\n",
       "    (1369, 1377, database),\n",
       "    (1509, 1524, data extraction),\n",
       "    (1677, 1693, computer science),\n",
       "    (1836, 1848, data quality),\n",
       "    (1874, 1890, data warehousing),\n",
       "    (1891, 1904, data modeling),\n",
       "    (2044, 2053, analytics),\n",
       "    (2094, 2102, database),\n",
       "    (2129, 2139, postgresql),\n",
       "    (2151, 2154, sql),\n",
       "    (2221, 2240, data infrastructure),\n",
       "    (2254, 2268, apache airflow)]}),\n",
       " ('azurity pharmaceutical inc fast growing pharmaceutical company focusing need patient requiring customized user friendly drug formulation especially child elderly azurity product benefited million patient need served commercially available therapy information visit www azurity com azurity success attributable incredibly talented dedicated team focus benefiting life patient bringing best science commitment quality mission implement data model data structure needed use case defined data architect convenient format data scientist structural element data e g data storage data piping interfacing analytics platform participate data requirement modelling testing task amp responsibility provide technical support related data structure data model meta data management relevant stakeholder creates data model providing right format structure use case solution participate early data modeling testing use case development provide input improve proposed solution implement necessary change extract relevant data solve analytical problem ensure development team required data interact business function understand data requirement develop business insight translates data structure data model close collaboration data architect work closely im team internal data acquisition e g crm erp data architect external data acquisition knowledge amp experience year experience advanced data management system e g postgresql etc deep expertise data modeling structuring experience high volume data environment ability quickly learn new technology developing maintaining formal documentation describes data data structure including data modelling strong attention detail ability think critically conceptually team oriented flexible proven track record collaborating multiple stakeholder strong verbal written',\n",
       "  {'entities': [(445, 459, data structure),\n",
       "    (560, 572, data storage),\n",
       "    (597, 606, analytics),\n",
       "    (721, 735, data structure),\n",
       "    (752, 767, data management),\n",
       "    (877, 890, data modeling),\n",
       "    (1163, 1177, data structure),\n",
       "    (1254, 1270, data acquisition),\n",
       "    (1307, 1323, data acquisition),\n",
       "    (1374, 1389, data management),\n",
       "    (1401, 1411, postgresql),\n",
       "    (1431, 1444, data modeling),\n",
       "    (1593, 1607, data structure)]}),\n",
       " ('job data foundation everside health play integral role delivering class healthcare service patient population utilize wide variety tool produce valuable data provide better patient care result senior data engineer established thought leader close partnership expert resource design develop implement data asset wide range new initiative everside health role involves heavy data exploration proficiency sql etl knowledge service based deployment apis ability discover learn quickly collaboration need think analytically outside box questioning current process continuing build business acumen combination team collaboration independent work effort role involves interaction analytics team wide range business area everside seek candidate strong quantitative background excellent analytical problem solving skill position combine business technical skill involving interaction business customer analytics partner internal external data supplier information technology partner essential duty amp responsibility deliver data warehouse analytic solution ingesting integrating curating data deliver information business stakeholder conduct etl design development maintenance including data extraction manipulation analysis source target mapping change data capture code performance ensure seamless integration data enterprise drive automation common repeated task conduct performance tuning optimization process executed data platform develop large scale data structure pipeline organize collect standardize data help generate insight address reporting need collaborate closely business user stakeholder define data design development deployment new solution support strategic business priority provide coaching training junior new team member etl architecture standard documentation qualification bachelor degree computer science related field year data engineering work experience following designing implementing etl pipeline working variety data warehousing model design fundamental working matillion azure data factory data brick similar etl tool working snowflake synapse similar mpp platform experience dataops devops agile methodology desired attribute experience messaging streaming system e g kafka azure event hub experience hybrid data processing method batch streaming experience aws azure application deployment experience api integration pay range yr actual offer vary dependent geographic location candidate year experience skill level everside benefit summary believe empowering teammate best work build better healthcare benefit offering eligibility based hr week health free everside membership person virtual care employer paid life disability insurance choice medical dental plan vision employer funded hsa fsa voluntary illness accident hospitalization plan benefit effective month following date hire financial support competitive compensation k match access financial coaching employee assistance program lifestyle paid time vacation sick leave holiday schedule learn http www eversidehealth com career',\n",
       "  {'entities': [(402, 405, sql),\n",
       "    (673, 682, analytics),\n",
       "    (789, 804, problem solving),\n",
       "    (884, 902, customer analytics),\n",
       "    (1179, 1194, data extraction),\n",
       "    (1246, 1258, data capture),\n",
       "    (1449, 1463, data structure),\n",
       "    (1808, 1824, computer science),\n",
       "    (1844, 1860, data engineering),\n",
       "    (1939, 1955, data warehousing),\n",
       "    (1999, 2017, azure data factory),\n",
       "    (2112, 2118, devops),\n",
       "    (2119, 2136, agile methodology),\n",
       "    (2237, 2252, data processing),\n",
       "    (2297, 2319, application deployment),\n",
       "    (2986, 2990, http)]}),\n",
       " ('united state position description like work empowered enthusiastic challenging atmosphere interested new technology automation electromobility connectivity opportunity drive development future volvo product excite great opportunity volvo group seeking associate engineer data analytics join test infrastructure automation team greensboro nc read detail team test infrastructure automation heart innovation vehicle engineering function volvo group north america develop solution build automate verification electronics hardware software volvo group product service mission includes building enabling shift left development virtual testing simulation driving data driven development test automation support continuous integration continuous development global diverse team highly skilled professional strong culture based company value central work believe work environment constantly strive outstanding performance obsessed customer success initiate change stay ahead willingly place trust huge passion data analytics engineer enable manage data pipeline data collected phase engineering testing develop method tool bring data actionable intelligence development team improve product responsibility collect clean model analyze data vehicle test rig determine apply appropriate analytics methodology use bi software enable stake holder discovery action based data collaborate software developer test engineer collaborate cross functional analytics community share finding best practice curious creative team player willingness learn responsibility passionate technology curious finding way computer automate mundane human task understand data infrastructure develop robust solution support need organization ability act intuition develop broad network gain trust leader peer interested role involved design development release electronic hardware software solution automate testing product service guaranteeing customer satisfaction requirement bachelor degree computer science engineering related field year related experience data management data science proficient programming language python sql etc experience data visualization powerbi qlikview matplotlib bokeh preferred master degree computer science engineering software development experience experience automotive transportation related industry compensation amp benefit competitive medical dental vision insurance generous paid time including paid caregiver parental leave policy competitive matching retirement saving plan working environment safety health wellbeing come focus professional personal development volvo group university program today challenging reality combining work personal life easier want learn program continue exploratory journey ready join team shape tomorrow society volvo group world leading manufacturer truck bus construction equipment marine industrial engine leading brand volvo renault truck mack ud truck eicher sdlg terex truck prevost nova bus ud bus volvo penta volvo group truck technology provides volvo group truck business area state art research cutting edge engineering product planning purchasing service aftermarket product support volvo group truck technology global diverse team highly skilled professional work passion trust embrace change stay ahead customer win auto req id br organization group truck technology state province north carolina city town greensboro employment assignment type regular travel required maximum travel required functional area technology application date nov disclaimer text volvo group north america equal opportunity employer e e f disability veteran',\n",
       "  {'entities': [(276, 285, analytics),\n",
       "    (705, 727, continuous integration),\n",
       "    (1007, 1016, analytics),\n",
       "    (1040, 1053, data pipeline),\n",
       "    (1276, 1285, analytics),\n",
       "    (1436, 1445, analytics),\n",
       "    (1636, 1655, data infrastructure),\n",
       "    (1959, 1975, computer science),\n",
       "    (2026, 2041, data management),\n",
       "    (2042, 2054, data science),\n",
       "    (2094, 2097, sql),\n",
       "    (2113, 2131, data visualization),\n",
       "    (2160, 2165, bokeh),\n",
       "    (2190, 2206, computer science)]}),\n",
       " ('rex solution technical integrator federal government space strategic partnership palantir premier big data analytics company product use numerous federal agency rex seeking strong developer data engineer support development integration expansion palantir foundry platform candidate cleared secret t able work site washington dc day week opportunity support federal agency customer responsibility setup transfer data feed source system location accessible foundry debug issue related delayed missing data feed write transformation derive new datasets spark distributed computation monitor build progress debug build problem foundry application development framework design application address operational question rapid development iteration cycle sme including testing troubleshooting application issue executing request information rfi surrounding platform data footprint requirement citizenship required active secret secret clearance strong engineering background preferably field computer science mathematics software engineering physic data science proficiency programming language python pyspark panda sql r similar language ability work effectively team technical non technical individual skill comfort working rapidly changing environment dynamic objective iteration user demonstrated ability continuously learn work independently decision minimal supervision rex overview established rex solution llc proven mid tier business providing data centric mission service federal government increasingly try secure leverage power data design integrate secure deploy advanced technical solution customer efficiently fulfill critical objective rex offer professional service numerous federal agency leader providing high quality innovative solution area cloud infrastructure service cyber security big data engineering rex constantly seeking qualified people join growing team built broad client base devotion delivering quality product customer service need quality individual rex committed creating culture support development employee personal professional life rex commitment maintain status industry leader compensation package benefit includes competitive salary performance bonus training educational reimbursement transamerica k cigna healthcare benefit rex equal opportunity employer qualified applicant receive consideration employment regard race religion color sex including pregnancy gender identity sexual orientation parental status national origin age disability family medical history genetic information political affiliation military service non merit based factor overall strategy commitment maintaining safe healthy workplace accordance applicable regulation employee rex fully vaccinated covid required customer client policy federal mandate condition employment subject reasonable accommodation requirement applicable federal state local law required customer selected individual submit documentation proof vaccination starting employment rex approved exemption prior start employment compliance pay transparency guideline annual base salary range position note salary information general guideline rex considers factor limited scope responsibility position candidate work experience education training key skill internal peer equity market business consideration extending offer',\n",
       "  {'entities': [(98, 116, big data analytics),\n",
       "    (411, 420, data feed),\n",
       "    (499, 508, data feed),\n",
       "    (631, 654, application development),\n",
       "    (984, 1000, computer science),\n",
       "    (1041, 1053, data science),\n",
       "    (1094, 1101, pyspark),\n",
       "    (1108, 1111, sql),\n",
       "    (1754, 1774, cloud infrastructure),\n",
       "    (1798, 1806, big data)]}),\n",
       " ('ixaris recruiting data engineer join diverse ambitious team launching europe virtual card shaping future payment innovation dna ixaris principal issuing member visa mastercard help customer country smarter payment choice processing mission payment smarter know payment driver innovation growth business industry challenge unique need different voice possible join solving voice like data engineer data engineer role business intelligence function ixaris data engineer collaborates closely stakeholder company design develop maintain payment data warehouse business intelligence solution data engineer draw expertise experience assist engineering high quality solution complex problem domain helping shape future platform constantly raise bar acquire maintain depth understanding business logic embodied data solution development infrastructure project delivery process work multi functional team design implement phase product technical initiative making effective data engineering decision accountable quality code champion best industry practice keeping date technological trend actively participate guild community practice team initiative shaping technology roadmap eager learn new technology adapt team dynamic ability support running data solution critical environment urge learn dynamic payment eco system year experience working data business intelligence environment rounded knowledge cutting edge technology data engineering practice process date latest development good knowledge data warehouse design concept good knowledge object oriented language python java scala expert sql experience database management system microsoft sql server mysql proficient data integration tool talend pentaho ssis experience data visualisation tool powerbi tableau qlikview experience analytics engine apache spark ability write application library dataframes spark streaming desirable knowledge statistical computing language r considered asset perk job assistance relocating malta health insurance coverage annual wellness stipend sport equipment gym membership free flowing healthy snack malta office location professional training development opportunity paid maternity paternity leave',\n",
       "  {'entities': [(416, 437, business intelligence),\n",
       "    (556, 577, business intelligence),\n",
       "    (965, 981, data engineering),\n",
       "    (1342, 1363, business intelligence),\n",
       "    (1418, 1434, data engineering),\n",
       "    (1568, 1572, java),\n",
       "    (1586, 1589, sql),\n",
       "    (1601, 1627, database management system),\n",
       "    (1638, 1641, sql),\n",
       "    (1649, 1654, mysql),\n",
       "    (1666, 1682, data integration),\n",
       "    (1779, 1788, analytics),\n",
       "    (1796, 1808, apache spark)]}),\n",
       " ('rockstar game create world class entertainment experience career rockstar game team working creatively rewarding ambitious project found entertainment medium welcomed dedicated inclusive environment learn collaborate talented people industry rockstar seeking data engineer join team focused building cutting edge game analytics platform tool better understand player enhance experience game time permanent position based rockstar unique game development studio andover ideal candidate skilled developing complex ingestion transformation process emphasis reliability performance collaboration data engineer machine learning engineer software engineer candidate empower team analyst data scientist deliver data driven insight application company stakeholder rockstar analytics team provide insight actionable result wide variety stakeholder organization support decision making currently adding team member multiple vertical including machine learning game data pipeline responsibility implement maintain real time batch data model deliver real time non real time data model analyst data scientist create insight analytics application stakeholder implement support streaming technology kafka spark cassandra amp azureml assist development deployment automation operational support strategy assist development big data platform hadoop pipeline technology spark airflow support variety requirement application set standard warehouse schema design massively parallel processing engine hadoop snowflake collaborating analyst data scientist creation efficient data model maintain extend ci cd process documentation qualification year work experience data modeling business intelligence machine learning big data architecture year experience hadoop ecosystem hdfs spark oozie impala etc big data ecosystem kafka cassandra etc year experience azure ecosystem azure ml azure data factory expert sql language sql pl sql experience developing managing data warehouse terabyte petabyte scale strong experience massively parallel processing amp columnar database experience building real time near real time ml pipeline experience python scala java experience shell scripting experience working linux environment skill deep understanding advanced data warehousing concept track record applying concept job ability manage numerous project concurrently strategically prioritizing necessary good communication skill dynamic team player passion technology plus note desirable skill required apply position experience python based library scikit learn experience databricks experience spark ml jupyter notebook azureml experience lambda architecture experience ci cd familiar restful apis experience artifact repository knowledge video game industry apply apply resume cover letter demonstrating meet skill like forward application rockstar recruiter reach explain step guide process rockstar proud equal opportunity employer committed hiring promoting compensating employee based qualification demonstrated ability perform job responsibility got right skill job want hear encourage application suitable candidate regardless age disability gender identity sexual orientation religion belief race',\n",
       "  {'entities': [(318, 327, analytics),\n",
       "    (765, 774, analytics),\n",
       "    (955, 968, data pipeline),\n",
       "    (1111, 1120, analytics),\n",
       "    (1307, 1315, big data),\n",
       "    (1358, 1365, airflow),\n",
       "    (1453, 1472, parallel processing),\n",
       "    (1580, 1585, ci cd),\n",
       "    (1643, 1656, data modeling),\n",
       "    (1657, 1678, business intelligence),\n",
       "    (1696, 1704, big data),\n",
       "    (1779, 1787, big data),\n",
       "    (1859, 1877, azure data factory),\n",
       "    (1885, 1888, sql),\n",
       "    (1898, 1901, sql),\n",
       "    (1902, 1908, pl sql),\n",
       "    (2007, 2026, parallel processing),\n",
       "    (2040, 2048, database),\n",
       "    (2130, 2134, java),\n",
       "    (2233, 2249, data warehousing),\n",
       "    (2544, 2554, databricks),\n",
       "    (2575, 2591, jupyter notebook),\n",
       "    (2642, 2647, ci cd)]}),\n",
       " ('stand provides end end solution enterprise partner united state office los angeles new york new jersey atlanta including internationally mexico india seeking data engineer work project large school district client qualification duty set r studio workbench connect package manager linux server manage library version write required security plan work security connect server snowflake data source provision server access data scientist including r studio server sftp command line provide training support data scientist including training visualization package like ggplot quatro shiny guide data scientist use version control git azure devops pay range contract position year pay range determined role level location range displayed job posting reflects minimum maximum target new hire salary position location range individual pay determined work location additional factor including job related skill experience relevant education training',\n",
       "  {'entities': [(538, 551, visualization), (630, 642, azure devops)]}),\n",
       " ('data engineer build maintains infrastructure data generation responsible implementing data architecture plan working multiple department ensure data presented effectively digestible manner role responsible maintaining quality data storage organization focus data source electronic health record ehr application programming interface api accounting software sale marketing software focus optimizing data delivery work time weekday work performed remotely dfw area traumatic brain injury life change instant greater reward easing human suffering helping people life cns help people brain injury find hope independence return meaningful life play critical role journey cns community pathfinder work find path possible patient reach maximum independence quality life create supportive work environment selflessness innovation hard work work seriously enjoy great atmosphere fun professional development committed promoting rich environment thrive collaboration empowerment moving corporate video story centre neuro skill http youtu jwqve gwtew benefit paid time extended sick leave medical dental vision life insurance starting st month hire paid holiday including floating personal observance day k responsibility create maintain optimal data pipeline architecture current understanding emerging technology aligned business need assemble complex datasets meet functional non functional business requirement key stakeholder identify design implement internal process improvement automating manual process optimizing data delivery designing infrastructure greater scalability etc collect document clearly align theme epic user story task meet timeline model application design enumerating stride threat spoofing tampering repudiation information disclosure denial service elevation privilege trust boundary practice secure design coding concept applying security best practice support role based access control interface logic data tier requirement bachelor degree computer science software engineering relevant experience year equivalent experience position requires year prior experience hand development demonstrated hand development development language concept platform including perl c javascript node amp angular net python restful api oauth openid saml experience public cloud platform azure aws expertise data pipeline workflow management tool devops service oriented architecture application lifecycle management alm enterprise saas experience agile methodology devops framework zero trust model rbac pattern practice reference architecture governance proven capability devops security regulatory compliance framework experience hitec hippa expert application azure paas generate scalable future proof application api connected solution experience boomi integration software service partner connection hl version message highly preferred apply',\n",
       "  {'entities': [(86, 103, data architecture),\n",
       "    (226, 238, data storage),\n",
       "    (299, 336, application programming interface api),\n",
       "    (1017, 1021, http),\n",
       "    (1235, 1248, data pipeline),\n",
       "    (1383, 1403, business requirement),\n",
       "    (1653, 1671, application design),\n",
       "    (1960, 1976, computer science),\n",
       "    (2185, 2186, c),\n",
       "    (2309, 2322, data pipeline),\n",
       "    (2348, 2354, devops),\n",
       "    (2385, 2417, application lifecycle management),\n",
       "    (2449, 2466, agile methodology),\n",
       "    (2467, 2473, devops),\n",
       "    (2575, 2581, devops)]}),\n",
       " ('description campfire data engineer maintain balance strategy execution provide outstanding service client reporting team lead client given project operate largely oversight hire report monkey looking curious mind love explore data dig insight opportunity recognized seattle business magazine best place work requirement partner directly client understand business problem project goal requirement functional technical architect end end solution ingestion storage prepping modeling serve client need build scalable data pipeline programmatically language sql python spark including extracting data apis implement data structure data modeling etl elt process sql technology minimum qualification year work experience data warehousing data modeling building etl pipeline python c net year work experience working cloud based technology azure aws google cloud platform year work experience writing sql query large datasets clickstream data terabyte year experience working data visualization tool power bi tableau looker job experience version control system e g git devops best practice preferred qualification consulting agency experience familiarity digital marketing concept hand experience working digital medium clickstream tapstream data experience distributed data cloud computing tool azure aws map reduce m cosmos hadoop hive spark experience dax mdx eg ranking date changing data transformation relating data compensation amp benefit compensation base salary pay range position dependent specific level skill work amp industry experience time employee eligible annual profit sharing bonus month employment eligible discretionary bonus benefit time employee eligible competitive medical dental vision coverage premium employee covered group life short amp long term disability coverage premium employee covered pto paid time flexible vacation policy recommended day calendar year psst paid sick amp safe time flexible sick policy minimum day calendar year holiday day calendar year k match match max match contribution training stipend calendar year ergonomics stipend year employment',\n",
       "  {'entities': [(514, 527, data pipeline),\n",
       "    (554, 557, sql),\n",
       "    (612, 626, data structure),\n",
       "    (627, 640, data modeling),\n",
       "    (657, 660, sql),\n",
       "    (715, 731, data warehousing),\n",
       "    (732, 745, data modeling),\n",
       "    (775, 776, c),\n",
       "    (894, 897, sql),\n",
       "    (969, 987, data visualization),\n",
       "    (993, 1001, power bi),\n",
       "    (1063, 1069, devops),\n",
       "    (1269, 1284, cloud computing),\n",
       "    (1382, 1401, data transformation)]}),\n",
       " ('homeward technology enabled healthcare provider delivering quality affordable comprehensive care starting rural america today million american living rural community facing crisis access care u healthcare system rural american experience significantly poorer clinical outcome trend rapidly accelerating rural hospital close physician shortage increase exacerbating health disparity fact american living rural community suffer mortality rate percent higher urban community lack access quality care vision care enables achieve best health rearchitecting way delivered leveraging breakthrough diagnostics remote monitoring variety domain homeward creating new healthcare delivery model purpose built rural america directly address issue historically limited access quality importantly aligning incentive partner health plan member getting keeping people healthy taking responsibility financial risk total cost member healthcare outcome homeward co founded leadership team defined delivered livongo product backed recently million series b co led arch venture human capital participation general catalyst total million funding leadership team funding homeward committed bringing high quality healthcare rural community need join tackling healthcare rural america seeking sr data engineer key contributor data team passionate positive societal impact technology good solving issue health inequity community perennially underserved sr data engineer partner closely product engineering analytics team support data driven decision making company leading design development technical data infrastructure work data science product engineering operation clinical team define requirement homeward data infrastructure collaborate engineering team ensure uniform consistent taxonomy data collection practice yield high quality analytics insight future design build infrastructure ingest store report member health data claim data adt admission discharge transfer data lab result medication clinical data design implement workflow ingestion transformation data source work modern data engineering stack piece include airbyte snowflake metabase bring year hand data engineering experience healthcare setting bachelor degree computer science computer engineering related technical discipline equivalent industry experience depth experience working data warehouse preferably snowflake relational database preferably postgres real world experience working data fhir hl standard healthy understanding security phi especially interacts hipaa regulation internalize security order priority evangelize position implementation documentation experience structuring permissioning data data lake mart data available role specific basis comfortable working variety data format data ingestion pipeline primarily deal csv parquet file implemented infrastructure cloud environment preferably aws built structured data pipeline etl elt tool preferably airflow dbt shape company deep commitment people community serve care enables achieve best health compassion empathy curiosity eagerness listen drive deliver high quality experience clinical care cost effectiveness strong focus sustainability business scalability service maximize reach impact nurturing diverse workforce wide range background experience point view taking mission business seriously taking seriously fun build benefit competitive salary equity grant unlimited pto comprehensive benefit package including medical dental amp vision insurance monthly premium covered employee company sponsored k plan flexible working arrangement homeward diverse set background experience enrich team allow achieve goal gain experience area detailed hope share unique background application additive team homeward affirmative action equal opportunity employer qualified applicant receive consideration employment regard race color religion sex disability age sexual orientation gender identity national origin veteran status genetic information homeward committed providing access equal opportunity reasonable accommodation individual disability employment service program activity',\n",
       "  {'entities': [(1479, 1488, analytics),\n",
       "    (1502, 1529, data driven decision making),\n",
       "    (1575, 1594, data infrastructure),\n",
       "    (1600, 1612, data science),\n",
       "    (1685, 1704, data infrastructure),\n",
       "    (1769, 1784, data collection),\n",
       "    (1813, 1822, analytics),\n",
       "    (2065, 2081, data engineering),\n",
       "    (2145, 2161, data engineering),\n",
       "    (2208, 2224, computer science),\n",
       "    (2225, 2245, computer engineering),\n",
       "    (2378, 2386, database),\n",
       "    (2659, 2668, data lake),\n",
       "    (2749, 2763, data ingestion),\n",
       "    (2882, 2895, data pipeline),\n",
       "    (2920, 2927, airflow)]}),\n",
       " ('position data engineer location suitland md clearance t sci salary highly competitive job description willcor seeking data engineer successful candidate perform following requirement task research opportunity data acquisition new us existing data collaborate respective government pocs frame open ended analytical question identify weakness develop solution determine trend opportunity design construct install test maintain highly scalable data management system include recommendation improving data reliability efficiency quality context meeting customer expectation delivery timeline integrate new data management technology software engineering tool existing structure architecture including limited cloud based data processing storage collaborate oni information technology enterprise command information office cio implementing updating disaster recovery procedure geoint data develop data set process data modeling mining production proof concept initiative employ variety scripting language develop integrated system data management accessibility warehousing contribute design new product feature articulating current anticipated customer need weighting design trade offs accompany oni personnel community geoint related conference meeting order gain insight understanding latest military civil sector geoint technique technology development required contractor travel ashore afloat naval joint service command geoint related trade show conference familiar special project exercise analytic method contractor permitted represent oni community conference meeting oni personnel available contractor shall act advisory role geoint technology technique application methodology gather fact meeting conference qualification bachelor degree accredited institution quantitative technical field study statistic mathematics computer science physical science engineering geographic information system gi year practical work experience data technology include limited exact similar equivalent software following spark scala hive hadoop year practical experience developing script apls program limited exact similar equivalent software following java shell perl python c year practical work experience building data pipeline stream processing year practical work experience geospatial data imagery data year experience writing technical analysis report demonstrated problem solving skill demonstrated presentation skill including limited presenting professional academic conference',\n",
       "  {'entities': [(209, 225, data acquisition),\n",
       "    (441, 456, data management),\n",
       "    (602, 617, data management),\n",
       "    (717, 732, data processing),\n",
       "    (909, 922, data modeling),\n",
       "    (1026, 1041, data management),\n",
       "    (1823, 1839, computer science),\n",
       "    (2142, 2146, java),\n",
       "    (2165, 2166, c),\n",
       "    (2207, 2220, data pipeline),\n",
       "    (2323, 2341, technical analysis),\n",
       "    (2362, 2377, problem solving)]}),\n",
       " ('ixaris recruiting data engineer join diverse ambitious team launching europe virtual card shaping future payment innovation dna ixaris principal issuing member visa mastercard help customer country smarter payment choice processing mission payment smarter know payment driver innovation growth business industry challenge unique need different voice possible join solving voice like data engineer data engineer role business intelligence function ixaris data engineer collaborates closely stakeholder company design develop maintain payment data warehouse business intelligence solution data engineer draw expertise experience assist engineering high quality solution complex problem domain helping shape future platform constantly raise bar acquire maintain depth understanding business logic embodied data solution development infrastructure project delivery process work multi functional team design implement phase product technical initiative making effective data engineering decision accountable quality code champion best industry practice keeping date technological trend actively participate guild community practice team initiative shaping technology roadmap eager learn new technology adapt team dynamic ability support running data solution critical environment urge learn dynamic payment eco system year experience working data business intelligence environment rounded knowledge cutting edge technology data engineering practice process date latest development good knowledge data warehouse design concept good knowledge object oriented language python java scala expert sql experience database management system microsoft sql server mysql proficient data integration tool talend pentaho ssis experience data visualisation tool powerbi tableau qlikview experience analytics engine apache spark ability write application library dataframes spark streaming desirable knowledge statistical computing language r considered asset perk job assistance relocating malta health insurance coverage annual wellness stipend sport equipment gym membership free flowing healthy snack malta office location professional training development opportunity paid maternity paternity leave',\n",
       "  {'entities': [(416, 437, business intelligence),\n",
       "    (556, 577, business intelligence),\n",
       "    (965, 981, data engineering),\n",
       "    (1342, 1363, business intelligence),\n",
       "    (1418, 1434, data engineering),\n",
       "    (1568, 1572, java),\n",
       "    (1586, 1589, sql),\n",
       "    (1601, 1627, database management system),\n",
       "    (1638, 1641, sql),\n",
       "    (1649, 1654, mysql),\n",
       "    (1666, 1682, data integration),\n",
       "    (1779, 1788, analytics),\n",
       "    (1796, 1808, apache spark)]}),\n",
       " ('randstad technology data center site engineer located wilmington de relocation assistance available global financial leader exciting data center opportunity physical hand essential worker creative schedule enables extra day compared normal day week family free weekly covid testing opportunity growth advancement unique workload day thing day world class data center seeking experienced data center technician long term project role located wilmington de area basic responsibility include limited rack stack enterprise class technology network storage enterprise server running tracing dressing testing copper fiber cable patch panel device familiarity patch panel troubleshooting diagnostics hardware connectivity commission decommission hardware able perform component replacement dimms hard drive power supply adhere change control process required skill previous experience working complex high availability data center environment providing layer hardware installation ongoing maintenance problem identification incident resolution strong working knowledge data center process design hand expertise midrange system large scale server environment ibm hp oracle amp dell server hardware exceptional troubleshooting problem resolution skill ability document complex problem resolution summary repetitive task interface online ticketing system understanding switch port configuration critical thinking decision making related outage risk assessment strong cable management skill experience managing fiber amp copper data center environment desired skill certification server network technical writing documentation skill preferred proficient microsoft office role operates hour shift amp rotating schedule benefit condition waiting period apply duration long term long term opportunity growth professional development promote job type time pay hour benefit k dental insurance employee assistance program employee discount health insurance life insurance paid time referral program vision insurance schedule hour shift overtime weekend needed covid consideration regular free testing education high school equivalent preferred experience data center year preferred work location person',\n",
       "  {'entities': [(20, 31, data center),\n",
       "    (133, 144, data center),\n",
       "    (355, 366, data center),\n",
       "    (387, 398, data center),\n",
       "    (912, 923, data center),\n",
       "    (1062, 1073, data center),\n",
       "    (1517, 1528, data center),\n",
       "    (2138, 2149, data center)]})]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 31, data center)\n",
      "(133, 144, data center)\n",
      "(355, 366, data center)\n",
      "(387, 398, data center)\n",
      "(912, 923, data center)\n",
      "(1062, 1073, data center)\n",
      "(1517, 1528, data center)\n",
      "(2138, 2149, data center)\n",
      "(355, 364, analytics)\n",
      "(481, 497, data engineering)\n",
      "(498, 514, cloud technology)\n",
      "(520, 529, analytics)\n",
      "(547, 556, analytics)\n",
      "(748, 761, data modeling)\n",
      "(762, 765, sql)\n",
      "(807, 808, c)\n",
      "(947, 965, data visualization)\n",
      "(1016, 1034, data visualization)\n",
      "(1074, 1090, data engineering)\n",
      "(1111, 1127, data engineering)\n",
      "(1128, 1144, cloud technology)\n",
      "(1192, 1211, data infrastructure)\n",
      "(1230, 1239, analytics)\n",
      "(1288, 1297, analytics)\n",
      "(1353, 1373, business requirement)\n",
      "(1383, 1392, analytics)\n",
      "(1423, 1432, analytics)\n",
      "(1443, 1462, predictive modeling)\n",
      "(1480, 1500, statistical analysis)\n",
      "(2184, 2199, problem solving)\n",
      "(2226, 2229, sql)\n",
      "(2230, 2247, data manipulation)\n",
      "(2296, 2297, c)\n",
      "(2381, 2399, data visualization)\n",
      "(2400, 2416, data engineering)\n",
      "(2417, 2433, cloud technology)\n",
      "(2439, 2448, analytics)\n",
      "(2466, 2475, analytics)\n",
      "(520, 536, data engineering)\n",
      "(886, 889, sql)\n",
      "(890, 898, database)\n",
      "(906, 916, postgresql)\n",
      "(940, 956, data engineering)\n",
      "(961, 968, pyspark)\n",
      "(1001, 1015, data migration)\n",
      "(1016, 1031, cloud migration)\n",
      "(1054, 1064, aws lambda)\n",
      "(1144, 1153, data lake)\n",
      "(1169, 1178, data mart)\n",
      "(1179, 1187, big data)\n",
      "(1539, 1547, database)\n",
      "(1555, 1565, postgresql)\n",
      "(1573, 1589, data engineering)\n",
      "(1594, 1601, pyspark)\n",
      "(1717, 1723, devops)\n",
      "(1742, 1751, bitbucket)\n",
      "(1838, 1854, computer science)\n",
      "(5284, 5285, c)\n",
      "(242, 255, data pipeline)\n",
      "(286, 295, dashboard)\n",
      "(296, 309, visualization)\n",
      "(483, 498, problem solving)\n",
      "(694, 703, dashboard)\n",
      "(745, 758, visualization)\n",
      "(769, 777, power bi)\n",
      "(815, 828, data pipeline)\n",
      "(880, 893, data pipeline)\n",
      "(982, 985, sql)\n",
      "(992, 1001, dashboard)\n",
      "(1044, 1059, data extraction)\n",
      "(1126, 1142, data integration)\n",
      "(1192, 1201, test data)\n",
      "(1319, 1335, data warehousing)\n",
      "(1336, 1347, data mining)\n",
      "(1456, 1476, requirement analysis)\n",
      "(1589, 1592, sql)\n",
      "(1604, 1612, database)\n",
      "(1797, 1801, java)\n",
      "(1919, 1935, computer science)\n",
      "(1936, 1952, data engineering)\n",
      "(1990, 2008, data visualization)\n",
      "(2019, 2027, power bi)\n",
      "(2085, 2108, artificial intelligence)\n",
      "(88, 105, data architecture)\n",
      "(168, 187, data infrastructure)\n",
      "(368, 381, data pipeline)\n",
      "(382, 400, azure data factory)\n",
      "(497, 515, azure data factory)\n",
      "(534, 549, azure data lake)\n",
      "(550, 559, analytics)\n",
      "(638, 650, data storage)\n",
      "(666, 669, sql)\n",
      "(670, 678, database)\n",
      "(693, 702, analytics)\n",
      "(899, 908, data mart)\n",
      "(1050, 1068, azure data factory)\n",
      "(1089, 1092, sql)\n",
      "(1093, 1108, azure data lake)\n",
      "(1155, 1168, data pipeline)\n",
      "(1246, 1251, ci cd)\n",
      "(1292, 1308, azure databricks)\n",
      "(1354, 1363, analytics)\n",
      "(1364, 1372, power bi)\n",
      "(1396, 1399, sql)\n",
      "(1400, 1408, database)\n",
      "(1409, 1427, azure data factory)\n",
      "(1461, 1471, data store)\n",
      "(1482, 1483, c)\n",
      "(1585, 1601, computer science)\n",
      "(1655, 1673, azure data factory)\n",
      "(1770, 1783, data modeling)\n",
      "(1796, 1815, database management)\n",
      "(1828, 1837, sql azure)\n",
      "(1838, 1841, sql)\n",
      "(1842, 1857, azure data lake)\n",
      "(1878, 1894, data warehousing)\n",
      "(1918, 1933, problem solving)\n",
      "(1961, 1979, project management)\n",
      "(416, 437, business intelligence)\n",
      "(556, 577, business intelligence)\n",
      "(965, 981, data engineering)\n",
      "(1342, 1363, business intelligence)\n",
      "(1418, 1434, data engineering)\n",
      "(1568, 1572, java)\n",
      "(1586, 1589, sql)\n",
      "(1601, 1627, database management system)\n",
      "(1638, 1641, sql)\n",
      "(1649, 1654, mysql)\n",
      "(1666, 1682, data integration)\n",
      "(1779, 1788, analytics)\n",
      "(1796, 1808, apache spark)\n",
      "(560, 575, data management)\n",
      "(757, 769, unit testing)\n",
      "(1172, 1189, data flow diagram)\n",
      "(1214, 1232, application design)\n",
      "(1258, 1269, code review)\n",
      "(1582, 1597, problem solving)\n",
      "(2236, 2253, agile methodology)\n",
      "(2352, 2367, data management)\n",
      "(2373, 2382, analytics)\n",
      "(2415, 2423, database)\n",
      "(2453, 2456, sql)\n",
      "(628, 648, statistical analysis)\n",
      "(659, 679, predictive analytics)\n",
      "(680, 697, data manipulation)\n",
      "(921, 942, quantitative analysis)\n",
      "(1418, 1444, business intelligence tool)\n",
      "(1619, 1622, sql)\n",
      "(1629, 1637, database)\n",
      "(2217, 2218, c)\n",
      "(2276, 2277, c)\n",
      "(60, 77, data architecture)\n",
      "(100, 108, database)\n",
      "(109, 122, data pipeline)\n",
      "(313, 322, analytics)\n",
      "(396, 409, data pipeline)\n",
      "(475, 484, dashboard)\n",
      "(543, 555, data quality)\n",
      "(681, 687, devops)\n",
      "(821, 824, sql)\n",
      "(825, 828, sql)\n",
      "(829, 835, pl sql)\n",
      "(922, 935, data pipeline)\n",
      "(997, 1012, database design)\n",
      "(1261, 1279, data visualization)\n",
      "(1463, 1479, computer science)\n",
      "(149, 163, data retrieval)\n",
      "(566, 574, big data)\n",
      "(855, 864, algorithm)\n",
      "(889, 897, database)\n",
      "(936, 956, statistical analysis)\n",
      "(1038, 1051, data pipeline)\n",
      "(1219, 1246, data driven decision making)\n",
      "(1263, 1290, data driven decision making)\n",
      "(1765, 1781, data engineering)\n",
      "(1782, 1791, analytics)\n",
      "(1893, 1914, quantitative analysis)\n",
      "(1915, 1926, data mining)\n",
      "(2102, 2118, computer science)\n",
      "(2184, 2200, data engineering)\n",
      "(2492, 2500, database)\n",
      "(2615, 2618, sql)\n",
      "(2644, 2664, database programming)\n",
      "(2722, 2726, java)\n",
      "(2749, 2775, business intelligence tool)\n",
      "(2791, 2799, power bi)\n",
      "(2840, 2858, azure data factory)\n",
      "(713, 722, data lake)\n",
      "(807, 820, data modeling)\n",
      "(907, 910, sql)\n",
      "(1007, 1022, problem solving)\n",
      "(1469, 1487, project management)\n",
      "(1685, 1689, http)\n",
      "(73, 76, sql)\n",
      "(108, 123, data management)\n",
      "(231, 234, sql)\n",
      "(272, 275, sql)\n",
      "(504, 524, business requirement)\n",
      "(525, 533, database)\n",
      "(740, 743, sql)\n",
      "(849, 852, sql)\n",
      "(925, 946, business intelligence)\n",
      "(971, 979, database)\n",
      "(1071, 1086, database design)\n",
      "(1525, 1540, problem solving)\n",
      "(1690, 1693, sql)\n",
      "(1715, 1718, sql)\n",
      "(1785, 1806, database architecture)\n",
      "(1915, 1936, business intelligence)\n",
      "(1967, 1984, agile methodology)\n",
      "(2119, 2120, c)\n",
      "(2121, 2128, asp net)\n",
      "(2312, 2315, sql)\n",
      "(2337, 2340, sql)\n",
      "(2407, 2428, database architecture)\n",
      "(845, 853, database)\n",
      "(984, 993, analytics)\n",
      "(1430, 1443, data pipeline)\n",
      "(1457, 1464, airflow)\n",
      "(1480, 1496, application data)\n",
      "(1684, 1700, data engineering)\n",
      "(1733, 1749, data engineering)\n",
      "(1818, 1831, data pipeline)\n",
      "(1949, 1962, data pipeline)\n",
      "(2008, 2011, sql)\n",
      "(2025, 2028, sql)\n",
      "(2077, 2096, data infrastructure)\n",
      "(2097, 2106, analytics)\n",
      "(2107, 2120, visualization)\n",
      "(2747, 2748, c)\n",
      "(445, 459, data structure)\n",
      "(560, 572, data storage)\n",
      "(597, 606, analytics)\n",
      "(721, 735, data structure)\n",
      "(752, 767, data management)\n",
      "(877, 890, data modeling)\n",
      "(1163, 1177, data structure)\n",
      "(1254, 1270, data acquisition)\n",
      "(1307, 1323, data acquisition)\n",
      "(1374, 1389, data management)\n",
      "(1401, 1411, postgresql)\n",
      "(1431, 1444, data modeling)\n",
      "(1593, 1607, data structure)\n",
      "(276, 285, analytics)\n",
      "(705, 727, continuous integration)\n",
      "(1007, 1016, analytics)\n",
      "(1040, 1053, data pipeline)\n",
      "(1276, 1285, analytics)\n",
      "(1436, 1445, analytics)\n",
      "(1636, 1655, data infrastructure)\n",
      "(1959, 1975, computer science)\n",
      "(2026, 2041, data management)\n",
      "(2042, 2054, data science)\n",
      "(2094, 2097, sql)\n",
      "(2113, 2131, data visualization)\n",
      "(2160, 2165, bokeh)\n",
      "(2190, 2206, computer science)\n",
      "(463, 472, algorithm)\n",
      "(493, 504, data fusion)\n",
      "(690, 701, data fusion)\n",
      "(910, 921, data fusion)\n",
      "(993, 1004, data fusion)\n",
      "(1046, 1057, data fusion)\n",
      "(1074, 1083, algorithm)\n",
      "(1126, 1135, algorithm)\n",
      "(1201, 1210, algorithm)\n",
      "(1291, 1317, machine learning algorithm)\n",
      "(1374, 1390, computer science)\n",
      "(1503, 1514, data fusion)\n",
      "(1729, 1745, computer science)\n",
      "(1862, 1873, data fusion)\n",
      "(1915, 1916, c)\n",
      "(1917, 1918, c)\n",
      "(1919, 1923, java)\n",
      "(2378, 2394, computer science)\n",
      "(2459, 2470, data fusion)\n",
      "(2471, 2480, algorithm)\n",
      "(2556, 2565, algorithm)\n",
      "(404, 420, data engineering)\n",
      "(444, 460, data acquisition)\n",
      "(465, 472, dataset)\n",
      "(771, 791, cloud infrastructure)\n",
      "(931, 944, data pipeline)\n",
      "(962, 982, business requirement)\n",
      "(1514, 1529, data governance)\n",
      "(1598, 1614, data engineering)\n",
      "(1638, 1654, data engineering)\n",
      "(1659, 1666, dataset)\n",
      "(1706, 1710, java)\n",
      "(1835, 1841, devops)\n",
      "(1856, 1861, ci cd)\n",
      "(1937, 1953, computer science)\n",
      "(433, 454, business intelligence)\n",
      "(641, 658, agile methodology)\n",
      "(995, 1015, business requirement)\n",
      "(1140, 1155, database schema)\n",
      "(1246, 1255, data feed)\n",
      "(1481, 1499, acceptance testing)\n",
      "(1780, 1783, sql)\n",
      "(1971, 1987, computer science)\n",
      "(1988, 2008, computer engineering)\n",
      "(2451, 2466, data redundancy)\n",
      "(2482, 2483, c)\n",
      "(2634, 2654, application planning)\n",
      "(2757, 2775, acceptance testing)\n",
      "(2904, 2919, database schema)\n",
      "(3038, 3047, analytics)\n",
      "(458, 473, data management)\n",
      "(480, 495, problem solving)\n",
      "(1397, 1408, code review)\n",
      "(1532, 1548, computer science)\n",
      "(1570, 1586, data engineering)\n",
      "(1651, 1667, data engineering)\n",
      "(1721, 1725, java)\n",
      "(1755, 1758, sql)\n",
      "(1764, 1765, c)\n",
      "(1852, 1867, database system)\n",
      "(1868, 1881, data modeling)\n",
      "(1955, 1970, problem solving)\n",
      "(2295, 2301, devops)\n",
      "(2643, 2658, data management)\n",
      "(398, 415, business analysis)\n",
      "(613, 630, business analysis)\n",
      "(631, 646, data management)\n",
      "(647, 659, data science)\n",
      "(776, 785, analytics)\n",
      "(844, 856, digital data)\n",
      "(1243, 1255, data science)\n",
      "(1261, 1270, analytics)\n",
      "(1271, 1287, data engineering)\n",
      "(1288, 1303, data management)\n",
      "(1480, 1495, data processing)\n",
      "(1502, 1517, data processing)\n",
      "(1663, 1676, data analysis)\n",
      "(1677, 1691, data cleansing)\n",
      "(1707, 1718, data domain)\n",
      "(1729, 1745, data integration)\n",
      "(1746, 1761, data management)\n",
      "(1762, 1779, data manipulation)\n",
      "(1794, 1807, data strategy)\n",
      "(1819, 1833, data structure)\n",
      "(1834, 1843, algorithm)\n",
      "(1844, 1862, data visualization)\n",
      "(1918, 1933, problem solving)\n",
      "(183, 191, data hub)\n",
      "(204, 218, data ingestion)\n",
      "(449, 464, data collection)\n",
      "(596, 620, data management platform)\n",
      "(685, 702, cloud application)\n",
      "(740, 756, data integration)\n",
      "(775, 790, data governance)\n",
      "(864, 877, data pipeline)\n",
      "(955, 975, business requirement)\n",
      "(982, 1001, data infrastructure)\n",
      "(1089, 1098, analytics)\n",
      "(1112, 1125, data pipeline)\n",
      "(1293, 1312, data infrastructure)\n",
      "(1330, 1343, data strategy)\n",
      "(1344, 1359, data governance)\n",
      "(1368, 1380, data quality)\n",
      "(1896, 1904, data hub)\n",
      "(1977, 1991, data ingestion)\n",
      "(2001, 2009, aws glue)\n",
      "(2075, 2083, database)\n",
      "(2115, 2129, data retrieval)\n",
      "(2142, 2145, sql)\n",
      "(2341, 2344, sql)\n",
      "(2371, 2386, data processing)\n",
      "(2387, 2400, risk analysis)\n",
      "(2418, 2431, data pipeline)\n",
      "(2538, 2563, financial data management)\n",
      "(2610, 2626, computer science)\n",
      "(2773, 2776, sql)\n",
      "(2806, 2820, data structure)\n",
      "(2892, 2915, database administration)\n",
      "(3037, 3054, data architecture)\n",
      "(3090, 3102, data mapping)\n",
      "(3196, 3219, application development)\n",
      "(3227, 3230, sql)\n",
      "(3237, 3241, java)\n",
      "(3275, 3290, microsoft azure)\n",
      "(3368, 3373, mysql)\n",
      "(3408, 3422, data streaming)\n",
      "(3451, 3456, nosql)\n",
      "(3493, 3509, data warehousing)\n",
      "(3710, 3719, analytics)\n",
      "(3976, 3994, project management)\n",
      "(5156, 5160, http)\n",
      "(209, 225, data acquisition)\n",
      "(441, 456, data management)\n",
      "(602, 617, data management)\n",
      "(717, 732, data processing)\n",
      "(909, 922, data modeling)\n",
      "(1026, 1041, data management)\n",
      "(1823, 1839, computer science)\n",
      "(2142, 2146, java)\n",
      "(2165, 2166, c)\n",
      "(2207, 2220, data pipeline)\n",
      "(2323, 2341, technical analysis)\n",
      "(2362, 2377, problem solving)\n",
      "(379, 400, business intelligence)\n",
      "(469, 490, business intelligence)\n",
      "(533, 545, data quality)\n",
      "(630, 639, algorithm)\n",
      "(801, 812, data mining)\n",
      "(851, 867, data integration)\n",
      "(884, 888, java)\n",
      "(889, 892, sql)\n",
      "(1180, 1188, database)\n",
      "(1266, 1287, business intelligence)\n",
      "(316, 324, big data)\n",
      "(394, 410, data acquisition)\n",
      "(263, 268, ci cd)\n",
      "(341, 344, sql)\n",
      "(345, 360, database design)\n",
      "(928, 929, c)\n",
      "(1261, 1264, sql)\n",
      "(1581, 1584, sql)\n",
      "(688, 707, data infrastructure)\n",
      "(738, 749, data system)\n",
      "(808, 821, data pipeline)\n",
      "(1176, 1187, data system)\n",
      "(1265, 1280, ad hoc analysis)\n",
      "(1369, 1377, database)\n",
      "(1509, 1524, data extraction)\n",
      "(1677, 1693, computer science)\n",
      "(1836, 1848, data quality)\n",
      "(1874, 1890, data warehousing)\n",
      "(1891, 1904, data modeling)\n",
      "(2044, 2053, analytics)\n",
      "(2094, 2102, database)\n",
      "(2129, 2139, postgresql)\n",
      "(2151, 2154, sql)\n",
      "(2221, 2240, data infrastructure)\n",
      "(2254, 2268, apache airflow)\n",
      "(388, 411, artificial intelligence)\n",
      "(1269, 1282, data curation)\n",
      "(1428, 1444, data warehousing)\n",
      "(1445, 1454, data lake)\n",
      "(1455, 1463, big data)\n",
      "(1597, 1613, computer science)\n",
      "(1857, 1865, big data)\n",
      "(1959, 1972, data pipeline)\n",
      "(1978, 1985, airflow)\n",
      "(1986, 1997, apache beam)\n",
      "(2166, 2171, nosql)\n",
      "(2172, 2180, database)\n",
      "(2218, 2226, database)\n",
      "(2234, 2239, mysql)\n",
      "(157, 160, sql)\n",
      "(573, 594, distributed computing)\n",
      "(616, 631, data processing)\n",
      "(755, 781, agile software development)\n",
      "(825, 847, application deployment)\n",
      "(1017, 1029, unit testing)\n",
      "(1104, 1113, algorithm)\n",
      "(1383, 1399, computer science)\n",
      "(1400, 1420, computer engineering)\n",
      "(1479, 1495, computer science)\n",
      "(1496, 1516, computer engineering)\n",
      "(1723, 1744, distributed computing)\n",
      "(1774, 1778, java)\n",
      "(1801, 1805, java)\n",
      "(1813, 1817, java)\n",
      "(1840, 1852, apache camel)\n",
      "(2067, 2089, continuous integration)\n",
      "(2110, 2115, ci cd)\n",
      "(2329, 2346, agile methodology)\n",
      "(2684, 2688, http)\n",
      "(2689, 2693, http)\n",
      "(594, 595, c)\n",
      "(596, 597, c)\n",
      "(768, 784, data engineering)\n",
      "(2031, 2047, computer science)\n",
      "(2197, 2201, java)\n",
      "(2227, 2230, sql)\n",
      "(2394, 2408, data structure)\n",
      "(2537, 2553, batch processing)\n",
      "(2565, 2577, apache kafka)\n",
      "(2645, 2650, ci cd)\n",
      "(2855, 2868, data pipeline)\n",
      "(357, 370, data pipeline)\n",
      "(413, 427, data structure)\n",
      "(442, 459, data architecture)\n",
      "(566, 579, data pipeline)\n",
      "(673, 689, data engineering)\n",
      "(759, 762, sql)\n",
      "(911, 924, data pipeline)\n",
      "(149, 162, data pipeline)\n",
      "(298, 311, data pipeline)\n",
      "(312, 322, databricks)\n",
      "(331, 345, cloud database)\n",
      "(369, 372, sql)\n",
      "(629, 638, data lake)\n",
      "(871, 878, pyspark)\n",
      "(1479, 1488, analytics)\n",
      "(1502, 1529, data driven decision making)\n",
      "(1575, 1594, data infrastructure)\n",
      "(1600, 1612, data science)\n",
      "(1685, 1704, data infrastructure)\n",
      "(1769, 1784, data collection)\n",
      "(1813, 1822, analytics)\n",
      "(2065, 2081, data engineering)\n",
      "(2145, 2161, data engineering)\n",
      "(2208, 2224, computer science)\n",
      "(2225, 2245, computer engineering)\n",
      "(2378, 2386, database)\n",
      "(2659, 2668, data lake)\n",
      "(2749, 2763, data ingestion)\n",
      "(2882, 2895, data pipeline)\n",
      "(2920, 2927, airflow)\n",
      "(514, 527, data pipeline)\n",
      "(554, 557, sql)\n",
      "(612, 626, data structure)\n",
      "(627, 640, data modeling)\n",
      "(657, 660, sql)\n",
      "(715, 731, data warehousing)\n",
      "(732, 745, data modeling)\n",
      "(775, 776, c)\n",
      "(894, 897, sql)\n",
      "(969, 987, data visualization)\n",
      "(993, 1001, power bi)\n",
      "(1063, 1069, devops)\n",
      "(1269, 1284, cloud computing)\n",
      "(1382, 1401, data transformation)\n",
      "(791, 800, analytics)\n",
      "(913, 926, data analysis)\n",
      "(996, 1008, data science)\n",
      "(1032, 1048, data engineering)\n",
      "(1084, 1092, database)\n",
      "(1612, 1621, dashboard)\n",
      "(1758, 1774, data engineering)\n",
      "(1811, 1828, business analysis)\n",
      "(1920, 1939, data transformation)\n",
      "(1966, 1975, analytics)\n",
      "(2101, 2110, analytics)\n",
      "(28, 31, sql)\n",
      "(48, 51, sql)\n",
      "(59, 69, databricks)\n",
      "(78, 86, power bi)\n",
      "(129, 145, data engineering)\n",
      "(146, 149, sql)\n",
      "(166, 169, sql)\n",
      "(177, 187, databricks)\n",
      "(196, 204, power bi)\n",
      "(538, 551, visualization)\n",
      "(630, 642, azure devops)\n",
      "(129, 146, data exploitation)\n",
      "(229, 242, data strategy)\n",
      "(403, 419, visual analytics)\n",
      "(563, 580, data exploitation)\n",
      "(866, 881, problem solving)\n",
      "(1145, 1153, database)\n",
      "(1256, 1282, agile software development)\n",
      "(1311, 1328, data exploitation)\n",
      "(1527, 1543, data engineering)\n",
      "(1560, 1568, big data)\n",
      "(1608, 1611, sql)\n",
      "(1612, 1617, nosql)\n",
      "(1618, 1626, database)\n",
      "(1656, 1669, data pipeline)\n",
      "(1709, 1716, airflow)\n",
      "(1776, 1790, data streaming)\n",
      "(1920, 1924, java)\n",
      "(1925, 1926, c)\n",
      "(1954, 1957, sql)\n",
      "(1998, 2012, database query)\n",
      "(2036, 2039, sql)\n",
      "(2068, 2076, database)\n",
      "(2138, 2146, big data)\n",
      "(2147, 2157, data store)\n",
      "(2286, 2299, data analysis)\n",
      "(2353, 2361, database)\n",
      "(2362, 2377, data processing)\n",
      "(2413, 2426, data modeling)\n",
      "(2464, 2475, data system)\n",
      "(2703, 2712, dashboard)\n",
      "(2720, 2729, analytics)\n",
      "(3177, 3181, http)\n",
      "(1901, 1914, data pipeline)\n",
      "(2018, 2027, data feed)\n",
      "(2080, 2089, data feed)\n",
      "(2200, 2226, business intelligence tool)\n",
      "(2323, 2337, data ingestion)\n",
      "(2359, 2377, business analytics)\n",
      "(2499, 2512, data pipeline)\n",
      "(2658, 2667, dashboard)\n",
      "(2780, 2791, data mining)\n",
      "(136, 152, computer science)\n",
      "(353, 366, data pipeline)\n",
      "(457, 471, data ingestion)\n",
      "(482, 496, data streaming)\n",
      "(601, 609, database)\n",
      "(651, 659, database)\n",
      "(714, 722, aws glue)\n",
      "(757, 765, big data)\n",
      "(829, 837, big data)\n",
      "(959, 970, code review)\n",
      "(997, 1010, build process)\n",
      "(1099, 1120, distributed computing)\n",
      "(1494, 1520, agile software development)\n",
      "(1617, 1630, data pipeline)\n",
      "(1694, 1714, business requirement)\n",
      "(1917, 1931, data ingestion)\n",
      "(1952, 1968, batch processing)\n",
      "(1969, 1984, data extraction)\n",
      "(2008, 2017, data lake)\n",
      "(2549, 2567, big data analytics)\n",
      "(402, 405, sql)\n",
      "(673, 682, analytics)\n",
      "(789, 804, problem solving)\n",
      "(884, 902, customer analytics)\n",
      "(1179, 1194, data extraction)\n",
      "(1246, 1258, data capture)\n",
      "(1449, 1463, data structure)\n",
      "(1808, 1824, computer science)\n",
      "(1844, 1860, data engineering)\n",
      "(1939, 1955, data warehousing)\n",
      "(1999, 2017, azure data factory)\n",
      "(2112, 2118, devops)\n",
      "(2119, 2136, agile methodology)\n",
      "(2237, 2252, data processing)\n",
      "(2297, 2319, application deployment)\n",
      "(2986, 2990, http)\n",
      "(1860, 1875, data processing)\n",
      "(1981, 1994, data pipeline)\n",
      "(2031, 2044, data pipeline)\n",
      "(2069, 2078, data lake)\n",
      "(2095, 2114, data transformation)\n",
      "(2368, 2388, business requirement)\n",
      "(2467, 2486, data infrastructure)\n",
      "(2670, 2673, sql)\n",
      "(2714, 2722, database)\n",
      "(2723, 2736, build process)\n",
      "(2748, 2767, data transformation)\n",
      "(2768, 2782, data structure)\n",
      "(2920, 2934, data integrity)\n",
      "(278, 294, data engineering)\n",
      "(342, 353, data system)\n",
      "(479, 491, data mapping)\n",
      "(630, 638, database)\n",
      "(989, 1005, data integration)\n",
      "(1085, 1097, data quality)\n",
      "(1158, 1177, predictive modeling)\n",
      "(1218, 1233, data management)\n",
      "(1323, 1336, data analysis)\n",
      "(1709, 1712, sql)\n",
      "(1713, 1726, data modeling)\n",
      "(1738, 1746, database)\n",
      "(1760, 1763, sql)\n",
      "(1764, 1774, postgresql)\n",
      "(1947, 1953, devops)\n",
      "(2213, 2222, analytics)\n",
      "(85, 98, data pipeline)\n",
      "(292, 312, business requirement)\n",
      "(528, 543, data processing)\n",
      "(557, 568, data mining)\n",
      "(680, 695, data validation)\n",
      "(741, 756, data conversion)\n",
      "(757, 771, data cleansing)\n",
      "(928, 944, data engineering)\n",
      "(1004, 1007, sql)\n",
      "(1023, 1026, sql)\n",
      "(1129, 1133, java)\n",
      "(1156, 1164, power bi)\n",
      "(1481, 1489, bigquery)\n",
      "(0, 11, data center)\n",
      "(128, 139, data center)\n",
      "(479, 490, data center)\n",
      "(760, 771, data center)\n",
      "(872, 883, data center)\n",
      "(1291, 1302, data center)\n",
      "(1303, 1314, data center)\n",
      "(2503, 2514, data center)\n",
      "(2579, 2590, data center)\n",
      "(2839, 2850, data center)\n",
      "(3024, 3035, data center)\n",
      "(3428, 3439, data center)\n",
      "(3601, 3612, data center)\n",
      "(3669, 3680, data center)\n",
      "(4438, 4449, data center)\n",
      "(4589, 4600, data center)\n",
      "(4826, 4837, data center)\n",
      "(5429, 5440, data center)\n",
      "(5612, 5623, data center)\n",
      "(5781, 5796, problem solving)\n",
      "(700, 711, data center)\n",
      "(1673, 1684, data center)\n",
      "(2015, 2026, data center)\n",
      "(2405, 2416, data center)\n",
      "(14, 28, data migration)\n",
      "(283, 296, data pipeline)\n",
      "(672, 688, data engineering)\n",
      "(694, 697, sql)\n",
      "(706, 713, airflow)\n",
      "(905, 918, visualization)\n",
      "(949, 964, problem solving)\n",
      "(1489, 1492, sql)\n",
      "(1508, 1522, data migration)\n",
      "(1538, 1545, airflow)\n",
      "(434, 455, business intelligence)\n",
      "(697, 712, data management)\n",
      "(743, 755, data storage)\n",
      "(835, 851, data warehousing)\n",
      "(1087, 1096, test data)\n",
      "(1880, 1896, computer science)\n",
      "(1989, 2004, database design)\n",
      "(2082, 2103, business intelligence)\n",
      "(2142, 2145, sql)\n",
      "(2163, 2166, sql)\n",
      "(2206, 2209, sql)\n",
      "(2362, 2377, database tuning)\n",
      "(2489, 2502, data modeling)\n",
      "(161, 177, data engineering)\n",
      "(248, 264, data engineering)\n",
      "(378, 394, data engineering)\n",
      "(672, 682, build tool)\n",
      "(725, 738, data pipeline)\n",
      "(1043, 1054, code review)\n",
      "(1382, 1387, mysql)\n",
      "(1431, 1439, bigquery)\n",
      "(1462, 1465, sql)\n",
      "(1525, 1528, sql)\n",
      "(1582, 1585, sql)\n",
      "(1769, 1777, bigquery)\n",
      "(1799, 1814, database design)\n",
      "(1881, 1896, problem solving)\n",
      "(2039, 2047, database)\n",
      "(2110, 2124, data profiling)\n",
      "(2175, 2178, sql)\n",
      "(2271, 2284, data modeling)\n",
      "(2294, 2302, database)\n",
      "(2522, 2525, sql)\n",
      "(2537, 2552, data extraction)\n",
      "(2722, 2738, data engineering)\n",
      "(3020, 3036, data engineering)\n",
      "(3141, 3168, data driven decision making)\n",
      "(118, 127, analytics)\n",
      "(171, 187, data engineering)\n",
      "(193, 202, analytics)\n",
      "(284, 293, analytics)\n",
      "(386, 395, analytics)\n",
      "(783, 792, analytics)\n",
      "(1093, 1106, data strategy)\n",
      "(1498, 1511, data pipeline)\n",
      "(1686, 1695, data link)\n",
      "(1750, 1762, data quality)\n",
      "(1841, 1859, data visualization)\n",
      "(2018, 2021, sql)\n",
      "(2100, 2113, data modeling)\n",
      "(276, 284, big data)\n",
      "(354, 370, data acquisition)\n",
      "(903, 907, java)\n",
      "(987, 1002, data extraction)\n",
      "(1064, 1073, analytics)\n",
      "(1273, 1281, dataflow)\n",
      "(1978, 1993, data extraction)\n",
      "(2055, 2064, analytics)\n",
      "(542, 559, data architecture)\n",
      "(1145, 1158, data pipeline)\n",
      "(1456, 1472, data engineering)\n",
      "(1473, 1488, data governance)\n",
      "(1589, 1626, relational database management system)\n",
      "(1778, 1796, data visualization)\n",
      "(1797, 1818, business intelligence)\n",
      "(2404, 2408, http)\n",
      "(74, 84, databricks)\n",
      "(85, 92, pyspark)\n",
      "(97, 100, sql)\n",
      "(250, 260, databricks)\n",
      "(343, 353, databricks)\n",
      "(376, 385, data lake)\n",
      "(424, 437, data pipeline)\n",
      "(548, 556, database)\n",
      "(557, 576, data classification)\n",
      "(577, 591, data profiling)\n",
      "(672, 681, analytics)\n",
      "(800, 816, data engineering)\n",
      "(848, 851, sql)\n",
      "(883, 895, apache spark)\n",
      "(935, 951, data acquisition)\n",
      "(994, 1015, business intelligence)\n",
      "(1016, 1028, data science)\n",
      "(1107, 1130, application development)\n",
      "(1147, 1159, data science)\n",
      "(1270, 1291, business intelligence)\n",
      "(1305, 1320, data governance)\n",
      "(1429, 1445, computer science)\n",
      "(1489, 1502, data pipeline)\n",
      "(1529, 1532, sql)\n",
      "(1545, 1557, apache spark)\n",
      "(1598, 1616, aws cloudformation)\n",
      "(1649, 1665, data warehousing)\n",
      "(1678, 1687, data lake)\n",
      "(798, 814, data engineering)\n",
      "(954, 975, business intelligence)\n",
      "(1122, 1149, data driven decision making)\n",
      "(1271, 1274, sql)\n",
      "(1361, 1377, data engineering)\n",
      "(1641, 1653, data science)\n",
      "(1722, 1734, data science)\n",
      "(1786, 1795, algorithm)\n",
      "(2085, 2093, database)\n",
      "(2168, 2180, data storage)\n",
      "(2426, 2437, data mining)\n",
      "(2615, 2627, data science)\n",
      "(2628, 2644, data engineering)\n",
      "(2645, 2663, data visualization)\n",
      "(2742, 2758, computer science)\n",
      "(2759, 2775, data engineering)\n",
      "(2853, 2869, computer science)\n",
      "(2870, 2882, data science)\n",
      "(2912, 2915, sql)\n",
      "(2963, 2978, database design)\n",
      "(2979, 2994, data collection)\n",
      "(3294, 3306, data science)\n",
      "(3381, 3393, data science)\n",
      "(3506, 3514, database)\n",
      "(3515, 3518, sql)\n",
      "(3563, 3581, data visualization)\n",
      "(3612, 3628, data engineering)\n",
      "(3778, 3787, algorithm)\n",
      "(4351, 4366, problem solving)\n",
      "(43, 56, data pipeline)\n",
      "(130, 145, data processing)\n",
      "(155, 168, visualization)\n",
      "(268, 288, database application)\n",
      "(339, 357, data visualization)\n",
      "(518, 530, data quality)\n",
      "(567, 580, data pipeline)\n",
      "(581, 594, visualization)\n",
      "(682, 698, computer science)\n",
      "(873, 876, sql)\n",
      "(877, 885, database)\n",
      "(918, 922, java)\n",
      "(974, 993, programming concept)\n",
      "(1015, 1030, problem solving)\n",
      "(1192, 1208, computer science)\n",
      "(1404, 1422, data visualization)\n",
      "(1558, 1566, big data)\n",
      "(98, 116, big data analytics)\n",
      "(411, 420, data feed)\n",
      "(499, 508, data feed)\n",
      "(631, 654, application development)\n",
      "(984, 1000, computer science)\n",
      "(1041, 1053, data science)\n",
      "(1094, 1101, pyspark)\n",
      "(1108, 1111, sql)\n",
      "(1754, 1774, cloud infrastructure)\n",
      "(1798, 1806, big data)\n",
      "(404, 413, analytics)\n",
      "(426, 447, business intelligence)\n",
      "(566, 582, data engineering)\n",
      "(628, 636, database)\n",
      "(658, 674, data engineering)\n",
      "(690, 706, data engineering)\n",
      "(785, 793, database)\n",
      "(926, 938, data science)\n",
      "(949, 950, c)\n",
      "(1086, 1089, sql)\n",
      "(1159, 1167, database)\n",
      "(1178, 1193, amazon redshift)\n",
      "(1204, 1207, sql)\n",
      "(1312, 1319, airflow)\n",
      "(1372, 1389, agile methodology)\n",
      "(0, 6, amazon)\n",
      "(53, 64, data center)\n",
      "(475, 486, data center)\n",
      "(584, 595, data center)\n",
      "(848, 854, amazon)\n",
      "(866, 877, data center)\n",
      "(1043, 1049, amazon)\n",
      "(1139, 1150, data center)\n",
      "(1566, 1577, data center)\n",
      "(1606, 1617, data center)\n",
      "(1863, 1881, amazon web service)\n",
      "(1947, 1962, cloud computing)\n",
      "(2833, 2844, data center)\n",
      "(2993, 3008, problem solving)\n",
      "(3070, 3085, data processing)\n",
      "(3183, 3194, data center)\n",
      "(3645, 3656, data center)\n",
      "(4509, 4515, amazon)\n",
      "(4567, 4573, amazon)\n",
      "(4617, 4623, amazon)\n",
      "(4662, 4668, amazon)\n",
      "(4898, 4902, http)\n",
      "(4907, 4913, amazon)\n",
      "(793, 804, data system)\n",
      "(904, 912, database)\n",
      "(925, 934, analytics)\n",
      "(950, 964, data wrangling)\n",
      "(1030, 1038, database)\n",
      "(1192, 1213, business intelligence)\n",
      "(1663, 1672, analytics)\n",
      "(1719, 1736, data architecture)\n",
      "(1820, 1832, data quality)\n",
      "(1911, 1925, data structure)\n",
      "(2157, 2177, business requirement)\n",
      "(2601, 2617, computer science)\n",
      "(2628, 2637, analytics)\n",
      "(2638, 2650, data science)\n",
      "(2937, 2952, microsoft azure)\n",
      "(2953, 2965, data storage)\n",
      "(2996, 3011, azure data lake)\n",
      "(3018, 3021, sql)\n",
      "(3022, 3030, database)\n",
      "(3193, 3196, sql)\n",
      "(789, 813, data pipeline management)\n",
      "(823, 834, data system)\n",
      "(910, 926, data acquisition)\n",
      "(1039, 1052, data modeling)\n",
      "(1062, 1071, algorithm)\n",
      "(1082, 1096, data cleansing)\n",
      "(1097, 1113, cloud management)\n",
      "(1198, 1210, data quality)\n",
      "(1356, 1372, computer science)\n",
      "(1648, 1659, data mining)\n",
      "(1701, 1719, data normalization)\n",
      "(1753, 1756, sql)\n",
      "(1757, 1761, java)\n",
      "(1790, 1793, sql)\n",
      "(1794, 1809, database design)\n",
      "(1894, 1903, analytics)\n",
      "(1921, 1934, data modeling)\n",
      "(1935, 1953, data visualization)\n",
      "(912, 925, data pipeline)\n",
      "(966, 982, cloud technology)\n",
      "(991, 1000, analytics)\n",
      "(1171, 1174, sql)\n",
      "(1249, 1262, visualization)\n",
      "(1348, 1361, data pipeline)\n",
      "(1403, 1411, database)\n",
      "(1774, 1787, build process)\n",
      "(1796, 1815, data transformation)\n",
      "(1836, 1850, data structure)\n",
      "(1942, 1962, business requirement)\n",
      "(2185, 2201, computer science)\n",
      "(2261, 2277, computer science)\n",
      "(2428, 2441, data modeling)\n",
      "(2456, 2464, database)\n",
      "(2465, 2476, data mining)\n",
      "(2580, 2595, problem solving)\n",
      "(2689, 2702, data analysis)\n",
      "(2703, 2717, data profiling)\n",
      "(3050, 3063, data analysis)\n",
      "(3138, 3141, sql)\n",
      "(3224, 3240, data engineering)\n",
      "(3265, 3281, data engineering)\n",
      "(3282, 3294, data science)\n",
      "(3419, 3420, c)\n",
      "(3421, 3422, c)\n",
      "(3423, 3427, java)\n",
      "(3428, 3439, data access)\n",
      "(3440, 3449, analytics)\n",
      "(86, 103, data architecture)\n",
      "(226, 238, data storage)\n",
      "(299, 336, application programming interface api)\n",
      "(1017, 1021, http)\n",
      "(1235, 1248, data pipeline)\n",
      "(1383, 1403, business requirement)\n",
      "(1653, 1671, application design)\n",
      "(1960, 1976, computer science)\n",
      "(2185, 2186, c)\n",
      "(2309, 2322, data pipeline)\n",
      "(2348, 2354, devops)\n",
      "(2385, 2417, application lifecycle management)\n",
      "(2449, 2466, agile methodology)\n",
      "(2467, 2473, devops)\n",
      "(2575, 2581, devops)\n",
      "(1411, 1427, application data)\n",
      "(1727, 1735, big data)\n",
      "(1806, 1818, data quality)\n",
      "(1847, 1856, analytics)\n",
      "(1886, 1912, business intelligence tool)\n",
      "(1921, 1934, data security)\n",
      "(1956, 1971, data governance)\n",
      "(2011, 2033, continuous integration)\n",
      "(2336, 2353, data architecture)\n",
      "(2472, 2484, data quality)\n",
      "(2547, 2550, sql)\n",
      "(2612, 2622, databricks)\n",
      "(2623, 2630, airflow)\n",
      "(2631, 2647, batch processing)\n",
      "(2648, 2651, sql)\n",
      "(2697, 2710, data pipeline)\n",
      "(2806, 2816, databricks)\n",
      "(2850, 2865, database system)\n",
      "(2935, 2945, data store)\n",
      "(3002, 3011, data lake)\n",
      "(672, 680, power bi)\n",
      "(768, 789, business intelligence)\n",
      "(831, 839, power bi)\n",
      "(176, 188, apache spark)\n",
      "(194, 210, data engineering)\n",
      "(262, 272, databricks)\n",
      "(342, 346, java)\n",
      "(431, 439, big data)\n",
      "(688, 705, cloud development)\n",
      "(821, 826, ci cd)\n",
      "(827, 840, build process)\n",
      "(976, 998, continuous integration)\n",
      "(1359, 1375, computer science)\n",
      "(1376, 1396, computer engineering)\n",
      "(615, 623, big data)\n",
      "(693, 709, data acquisition)\n",
      "(1250, 1254, java)\n",
      "(1334, 1349, data extraction)\n",
      "(1411, 1420, analytics)\n",
      "(1643, 1651, dataflow)\n",
      "(1730, 1735, nosql)\n",
      "(1736, 1744, database)\n",
      "(540, 552, data quality)\n",
      "(553, 562, analytics)\n",
      "(563, 572, dashboard)\n",
      "(607, 620, data strategy)\n",
      "(731, 740, data mart)\n",
      "(741, 750, data lake)\n",
      "(1225, 1239, data cleansing)\n",
      "(1402, 1405, sql)\n",
      "(1505, 1520, problem solving)\n",
      "(440, 453, data pipeline)\n",
      "(553, 564, data system)\n",
      "(609, 628, data infrastructure)\n",
      "(849, 857, database)\n",
      "(867, 880, data pipeline)\n",
      "(916, 925, dashboard)\n",
      "(1052, 1065, data exchange)\n",
      "(1192, 1205, visualization)\n",
      "(1371, 1383, data quality)\n",
      "(1461, 1477, data engineering)\n",
      "(1561, 1570, algorithm)\n",
      "(2189, 2200, data system)\n",
      "(2254, 2270, computer science)\n",
      "(2339, 2347, big data)\n",
      "(2499, 2502, sql)\n",
      "(2543, 2557, database query)\n",
      "(2596, 2604, database)\n",
      "(2745, 2753, big data)\n",
      "(2754, 2767, data pipeline)\n",
      "(2859, 2867, big data)\n",
      "(2868, 2878, data store)\n",
      "(2918, 2923, ci cd)\n",
      "(2938, 2956, project management)\n",
      "(3318, 3324, apache)\n",
      "(3325, 3333, big data)\n",
      "(3339, 3346, airflow)\n",
      "(3408, 3440, application lifecycle management)\n",
      "(3445, 3473, product lifecycle management)\n",
      "(240, 251, data fusion)\n",
      "(252, 275, artificial intelligence)\n",
      "(1935, 1958, artificial intelligence)\n",
      "(2017, 2023, devops)\n",
      "(2247, 2267, business requirement)\n",
      "(2457, 2470, data pipeline)\n",
      "(2678, 2693, cloud computing)\n",
      "(2753, 2768, database system)\n",
      "(2779, 2791, data sharing)\n",
      "(2840, 2848, database)\n",
      "(2890, 2898, database)\n",
      "(2936, 2947, data mining)\n",
      "(2948, 2964, data warehousing)\n",
      "(3056, 3072, data integration)\n",
      "(3073, 3085, data quality)\n",
      "(3173, 3176, sql)\n",
      "(3177, 3190, data modeling)\n",
      "(3270, 3278, database)\n",
      "(3314, 3318, java)\n",
      "(3377, 3393, data engineering)\n",
      "(3403, 3411, big data)\n",
      "(3445, 3459, apache airflow)\n",
      "(3480, 3483, sql)\n",
      "(3484, 3492, database)\n",
      "(3493, 3509, data engineering)\n",
      "(3525, 3530, nosql)\n",
      "(3568, 3580, data science)\n",
      "(706, 719, data pipeline)\n",
      "(842, 855, data pipeline)\n",
      "(887, 901, data ingestion)\n",
      "(979, 982, sql)\n",
      "(1066, 1079, data pipeline)\n",
      "(1183, 1199, computer science)\n",
      "(1231, 1247, data engineering)\n",
      "(1367, 1370, sql)\n",
      "(1439, 1455, cloud technology)\n",
      "(1530, 1537, airflow)\n",
      "(890, 903, data pipeline)\n",
      "(918, 927, analytics)\n",
      "(1206, 1214, database)\n",
      "(1917, 1933, computer science)\n",
      "(2049, 2065, data engineering)\n",
      "(2143, 2146, sql)\n",
      "(2169, 2187, data visualization)\n",
      "(2188, 2197, analytics)\n",
      "(2231, 2237, devops)\n",
      "(2416, 2419, sql)\n",
      "(2446, 2460, database query)\n",
      "(2522, 2523, c)\n",
      "(2524, 2528, java)\n",
      "(2529, 2532, sql)\n",
      "(2560, 2578, data visualization)\n",
      "(2593, 2601, power bi)\n",
      "(2637, 2640, sql)\n",
      "(2793, 2802, data feed)\n",
      "(166, 187, business intelligence)\n",
      "(229, 242, data pipeline)\n",
      "(417, 428, data system)\n",
      "(1002, 1005, sql)\n",
      "(1068, 1083, database system)\n",
      "(1118, 1133, problem solving)\n",
      "(1160, 1169, algorithm)\n",
      "(1209, 1217, database)\n",
      "(1247, 1261, data structure)\n",
      "(1262, 1271, algorithm)\n",
      "(1282, 1303, database architecture)\n",
      "(1511, 1527, data warehousing)\n",
      "(1528, 1544, data integration)\n",
      "(1545, 1554, data lake)\n",
      "(1565, 1574, analytics)\n",
      "(1575, 1592, database software)\n",
      "(1616, 1619, sql)\n",
      "(1620, 1623, sql)\n",
      "(1655, 1658, sql)\n",
      "(3166, 3169, sql)\n",
      "(3215, 3230, cloud computing)\n",
      "(1357, 1372, data processing)\n",
      "(1649, 1668, technology solution)\n",
      "(2001, 2005, java)\n",
      "(2023, 2038, data processing)\n",
      "(2088, 2093, nosql)\n",
      "(2094, 2102, database)\n",
      "(2174, 2189, data processing)\n",
      "(2403, 2425, continuous integration)\n",
      "(2446, 2452, devops)\n",
      "(2508, 2519, data system)\n",
      "(3630, 3634, http)\n",
      "(555, 567, data science)\n",
      "(636, 648, data science)\n",
      "(700, 709, algorithm)\n",
      "(999, 1007, database)\n",
      "(1082, 1094, data storage)\n",
      "(1340, 1351, data mining)\n",
      "(1529, 1541, data science)\n",
      "(1542, 1558, data engineering)\n",
      "(1559, 1577, data visualization)\n",
      "(1656, 1672, computer science)\n",
      "(1673, 1689, data engineering)\n",
      "(1767, 1783, computer science)\n",
      "(1784, 1796, data science)\n",
      "(1826, 1829, sql)\n",
      "(1877, 1892, database design)\n",
      "(1893, 1908, data collection)\n",
      "(2208, 2220, data science)\n",
      "(2295, 2307, data science)\n",
      "(2420, 2428, database)\n",
      "(2429, 2432, sql)\n",
      "(2477, 2495, data visualization)\n",
      "(2526, 2542, data engineering)\n",
      "(2692, 2701, algorithm)\n",
      "(3265, 3280, problem solving)\n",
      "(70, 81, data center)\n",
      "(212, 223, data center)\n",
      "(301, 312, data center)\n",
      "(439, 450, data center)\n",
      "(652, 663, data center)\n",
      "(695, 706, data center)\n",
      "(736, 747, data center)\n",
      "(811, 822, data center)\n",
      "(1057, 1068, data center)\n",
      "(1146, 1157, data center)\n",
      "(1334, 1345, data center)\n",
      "(1413, 1424, data center)\n",
      "(1609, 1620, data center)\n",
      "(1826, 1837, data center)\n",
      "(1878, 1889, data center)\n",
      "(1942, 1953, data center)\n",
      "(2556, 2567, data center)\n",
      "(2590, 2601, data center)\n",
      "(2649, 2660, data center)\n",
      "(2689, 2700, data center)\n",
      "(3365, 3369, http)\n",
      "(315, 324, analytics)\n",
      "(400, 409, analytics)\n",
      "(457, 466, analytics)\n",
      "(647, 656, dashboard)\n",
      "(743, 763, business requirement)\n",
      "(980, 983, sql)\n",
      "(984, 1000, data warehousing)\n",
      "(2020, 2024, http)\n",
      "(433, 454, business intelligence)\n",
      "(641, 658, agile methodology)\n",
      "(995, 1015, business requirement)\n",
      "(1140, 1155, database schema)\n",
      "(1246, 1255, data feed)\n",
      "(1481, 1499, acceptance testing)\n",
      "(1780, 1783, sql)\n",
      "(1971, 1987, computer science)\n",
      "(1988, 2008, computer engineering)\n",
      "(2451, 2466, data redundancy)\n",
      "(2482, 2483, c)\n",
      "(2634, 2654, application planning)\n",
      "(2757, 2775, acceptance testing)\n",
      "(2904, 2919, database schema)\n",
      "(3038, 3047, analytics)\n",
      "(242, 255, data pipeline)\n",
      "(286, 295, dashboard)\n",
      "(296, 309, visualization)\n",
      "(483, 498, problem solving)\n",
      "(694, 703, dashboard)\n",
      "(745, 758, visualization)\n",
      "(769, 777, power bi)\n",
      "(815, 828, data pipeline)\n",
      "(880, 893, data pipeline)\n",
      "(982, 985, sql)\n",
      "(992, 1001, dashboard)\n",
      "(1044, 1059, data extraction)\n",
      "(1126, 1142, data integration)\n",
      "(1192, 1201, test data)\n",
      "(1319, 1335, data warehousing)\n",
      "(1336, 1347, data mining)\n",
      "(1456, 1476, requirement analysis)\n",
      "(1589, 1592, sql)\n",
      "(1604, 1612, database)\n",
      "(1797, 1801, java)\n",
      "(1919, 1935, computer science)\n",
      "(1936, 1952, data engineering)\n",
      "(1990, 2008, data visualization)\n",
      "(2019, 2027, power bi)\n",
      "(2085, 2108, artificial intelligence)\n",
      "(227, 239, apache spark)\n",
      "(245, 261, data engineering)\n",
      "(313, 323, databricks)\n",
      "(360, 364, java)\n",
      "(449, 457, big data)\n",
      "(561, 578, cloud development)\n",
      "(694, 699, ci cd)\n",
      "(700, 713, build process)\n",
      "(849, 871, continuous integration)\n",
      "(1209, 1225, computer science)\n",
      "(1226, 1246, computer engineering)\n",
      "(318, 327, analytics)\n",
      "(765, 774, analytics)\n",
      "(955, 968, data pipeline)\n",
      "(1111, 1120, analytics)\n",
      "(1307, 1315, big data)\n",
      "(1358, 1365, airflow)\n",
      "(1453, 1472, parallel processing)\n",
      "(1580, 1585, ci cd)\n",
      "(1643, 1656, data modeling)\n",
      "(1657, 1678, business intelligence)\n",
      "(1696, 1704, big data)\n",
      "(1779, 1787, big data)\n",
      "(1859, 1877, azure data factory)\n",
      "(1885, 1888, sql)\n",
      "(1898, 1901, sql)\n",
      "(1902, 1908, pl sql)\n",
      "(2007, 2026, parallel processing)\n",
      "(2040, 2048, database)\n",
      "(2130, 2134, java)\n",
      "(2233, 2249, data warehousing)\n",
      "(2544, 2554, databricks)\n",
      "(2575, 2591, jupyter notebook)\n",
      "(2642, 2647, ci cd)\n",
      "(209, 225, data acquisition)\n",
      "(441, 456, data management)\n",
      "(602, 617, data management)\n",
      "(717, 732, data processing)\n",
      "(909, 922, data modeling)\n",
      "(1026, 1041, data management)\n",
      "(1823, 1839, computer science)\n",
      "(2142, 2146, java)\n",
      "(2165, 2166, c)\n",
      "(2207, 2220, data pipeline)\n",
      "(2323, 2341, technical analysis)\n",
      "(2362, 2377, problem solving)\n",
      "(791, 800, analytics)\n",
      "(913, 926, data analysis)\n",
      "(996, 1008, data science)\n",
      "(1032, 1048, data engineering)\n",
      "(1084, 1092, database)\n",
      "(1612, 1621, dashboard)\n",
      "(1758, 1774, data engineering)\n",
      "(1811, 1828, business analysis)\n",
      "(1920, 1939, data transformation)\n",
      "(1966, 1975, analytics)\n",
      "(2101, 2110, analytics)\n",
      "(1901, 1914, data pipeline)\n",
      "(2018, 2027, data feed)\n",
      "(2080, 2089, data feed)\n",
      "(2200, 2226, business intelligence tool)\n",
      "(2323, 2337, data ingestion)\n",
      "(2359, 2377, business analytics)\n",
      "(2499, 2512, data pipeline)\n",
      "(2658, 2667, dashboard)\n",
      "(2780, 2791, data mining)\n",
      "(85, 98, data pipeline)\n",
      "(292, 312, business requirement)\n",
      "(528, 543, data processing)\n",
      "(557, 568, data mining)\n",
      "(680, 695, data validation)\n",
      "(741, 756, data conversion)\n",
      "(757, 771, data cleansing)\n",
      "(928, 944, data engineering)\n",
      "(1004, 1007, sql)\n",
      "(1023, 1026, sql)\n",
      "(1129, 1133, java)\n",
      "(1156, 1164, power bi)\n",
      "(1481, 1489, bigquery)\n",
      "(475, 480, ci cd)\n",
      "(706, 720, data structure)\n",
      "(721, 734, data modeling)\n",
      "(751, 754, sql)\n",
      "(1055, 1072, data architecture)\n",
      "(1110, 1126, data warehousing)\n",
      "(1293, 1296, sql)\n",
      "(1343, 1363, computer engineering)\n",
      "(1364, 1380, computer science)\n",
      "(1511, 1525, database query)\n",
      "(1548, 1551, sql)\n",
      "(1552, 1558, pl sql)\n",
      "(1571, 1589, data visualization)\n",
      "(1650, 1653, sql)\n",
      "(1654, 1669, oracle database)\n",
      "(1719, 1728, analytics)\n",
      "(1744, 1756, data storage)\n",
      "(1757, 1766, analytics)\n",
      "(1771, 1780, data lake)\n",
      "(2008, 2019, data system)\n",
      "(3164, 3173, analytics)\n",
      "(26, 42, data engineering)\n",
      "(183, 191, big data)\n",
      "(238, 274, object oriented programming language)\n",
      "(275, 279, java)\n",
      "(426, 441, database system)\n",
      "(485, 490, mysql)\n",
      "(491, 496, nosql)\n",
      "(571, 587, data integration)\n",
      "(880, 892, data science)\n",
      "(893, 909, data engineering)\n",
      "(1027, 1040, data modeling)\n",
      "(1059, 1067, database)\n",
      "(1229, 1238, analytics)\n",
      "(1318, 1331, data pipeline)\n",
      "(1498, 1514, data engineering)\n",
      "(1612, 1624, unit testing)\n",
      "(1680, 1696, data engineering)\n",
      "(161, 177, data engineering)\n",
      "(248, 264, data engineering)\n",
      "(378, 394, data engineering)\n",
      "(672, 682, build tool)\n",
      "(725, 738, data pipeline)\n",
      "(1043, 1054, code review)\n",
      "(1382, 1387, mysql)\n",
      "(1431, 1439, bigquery)\n",
      "(1462, 1465, sql)\n",
      "(1525, 1528, sql)\n",
      "(1582, 1585, sql)\n",
      "(1769, 1777, bigquery)\n",
      "(1799, 1814, database design)\n",
      "(1881, 1896, problem solving)\n",
      "(2039, 2047, database)\n",
      "(2110, 2124, data profiling)\n",
      "(2175, 2178, sql)\n",
      "(2271, 2284, data modeling)\n",
      "(2294, 2302, database)\n",
      "(2522, 2525, sql)\n",
      "(2537, 2552, data extraction)\n",
      "(2722, 2738, data engineering)\n",
      "(3020, 3036, data engineering)\n",
      "(3141, 3168, data driven decision making)\n",
      "(0, 11, data center)\n",
      "(128, 139, data center)\n",
      "(479, 490, data center)\n",
      "(760, 771, data center)\n",
      "(872, 883, data center)\n",
      "(1291, 1302, data center)\n",
      "(1303, 1314, data center)\n",
      "(2503, 2514, data center)\n",
      "(2579, 2590, data center)\n",
      "(2839, 2850, data center)\n",
      "(3024, 3035, data center)\n",
      "(3428, 3439, data center)\n",
      "(3601, 3612, data center)\n",
      "(3669, 3680, data center)\n",
      "(4438, 4449, data center)\n",
      "(4589, 4600, data center)\n",
      "(4826, 4837, data center)\n",
      "(5429, 5440, data center)\n",
      "(5612, 5623, data center)\n",
      "(5781, 5796, problem solving)\n",
      "(1389, 1406, agile methodology)\n",
      "(1407, 1419, azure devops)\n",
      "(748, 762, data migration)\n",
      "(789, 803, data migration)\n",
      "(842, 852, postgresql)\n",
      "(853, 856, sql)\n",
      "(887, 901, data migration)\n",
      "(929, 933, java)\n",
      "(1027, 1046, technology solution)\n",
      "(1093, 1119, agile software development)\n",
      "(1288, 1304, system lifecycle)\n",
      "(1562, 1566, http)\n",
      "(1921, 1925, http)\n",
      "(2272, 2282, postgresql)\n",
      "(2318, 2332, data migration)\n",
      "(121, 134, data pipeline)\n",
      "(332, 351, data infrastructure)\n",
      "(500, 513, data pipeline)\n",
      "(532, 540, big data)\n",
      "(672, 688, data engineering)\n",
      "(782, 798, data engineering)\n",
      "(810, 819, sql azure)\n",
      "(852, 872, database development)\n",
      "(932, 948, computer science)\n",
      "(1002, 1015, data pipeline)\n",
      "(1068, 1087, data infrastructure)\n",
      "(1098, 1107, analytics)\n",
      "(1175, 1188, data security)\n",
      "(1207, 1216, analytics)\n",
      "(1225, 1236, data system)\n",
      "(121, 125, http)\n",
      "(323, 326, sql)\n",
      "(361, 369, database)\n",
      "(459, 496, relational database management system)\n",
      "(637, 640, sql)\n",
      "(675, 683, database)\n",
      "(773, 810, relational database management system)\n",
      "(1267, 1286, database management)\n",
      "(1328, 1349, database architecture)\n",
      "(1371, 1382, code review)\n",
      "(1772, 1787, database design)\n",
      "(2076, 2079, sql)\n",
      "(2103, 2106, sql)\n",
      "(2164, 2182, azure data factory)\n",
      "(2229, 2242, data pipeline)\n",
      "(2243, 2252, analytics)\n",
      "(2358, 2366, database)\n",
      "(2444, 2447, sql)\n",
      "(2448, 2456, database)\n",
      "(2483, 2486, sql)\n",
      "(2487, 2495, database)\n",
      "(2516, 2519, sql)\n",
      "(2668, 2676, big data)\n",
      "(2685, 2703, azure data factory)\n",
      "(2724, 2727, sql)\n",
      "(2735, 2772, relational database management system)\n",
      "(2812, 2821, analytics)\n",
      "(2842, 2850, power bi)\n",
      "(2872, 2888, data warehousing)\n",
      "(2921, 2937, data engineering)\n",
      "(3012, 3017, ci cd)\n",
      "(3051, 3063, azure devops)\n",
      "(3110, 3121, data mining)\n",
      "(3223, 3238, database design)\n",
      "(3289, 3297, database)\n",
      "(3397, 3400, sql)\n",
      "(278, 293, problem solving)\n",
      "(426, 437, data system)\n",
      "(438, 453, data processing)\n",
      "(454, 473, data transformation)\n",
      "(1113, 1126, data pipeline)\n",
      "(1243, 1251, database)\n",
      "(1292, 1297, nosql)\n",
      "(1298, 1306, database)\n",
      "(1329, 1337, bigtable)\n",
      "(1364, 1379, database system)\n",
      "(1382, 1385, sql)\n",
      "(1400, 1405, mysql)\n",
      "(1430, 1443, data pipeline)\n",
      "(1492, 1510, azure data factory)\n",
      "(1636, 1649, data modeling)\n",
      "(1732, 1754, continuous integration)\n",
      "(1755, 1768, data modeling)\n",
      "(1802, 1807, nosql)\n",
      "(1819, 1833, graph database)\n",
      "(1849, 1858, data lake)\n",
      "(1859, 1874, data processing)\n",
      "(1887, 1890, sql)\n",
      "(1948, 1955, pyspark)\n",
      "(1956, 1960, java)\n",
      "(2164, 2179, data validation)\n",
      "(2327, 2335, aws glue)\n",
      "(2372, 2383, code review)\n",
      "(2411, 2427, computer science)\n",
      "(3153, 3168, problem solving)\n",
      "(700, 711, data center)\n",
      "(1673, 1684, data center)\n",
      "(2015, 2026, data center)\n",
      "(2405, 2416, data center)\n",
      "(416, 437, business intelligence)\n",
      "(556, 577, business intelligence)\n",
      "(965, 981, data engineering)\n",
      "(1342, 1363, business intelligence)\n",
      "(1418, 1434, data engineering)\n",
      "(1568, 1572, java)\n",
      "(1586, 1589, sql)\n",
      "(1601, 1627, database management system)\n",
      "(1638, 1641, sql)\n",
      "(1649, 1654, mysql)\n",
      "(1666, 1682, data integration)\n",
      "(1779, 1788, analytics)\n",
      "(1796, 1808, apache spark)\n",
      "(845, 853, database)\n",
      "(984, 993, analytics)\n",
      "(1430, 1443, data pipeline)\n",
      "(1457, 1464, airflow)\n",
      "(1480, 1496, application data)\n",
      "(1684, 1700, data engineering)\n",
      "(1733, 1749, data engineering)\n",
      "(1818, 1831, data pipeline)\n",
      "(1949, 1962, data pipeline)\n",
      "(2008, 2011, sql)\n",
      "(2025, 2028, sql)\n",
      "(2077, 2096, data infrastructure)\n",
      "(2097, 2106, analytics)\n",
      "(2107, 2120, visualization)\n",
      "(2747, 2748, c)\n",
      "(379, 400, business intelligence)\n",
      "(469, 490, business intelligence)\n",
      "(533, 545, data quality)\n",
      "(630, 639, algorithm)\n",
      "(801, 812, data mining)\n",
      "(851, 867, data integration)\n",
      "(884, 888, java)\n",
      "(889, 892, sql)\n",
      "(1180, 1188, database)\n",
      "(1266, 1287, business intelligence)\n",
      "(316, 324, big data)\n",
      "(394, 410, data acquisition)\n",
      "(688, 707, data infrastructure)\n",
      "(738, 749, data system)\n",
      "(808, 821, data pipeline)\n",
      "(1176, 1187, data system)\n",
      "(1265, 1280, ad hoc analysis)\n",
      "(1369, 1377, database)\n",
      "(1509, 1524, data extraction)\n",
      "(1677, 1693, computer science)\n",
      "(1836, 1848, data quality)\n",
      "(1874, 1890, data warehousing)\n",
      "(1891, 1904, data modeling)\n",
      "(2044, 2053, analytics)\n",
      "(2094, 2102, database)\n",
      "(2129, 2139, postgresql)\n",
      "(2151, 2154, sql)\n",
      "(2221, 2240, data infrastructure)\n",
      "(2254, 2268, apache airflow)\n"
     ]
    }
   ],
   "source": [
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `Path` not found.\n",
      "visualization\n",
      "azure devops\n",
      "data pipeline\n",
      "data pipeline\n",
      "data ingestion\n",
      "sql\n",
      "data pipeline\n",
      "computer science\n",
      "data engineering\n",
      "sql\n",
      "cloud technology\n",
      "airflow\n",
      "big data\n",
      "data acquisition\n",
      "java\n",
      "data extraction\n",
      "analytics\n",
      "dataflow\n",
      "data extraction\n",
      "analytics\n",
      "business intelligence\n",
      "data management\n",
      "data storage\n",
      "data warehousing\n",
      "test data\n",
      "computer science\n",
      "database design\n",
      "business intelligence\n",
      "sql\n",
      "sql\n",
      "sql\n",
      "database tuning\n",
      "data modeling\n",
      "ci cd\n",
      "sql\n",
      "database design\n",
      "c\n",
      "sql\n",
      "sql\n",
      "c\n",
      "c\n",
      "data engineering\n",
      "computer science\n",
      "java\n",
      "sql\n",
      "data structure\n",
      "batch processing\n",
      "apache kafka\n",
      "ci cd\n",
      "data pipeline\n",
      "sql\n",
      "data management\n",
      "sql\n",
      "sql\n",
      "business requirement\n",
      "database\n",
      "sql\n",
      "sql\n",
      "business intelligence\n",
      "database\n",
      "database design\n",
      "problem solving\n",
      "sql\n",
      "sql\n",
      "database architecture\n",
      "business intelligence\n",
      "agile methodology\n",
      "c\n",
      "asp net\n",
      "sql\n",
      "sql\n",
      "database architecture\n",
      "sql\n",
      "data engineering\n",
      "data system\n",
      "data mapping\n",
      "database\n",
      "data integration\n",
      "data quality\n",
      "predictive modeling\n",
      "data management\n",
      "data analysis\n",
      "sql\n",
      "data modeling\n",
      "database\n",
      "sql\n",
      "postgresql\n",
      "devops\n",
      "analytics\n",
      "sql\n",
      "sql\n",
      "databricks\n",
      "power bi\n",
      "data engineering\n",
      "sql\n",
      "sql\n",
      "databricks\n",
      "power bi\n",
      "data pipeline\n",
      "cloud technology\n",
      "analytics\n",
      "sql\n",
      "visualization\n",
      "data pipeline\n",
      "database\n",
      "build process\n",
      "data transformation\n",
      "data structure\n",
      "business requirement\n",
      "computer science\n",
      "computer science\n",
      "data modeling\n",
      "database\n",
      "data mining\n",
      "problem solving\n",
      "data analysis\n",
      "data profiling\n",
      "data analysis\n",
      "sql\n",
      "data engineering\n",
      "data engineering\n",
      "data science\n",
      "c\n",
      "c\n",
      "java\n",
      "data access\n",
      "analytics\n",
      "business intelligence\n",
      "business intelligence\n",
      "data engineering\n",
      "business intelligence\n",
      "data engineering\n",
      "java\n",
      "sql\n",
      "database management system\n",
      "sql\n",
      "mysql\n",
      "data integration\n",
      "analytics\n",
      "apache spark\n",
      "data pipeline\n",
      "data infrastructure\n",
      "data pipeline\n",
      "big data\n",
      "data engineering\n",
      "data engineering\n",
      "sql azure\n",
      "database development\n",
      "computer science\n",
      "data pipeline\n",
      "data infrastructure\n",
      "analytics\n",
      "data security\n",
      "analytics\n",
      "data system\n",
      "data quality\n",
      "analytics\n",
      "dashboard\n",
      "data strategy\n",
      "data mart\n",
      "data lake\n",
      "data cleansing\n",
      "sql\n",
      "problem solving\n",
      "data infrastructure\n",
      "data system\n",
      "data pipeline\n",
      "data system\n",
      "ad hoc analysis\n",
      "database\n",
      "data extraction\n",
      "computer science\n",
      "data quality\n",
      "data warehousing\n",
      "data modeling\n",
      "analytics\n",
      "database\n",
      "postgresql\n",
      "sql\n",
      "data infrastructure\n",
      "apache airflow\n",
      "apache spark\n",
      "data engineering\n",
      "databricks\n",
      "java\n",
      "big data\n",
      "cloud development\n",
      "ci cd\n",
      "build process\n",
      "continuous integration\n",
      "computer science\n",
      "computer engineering\n",
      "agile methodology\n",
      "azure devops\n",
      "big data\n",
      "data acquisition\n",
      "java\n",
      "data extraction\n",
      "analytics\n",
      "dataflow\n",
      "nosql\n",
      "database\n",
      "data pipeline management\n",
      "data system\n",
      "data acquisition\n",
      "data modeling\n",
      "algorithm\n",
      "data cleansing\n",
      "cloud management\n",
      "data quality\n",
      "computer science\n",
      "data mining\n",
      "data normalization\n",
      "sql\n",
      "java\n",
      "sql\n",
      "database design\n",
      "analytics\n",
      "data modeling\n",
      "data visualization\n",
      "data architecture\n",
      "data infrastructure\n",
      "data pipeline\n",
      "azure data factory\n",
      "azure data factory\n",
      "azure data lake\n",
      "analytics\n",
      "data storage\n",
      "sql\n",
      "database\n",
      "analytics\n",
      "data mart\n",
      "azure data factory\n",
      "sql\n",
      "azure data lake\n",
      "data pipeline\n",
      "ci cd\n",
      "azure databricks\n",
      "analytics\n",
      "power bi\n",
      "sql\n",
      "database\n",
      "azure data factory\n",
      "data store\n",
      "c\n",
      "computer science\n",
      "azure data factory\n",
      "data modeling\n",
      "database management\n",
      "sql azure\n",
      "sql\n",
      "azure data lake\n",
      "data warehousing\n",
      "problem solving\n",
      "project management\n",
      "business intelligence\n",
      "agile methodology\n",
      "business requirement\n",
      "database schema\n",
      "data feed\n",
      "acceptance testing\n",
      "sql\n",
      "computer science\n",
      "computer engineering\n",
      "data redundancy\n",
      "c\n",
      "application planning\n",
      "acceptance testing\n",
      "database schema\n",
      "analytics\n",
      "data fusion\n",
      "artificial intelligence\n",
      "artificial intelligence\n",
      "devops\n",
      "business requirement\n",
      "data pipeline\n",
      "cloud computing\n",
      "database system\n",
      "data sharing\n",
      "database\n",
      "database\n",
      "data mining\n",
      "data warehousing\n",
      "data integration\n",
      "data quality\n",
      "sql\n",
      "data modeling\n",
      "database\n",
      "java\n",
      "data engineering\n",
      "big data\n",
      "apache airflow\n",
      "sql\n",
      "database\n",
      "data engineering\n",
      "nosql\n",
      "data science\n",
      "data acquisition\n",
      "data management\n",
      "data management\n",
      "data processing\n",
      "data modeling\n",
      "data management\n",
      "computer science\n",
      "java\n",
      "c\n",
      "data pipeline\n",
      "technical analysis\n",
      "problem solving\n",
      "data processing\n",
      "technology solution\n",
      "java\n",
      "data processing\n",
      "nosql\n",
      "database\n",
      "data processing\n",
      "continuous integration\n",
      "devops\n",
      "data system\n",
      "http\n",
      "data pipeline\n",
      "dashboard\n",
      "visualization\n",
      "problem solving\n",
      "dashboard\n",
      "visualization\n",
      "power bi\n",
      "data pipeline\n",
      "data pipeline\n",
      "sql\n",
      "dashboard\n",
      "data extraction\n",
      "data integration\n",
      "test data\n",
      "data warehousing\n",
      "data mining\n",
      "requirement analysis\n",
      "sql\n",
      "database\n",
      "java\n",
      "computer science\n",
      "data engineering\n",
      "data visualization\n",
      "power bi\n",
      "artificial intelligence\n",
      "data infrastructure\n",
      "data system\n",
      "data pipeline\n",
      "data system\n",
      "ad hoc analysis\n",
      "database\n",
      "data extraction\n",
      "computer science\n",
      "data quality\n",
      "data warehousing\n",
      "data modeling\n",
      "analytics\n",
      "database\n",
      "postgresql\n",
      "sql\n",
      "data infrastructure\n",
      "apache airflow\n",
      "data system\n",
      "database\n",
      "analytics\n",
      "data wrangling\n",
      "database\n",
      "business intelligence\n",
      "analytics\n",
      "data architecture\n",
      "data quality\n",
      "data structure\n",
      "business requirement\n",
      "computer science\n",
      "analytics\n",
      "data science\n",
      "microsoft azure\n",
      "data storage\n",
      "azure data lake\n",
      "sql\n",
      "database\n",
      "sql\n",
      "database\n",
      "analytics\n",
      "data pipeline\n",
      "airflow\n",
      "application data\n",
      "data engineering\n",
      "data engineering\n",
      "data pipeline\n",
      "data pipeline\n",
      "sql\n",
      "sql\n",
      "data infrastructure\n",
      "analytics\n",
      "visualization\n",
      "c\n",
      "data migration\n",
      "data pipeline\n",
      "data engineering\n",
      "sql\n",
      "airflow\n",
      "visualization\n",
      "problem solving\n",
      "sql\n",
      "data migration\n",
      "airflow\n",
      "data engineering\n",
      "data engineering\n",
      "data engineering\n",
      "build tool\n",
      "data pipeline\n",
      "code review\n",
      "mysql\n",
      "bigquery\n",
      "sql\n",
      "sql\n",
      "sql\n",
      "bigquery\n",
      "database design\n",
      "problem solving\n",
      "database\n",
      "data profiling\n",
      "sql\n",
      "data modeling\n",
      "database\n",
      "sql\n",
      "data extraction\n",
      "data engineering\n",
      "data engineering\n",
      "data driven decision making\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "big data\n",
      "data acquisition\n",
      "big data\n",
      "data acquisition\n",
      "data engineering\n",
      "data acquisition\n",
      "dataset\n",
      "cloud infrastructure\n",
      "data pipeline\n",
      "business requirement\n",
      "data governance\n",
      "data engineering\n",
      "data engineering\n",
      "dataset\n",
      "java\n",
      "devops\n",
      "ci cd\n",
      "computer science\n",
      "data architecture\n",
      "database\n",
      "data pipeline\n",
      "analytics\n",
      "data pipeline\n",
      "dashboard\n",
      "data quality\n",
      "devops\n",
      "sql\n",
      "sql\n",
      "pl sql\n",
      "data pipeline\n",
      "database design\n",
      "data visualization\n",
      "computer science\n",
      "analytics\n",
      "data driven decision making\n",
      "data infrastructure\n",
      "data science\n",
      "data infrastructure\n",
      "data collection\n",
      "analytics\n",
      "data engineering\n",
      "data engineering\n",
      "computer science\n",
      "computer engineering\n",
      "database\n",
      "data lake\n",
      "data ingestion\n",
      "data pipeline\n",
      "airflow\n",
      "data engineering\n",
      "sql\n",
      "database\n",
      "postgresql\n",
      "data engineering\n",
      "pyspark\n",
      "data migration\n",
      "cloud migration\n",
      "aws lambda\n",
      "data lake\n",
      "data mart\n",
      "big data\n",
      "database\n",
      "postgresql\n",
      "data engineering\n",
      "pyspark\n",
      "devops\n",
      "bitbucket\n",
      "computer science\n",
      "c\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data processing\n",
      "data pipeline\n",
      "data pipeline\n",
      "data lake\n",
      "data transformation\n",
      "business requirement\n",
      "data infrastructure\n",
      "sql\n",
      "database\n",
      "build process\n",
      "data transformation\n",
      "data structure\n",
      "data integrity\n",
      "business intelligence\n",
      "business intelligence\n",
      "data engineering\n",
      "business intelligence\n",
      "data engineering\n",
      "java\n",
      "sql\n",
      "database management system\n",
      "sql\n",
      "mysql\n",
      "data integration\n",
      "analytics\n",
      "apache spark\n",
      "business intelligence\n",
      "agile methodology\n",
      "business requirement\n",
      "database schema\n",
      "data feed\n",
      "acceptance testing\n",
      "sql\n",
      "computer science\n",
      "computer engineering\n",
      "data redundancy\n",
      "c\n",
      "application planning\n",
      "acceptance testing\n",
      "database schema\n",
      "analytics\n",
      "data hub\n",
      "data ingestion\n",
      "data collection\n",
      "data management platform\n",
      "cloud application\n",
      "data integration\n",
      "data governance\n",
      "data pipeline\n",
      "business requirement\n",
      "data infrastructure\n",
      "analytics\n",
      "data pipeline\n",
      "data infrastructure\n",
      "data strategy\n",
      "data governance\n",
      "data quality\n",
      "data hub\n",
      "data ingestion\n",
      "aws glue\n",
      "database\n",
      "data retrieval\n",
      "sql\n",
      "sql\n",
      "data processing\n",
      "risk analysis\n",
      "data pipeline\n",
      "financial data management\n",
      "computer science\n",
      "sql\n",
      "data structure\n",
      "database administration\n",
      "data architecture\n",
      "data mapping\n",
      "application development\n",
      "sql\n",
      "java\n",
      "microsoft azure\n",
      "mysql\n",
      "data streaming\n",
      "nosql\n",
      "data warehousing\n",
      "analytics\n",
      "project management\n",
      "http\n",
      "analytics\n",
      "data analysis\n",
      "data science\n",
      "data engineering\n",
      "database\n",
      "dashboard\n",
      "data engineering\n",
      "business analysis\n",
      "data transformation\n",
      "analytics\n",
      "analytics\n",
      "computer science\n",
      "data pipeline\n",
      "data ingestion\n",
      "data streaming\n",
      "database\n",
      "database\n",
      "aws glue\n",
      "big data\n",
      "big data\n",
      "code review\n",
      "build process\n",
      "distributed computing\n",
      "agile software development\n",
      "data pipeline\n",
      "business requirement\n",
      "data ingestion\n",
      "batch processing\n",
      "data extraction\n",
      "data lake\n",
      "big data analytics\n",
      "data engineering\n",
      "data engineering\n",
      "data engineering\n",
      "build tool\n",
      "data pipeline\n",
      "code review\n",
      "mysql\n",
      "bigquery\n",
      "sql\n",
      "sql\n",
      "sql\n",
      "bigquery\n",
      "database design\n",
      "problem solving\n",
      "database\n",
      "data profiling\n",
      "sql\n",
      "data modeling\n",
      "database\n",
      "sql\n",
      "data extraction\n",
      "data engineering\n",
      "data engineering\n",
      "data driven decision making\n",
      "data architecture\n",
      "data storage\n",
      "application programming interface api\n",
      "http\n",
      "data pipeline\n",
      "business requirement\n",
      "application design\n",
      "computer science\n",
      "c\n",
      "data pipeline\n",
      "devops\n",
      "application lifecycle management\n",
      "agile methodology\n",
      "devops\n",
      "devops\n",
      "data pipeline\n",
      "analytics\n",
      "database\n",
      "computer science\n",
      "data engineering\n",
      "sql\n",
      "data visualization\n",
      "analytics\n",
      "devops\n",
      "sql\n",
      "database query\n",
      "c\n",
      "java\n",
      "sql\n",
      "data visualization\n",
      "power bi\n",
      "sql\n",
      "data feed\n",
      "analytics\n",
      "analytics\n",
      "analytics\n",
      "dashboard\n",
      "business requirement\n",
      "sql\n",
      "data warehousing\n",
      "http\n",
      "data pipeline\n",
      "business requirement\n",
      "data processing\n",
      "data mining\n",
      "data validation\n",
      "data conversion\n",
      "data cleansing\n",
      "data engineering\n",
      "sql\n",
      "sql\n",
      "java\n",
      "power bi\n",
      "bigquery\n",
      "business analysis\n",
      "business analysis\n",
      "data management\n",
      "data science\n",
      "analytics\n",
      "digital data\n",
      "data science\n",
      "analytics\n",
      "data engineering\n",
      "data management\n",
      "data processing\n",
      "data processing\n",
      "data analysis\n",
      "data cleansing\n",
      "data domain\n",
      "data integration\n",
      "data management\n",
      "data manipulation\n",
      "data strategy\n",
      "data structure\n",
      "algorithm\n",
      "data visualization\n",
      "problem solving\n",
      "data pipeline\n",
      "dashboard\n",
      "visualization\n",
      "problem solving\n",
      "dashboard\n",
      "visualization\n",
      "power bi\n",
      "data pipeline\n",
      "data pipeline\n",
      "sql\n",
      "dashboard\n",
      "data extraction\n",
      "data integration\n",
      "test data\n",
      "data warehousing\n",
      "data mining\n",
      "requirement analysis\n",
      "sql\n",
      "database\n",
      "java\n",
      "computer science\n",
      "data engineering\n",
      "data visualization\n",
      "power bi\n",
      "artificial intelligence\n",
      "data pipeline\n",
      "data structure\n",
      "data architecture\n",
      "data pipeline\n",
      "data engineering\n",
      "sql\n",
      "data pipeline\n",
      "problem solving\n",
      "data system\n",
      "data processing\n",
      "data transformation\n",
      "data pipeline\n",
      "database\n",
      "nosql\n",
      "database\n",
      "bigtable\n",
      "database system\n",
      "sql\n",
      "mysql\n",
      "data pipeline\n",
      "azure data factory\n",
      "data modeling\n",
      "continuous integration\n",
      "data modeling\n",
      "nosql\n",
      "graph database\n",
      "data lake\n",
      "data processing\n",
      "sql\n",
      "pyspark\n",
      "java\n",
      "data validation\n",
      "aws glue\n",
      "code review\n",
      "computer science\n",
      "problem solving\n",
      "database\n",
      "analytics\n",
      "data pipeline\n",
      "airflow\n",
      "application data\n",
      "data engineering\n",
      "data engineering\n",
      "data pipeline\n",
      "data pipeline\n",
      "sql\n",
      "sql\n",
      "data infrastructure\n",
      "analytics\n",
      "visualization\n",
      "c\n",
      "big data analytics\n",
      "data feed\n",
      "data feed\n",
      "application development\n",
      "computer science\n",
      "data science\n",
      "pyspark\n",
      "sql\n",
      "cloud infrastructure\n",
      "big data\n",
      "application data\n",
      "big data\n",
      "data quality\n",
      "analytics\n",
      "business intelligence tool\n",
      "data security\n",
      "data governance\n",
      "continuous integration\n",
      "data architecture\n",
      "data quality\n",
      "sql\n",
      "databricks\n",
      "airflow\n",
      "batch processing\n",
      "sql\n",
      "data pipeline\n",
      "databricks\n",
      "database system\n",
      "data store\n",
      "data lake\n",
      "data management\n",
      "problem solving\n",
      "code review\n",
      "computer science\n",
      "data engineering\n",
      "data engineering\n",
      "java\n",
      "sql\n",
      "c\n",
      "database system\n",
      "data modeling\n",
      "problem solving\n",
      "devops\n",
      "data management\n",
      "business intelligence\n",
      "business intelligence\n",
      "data quality\n",
      "algorithm\n",
      "data mining\n",
      "data integration\n",
      "java\n",
      "sql\n",
      "database\n",
      "business intelligence\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "problem solving\n",
      "analytics\n",
      "data analysis\n",
      "data science\n",
      "data engineering\n",
      "database\n",
      "dashboard\n",
      "data engineering\n",
      "business analysis\n",
      "data transformation\n",
      "analytics\n",
      "analytics\n",
      "data management\n",
      "unit testing\n",
      "data flow diagram\n",
      "application design\n",
      "code review\n",
      "problem solving\n",
      "agile methodology\n",
      "data management\n",
      "analytics\n",
      "database\n",
      "sql\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "problem solving\n",
      "artificial intelligence\n",
      "data curation\n",
      "data warehousing\n",
      "data lake\n",
      "big data\n",
      "computer science\n",
      "big data\n",
      "data pipeline\n",
      "airflow\n",
      "apache beam\n",
      "nosql\n",
      "database\n",
      "database\n",
      "mysql\n",
      "distributed computing\n",
      "data processing\n",
      "agile software development\n",
      "application deployment\n",
      "unit testing\n",
      "algorithm\n",
      "computer science\n",
      "computer engineering\n",
      "computer science\n",
      "computer engineering\n",
      "distributed computing\n",
      "java\n",
      "java\n",
      "java\n",
      "apache camel\n",
      "continuous integration\n",
      "ci cd\n",
      "agile methodology\n",
      "http\n",
      "http\n",
      "data migration\n",
      "data migration\n",
      "postgresql\n",
      "sql\n",
      "data migration\n",
      "java\n",
      "technology solution\n",
      "agile software development\n",
      "system lifecycle\n",
      "http\n",
      "http\n",
      "postgresql\n",
      "data migration\n",
      "analytics\n",
      "continuous integration\n",
      "analytics\n",
      "data pipeline\n",
      "analytics\n",
      "analytics\n",
      "data infrastructure\n",
      "computer science\n",
      "data management\n",
      "data science\n",
      "sql\n",
      "data visualization\n",
      "bokeh\n",
      "computer science\n",
      "data architecture\n",
      "data pipeline\n",
      "data engineering\n",
      "data governance\n",
      "relational database management system\n",
      "data visualization\n",
      "business intelligence\n",
      "http\n",
      "data exploitation\n",
      "data strategy\n",
      "visual analytics\n",
      "data exploitation\n",
      "problem solving\n",
      "database\n",
      "agile software development\n",
      "data exploitation\n",
      "data engineering\n",
      "big data\n",
      "sql\n",
      "nosql\n",
      "database\n",
      "data pipeline\n",
      "airflow\n",
      "data streaming\n",
      "java\n",
      "c\n",
      "sql\n",
      "database query\n",
      "sql\n",
      "database\n",
      "big data\n",
      "data store\n",
      "data analysis\n",
      "database\n",
      "data processing\n",
      "data modeling\n",
      "data system\n",
      "dashboard\n",
      "analytics\n",
      "http\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "business intelligence\n",
      "business intelligence\n",
      "data quality\n",
      "algorithm\n",
      "data mining\n",
      "data integration\n",
      "java\n",
      "sql\n",
      "database\n",
      "business intelligence\n",
      "data structure\n",
      "data storage\n",
      "analytics\n",
      "data structure\n",
      "data management\n",
      "data modeling\n",
      "data structure\n",
      "data acquisition\n",
      "data acquisition\n",
      "data management\n",
      "postgresql\n",
      "data modeling\n",
      "data structure\n",
      "data lake\n",
      "data modeling\n",
      "sql\n",
      "problem solving\n",
      "project management\n",
      "http\n",
      "analytics\n",
      "data engineering\n",
      "analytics\n",
      "analytics\n",
      "analytics\n",
      "analytics\n",
      "data strategy\n",
      "data pipeline\n",
      "data link\n",
      "data quality\n",
      "data visualization\n",
      "sql\n",
      "data modeling\n",
      "business intelligence\n",
      "data pipeline\n",
      "data system\n",
      "sql\n",
      "database system\n",
      "problem solving\n",
      "algorithm\n",
      "database\n",
      "data structure\n",
      "algorithm\n",
      "database architecture\n",
      "data warehousing\n",
      "data integration\n",
      "data lake\n",
      "analytics\n",
      "database software\n",
      "sql\n",
      "sql\n",
      "sql\n",
      "sql\n",
      "cloud computing\n",
      "data engineering\n",
      "big data\n",
      "object oriented programming language\n",
      "java\n",
      "database system\n",
      "mysql\n",
      "nosql\n",
      "data integration\n",
      "data science\n",
      "data engineering\n",
      "data modeling\n",
      "database\n",
      "analytics\n",
      "data pipeline\n",
      "data engineering\n",
      "unit testing\n",
      "data engineering\n",
      "data pipeline\n",
      "sql\n",
      "data structure\n",
      "data modeling\n",
      "sql\n",
      "data warehousing\n",
      "data modeling\n",
      "c\n",
      "sql\n",
      "data visualization\n",
      "power bi\n",
      "devops\n",
      "cloud computing\n",
      "data transformation\n",
      "data pipeline\n",
      "data system\n",
      "data infrastructure\n",
      "database\n",
      "data pipeline\n",
      "dashboard\n",
      "data exchange\n",
      "visualization\n",
      "data quality\n",
      "data engineering\n",
      "algorithm\n",
      "data system\n",
      "computer science\n",
      "big data\n",
      "sql\n",
      "database query\n",
      "database\n",
      "big data\n",
      "data pipeline\n",
      "big data\n",
      "data store\n",
      "ci cd\n",
      "project management\n",
      "apache\n",
      "big data\n",
      "airflow\n",
      "application lifecycle management\n",
      "product lifecycle management\n",
      "data retrieval\n",
      "big data\n",
      "algorithm\n",
      "database\n",
      "statistical analysis\n",
      "data pipeline\n",
      "data driven decision making\n",
      "data driven decision making\n",
      "data engineering\n",
      "analytics\n",
      "quantitative analysis\n",
      "data mining\n",
      "computer science\n",
      "data engineering\n",
      "database\n",
      "sql\n",
      "database programming\n",
      "java\n",
      "business intelligence tool\n",
      "power bi\n",
      "azure data factory\n",
      "data pipeline\n",
      "data pipeline\n",
      "databricks\n",
      "cloud database\n",
      "sql\n",
      "data lake\n",
      "pyspark\n",
      "analytics\n",
      "business intelligence\n",
      "data engineering\n",
      "database\n",
      "data engineering\n",
      "data engineering\n",
      "database\n",
      "data science\n",
      "c\n",
      "sql\n",
      "database\n",
      "amazon redshift\n",
      "sql\n",
      "airflow\n",
      "agile methodology\n",
      "data pipeline\n",
      "business requirement\n",
      "data processing\n",
      "data mining\n",
      "data validation\n",
      "data conversion\n",
      "data cleansing\n",
      "data engineering\n",
      "sql\n",
      "sql\n",
      "java\n",
      "power bi\n",
      "bigquery\n",
      "algorithm\n",
      "data fusion\n",
      "data fusion\n",
      "data fusion\n",
      "data fusion\n",
      "data fusion\n",
      "algorithm\n",
      "algorithm\n",
      "algorithm\n",
      "machine learning algorithm\n",
      "computer science\n",
      "data fusion\n",
      "computer science\n",
      "data fusion\n",
      "c\n",
      "c\n",
      "java\n",
      "computer science\n",
      "data fusion\n",
      "algorithm\n",
      "algorithm\n",
      "data pipeline\n",
      "data feed\n",
      "data feed\n",
      "business intelligence tool\n",
      "data ingestion\n",
      "business analytics\n",
      "data pipeline\n",
      "dashboard\n",
      "data mining\n",
      "data science\n",
      "data science\n",
      "algorithm\n",
      "database\n",
      "data storage\n",
      "data mining\n",
      "data science\n",
      "data engineering\n",
      "data visualization\n",
      "computer science\n",
      "data engineering\n",
      "computer science\n",
      "data science\n",
      "sql\n",
      "database design\n",
      "data collection\n",
      "data science\n",
      "data science\n",
      "database\n",
      "sql\n",
      "data visualization\n",
      "data engineering\n",
      "algorithm\n",
      "problem solving\n",
      "apache spark\n",
      "data engineering\n",
      "databricks\n",
      "java\n",
      "big data\n",
      "cloud development\n",
      "ci cd\n",
      "build process\n",
      "continuous integration\n",
      "computer science\n",
      "computer engineering\n",
      "statistical analysis\n",
      "predictive analytics\n",
      "data manipulation\n",
      "quantitative analysis\n",
      "business intelligence tool\n",
      "sql\n",
      "database\n",
      "c\n",
      "c\n",
      "data acquisition\n",
      "data management\n",
      "data management\n",
      "data processing\n",
      "data modeling\n",
      "data management\n",
      "computer science\n",
      "java\n",
      "c\n",
      "data pipeline\n",
      "technical analysis\n",
      "problem solving\n",
      "amazon\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "amazon\n",
      "data center\n",
      "amazon\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "amazon web service\n",
      "cloud computing\n",
      "data center\n",
      "problem solving\n",
      "data processing\n",
      "data center\n",
      "data center\n",
      "amazon\n",
      "amazon\n",
      "amazon\n",
      "amazon\n",
      "http\n",
      "amazon\n",
      "ci cd\n",
      "data structure\n",
      "data modeling\n",
      "sql\n",
      "data architecture\n",
      "data warehousing\n",
      "sql\n",
      "computer engineering\n",
      "computer science\n",
      "database query\n",
      "sql\n",
      "pl sql\n",
      "data visualization\n",
      "sql\n",
      "oracle database\n",
      "analytics\n",
      "data storage\n",
      "analytics\n",
      "data lake\n",
      "data system\n",
      "analytics\n",
      "analytics\n",
      "data engineering\n",
      "sql\n",
      "apache spark\n",
      "data acquisition\n",
      "business intelligence\n",
      "data science\n",
      "application development\n",
      "data science\n",
      "business intelligence\n",
      "data governance\n",
      "computer science\n",
      "data pipeline\n",
      "sql\n",
      "apache spark\n",
      "aws cloudformation\n",
      "data warehousing\n",
      "data lake\n",
      "power bi\n",
      "business intelligence\n",
      "power bi\n",
      "analytics\n",
      "analytics\n",
      "data pipeline\n",
      "analytics\n",
      "big data\n",
      "airflow\n",
      "parallel processing\n",
      "ci cd\n",
      "data modeling\n",
      "business intelligence\n",
      "big data\n",
      "big data\n",
      "azure data factory\n",
      "sql\n",
      "sql\n",
      "pl sql\n",
      "parallel processing\n",
      "database\n",
      "java\n",
      "data warehousing\n",
      "databricks\n",
      "jupyter notebook\n",
      "ci cd\n",
      "http\n",
      "sql\n",
      "database\n",
      "relational database management system\n",
      "sql\n",
      "database\n",
      "relational database management system\n",
      "database management\n",
      "database architecture\n",
      "code review\n",
      "database design\n",
      "sql\n",
      "sql\n",
      "azure data factory\n",
      "data pipeline\n",
      "analytics\n",
      "database\n",
      "sql\n",
      "database\n",
      "sql\n",
      "database\n",
      "sql\n",
      "big data\n",
      "azure data factory\n",
      "sql\n",
      "relational database management system\n",
      "analytics\n",
      "power bi\n",
      "data warehousing\n",
      "data engineering\n",
      "ci cd\n",
      "azure devops\n",
      "data mining\n",
      "database design\n",
      "database\n",
      "sql\n",
      "data engineering\n",
      "business intelligence\n",
      "data driven decision making\n",
      "sql\n",
      "data engineering\n",
      "data science\n",
      "data science\n",
      "algorithm\n",
      "database\n",
      "data storage\n",
      "data mining\n",
      "data science\n",
      "data engineering\n",
      "data visualization\n",
      "computer science\n",
      "data engineering\n",
      "computer science\n",
      "data science\n",
      "sql\n",
      "database design\n",
      "data collection\n",
      "data science\n",
      "data science\n",
      "database\n",
      "sql\n",
      "data visualization\n",
      "data engineering\n",
      "algorithm\n",
      "problem solving\n",
      "data pipeline\n",
      "data feed\n",
      "data feed\n",
      "business intelligence tool\n",
      "data ingestion\n",
      "business analytics\n",
      "data pipeline\n",
      "dashboard\n",
      "data mining\n",
      "databricks\n",
      "pyspark\n",
      "sql\n",
      "databricks\n",
      "databricks\n",
      "data lake\n",
      "data pipeline\n",
      "database\n",
      "data classification\n",
      "data profiling\n",
      "data pipeline\n",
      "data processing\n",
      "visualization\n",
      "database application\n",
      "data visualization\n",
      "data quality\n",
      "data pipeline\n",
      "visualization\n",
      "computer science\n",
      "sql\n",
      "database\n",
      "java\n",
      "programming concept\n",
      "problem solving\n",
      "computer science\n",
      "data visualization\n",
      "big data\n",
      "analytics\n",
      "data engineering\n",
      "cloud technology\n",
      "analytics\n",
      "analytics\n",
      "data modeling\n",
      "sql\n",
      "c\n",
      "data visualization\n",
      "data visualization\n",
      "data engineering\n",
      "data engineering\n",
      "cloud technology\n",
      "data infrastructure\n",
      "analytics\n",
      "analytics\n",
      "business requirement\n",
      "analytics\n",
      "analytics\n",
      "predictive modeling\n",
      "statistical analysis\n",
      "problem solving\n",
      "sql\n",
      "data manipulation\n",
      "c\n",
      "data visualization\n",
      "data engineering\n",
      "cloud technology\n",
      "analytics\n",
      "analytics\n",
      "sql\n",
      "analytics\n",
      "problem solving\n",
      "customer analytics\n",
      "data extraction\n",
      "data capture\n",
      "data structure\n",
      "computer science\n",
      "data engineering\n",
      "data warehousing\n",
      "azure data factory\n",
      "devops\n",
      "agile methodology\n",
      "data processing\n",
      "application deployment\n",
      "http\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "data center\n",
      "http\n",
      "{'ner': 3562.240463007927}\n",
      "{'ner': 3250.3616325343482}\n",
      "{'ner': 3011.1869186668064}\n",
      "{'ner': 2694.4336341346957}\n",
      "{'ner': 3972.0164261601003}\n",
      "{'ner': 2208.0634694182236}\n",
      "{'ner': 2418.7934446620966}\n",
      "{'ner': 2469.4902089780444}\n",
      "{'ner': 2050.350253858717}\n",
      "{'ner': 2324.616554841216}\n",
      "{'ner': 1953.537507983196}\n",
      "{'ner': 2001.6570903027189}\n",
      "{'ner': 2356.9509146398636}\n",
      "{'ner': 1807.4485126576203}\n",
      "{'ner': 1822.2623206957396}\n",
      "{'ner': 1923.731355262788}\n",
      "{'ner': 1832.1950834616869}\n",
      "{'ner': 1474.354933950537}\n",
      "{'ner': 1303.0848741482143}\n",
      "{'ner': 1396.1554053776263}\n",
      "{'ner': 1870.2004686112973}\n",
      "{'ner': 1953.9560935932805}\n",
      "{'ner': 1305.2677477748123}\n",
      "{'ner': 1201.7379191973723}\n",
      "{'ner': 1189.3240341831918}\n",
      "{'ner': 1224.9630256674845}\n",
      "{'ner': 1429.637726758647}\n",
      "{'ner': 1276.9280213977872}\n",
      "{'ner': 1202.6795844861042}\n",
      "{'ner': 943.70560706558}\n",
      "{'ner': 1123.996303413768}\n",
      "{'ner': 1022.6578162397282}\n",
      "{'ner': 1303.0341369807322}\n",
      "{'ner': 997.9154391933904}\n",
      "{'ner': 851.4056482414294}\n",
      "{'ner': 713.3458545892181}\n",
      "{'ner': 816.8282640728185}\n",
      "{'ner': 988.8249089134433}\n",
      "{'ner': 1014.1162499938515}\n",
      "{'ner': 863.9322422095338}\n",
      "{'ner': 789.2936083567652}\n",
      "{'ner': 790.9645890536946}\n",
      "{'ner': 697.460014152163}\n",
      "{'ner': 699.4105332176689}\n",
      "{'ner': 1131.606273916889}\n",
      "{'ner': 576.8481722454522}\n",
      "{'ner': 601.3063265112618}\n",
      "{'ner': 664.7372292548218}\n",
      "{'ner': 680.4649126463886}\n",
      "{'ner': 683.8705425719679}\n",
      "{'ner': 688.8284356366396}\n",
      "{'ner': 517.3468529829097}\n",
      "{'ner': 742.3818458433066}\n",
      "{'ner': 491.6721901817254}\n",
      "{'ner': 600.8605061653936}\n",
      "{'ner': 476.2112541978213}\n",
      "{'ner': 555.4425741960947}\n",
      "{'ner': 478.7567839691023}\n",
      "{'ner': 464.3773457024239}\n",
      "{'ner': 532.7792157114962}\n",
      "{'ner': 430.52285937998295}\n",
      "{'ner': 435.3651920741999}\n",
      "{'ner': 411.90727590273934}\n",
      "{'ner': 416.50465539014766}\n",
      "{'ner': 390.92284950526755}\n",
      "{'ner': 413.73922491955653}\n",
      "{'ner': 407.51894231872024}\n",
      "{'ner': 615.1804377137205}\n",
      "{'ner': 460.37043331743337}\n",
      "{'ner': 378.04318537875736}\n",
      "{'ner': 370.2487092226935}\n",
      "{'ner': 404.6061695717653}\n",
      "{'ner': 434.11260809308}\n",
      "{'ner': 374.5900144958418}\n",
      "{'ner': 345.01394576982767}\n",
      "{'ner': 350.68797796002775}\n",
      "{'ner': 329.83234494379565}\n",
      "{'ner': 358.97013404924803}\n",
      "{'ner': 323.5945648874752}\n",
      "{'ner': 379.44269471628894}\n",
      "{'ner': 330.4684501324554}\n",
      "{'ner': 301.97374016150775}\n",
      "{'ner': 297.96927192771483}\n",
      "{'ner': 308.31366574527897}\n",
      "{'ner': 309.2462248404094}\n",
      "{'ner': 318.3627586464913}\n",
      "{'ner': 289.3537863031574}\n",
      "{'ner': 315.53445873636315}\n",
      "{'ner': 338.6265813535222}\n",
      "{'ner': 283.0874871662463}\n",
      "{'ner': 293.5699738160847}\n",
      "{'ner': 301.0370466906445}\n",
      "{'ner': 276.2080054833605}\n",
      "{'ner': 272.1315522007812}\n",
      "{'ner': 274.63296863909363}\n",
      "{'ner': 266.56031875403914}\n",
      "{'ner': 252.75718505942035}\n",
      "{'ner': 270.2452812309549}\n",
      "{'ner': 281.03890317934594}\n",
      "{'ner': 246.69024270714633}\n"
     ]
    }
   ],
   "source": [
    "#train the model \n",
    "starting_fresh = False\n",
    "# Load a pre-existing spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')  # for example\n",
    "import random\n",
    "from spacy.util import minibatch\n",
    "# Get the Named Entity Recognizer component in the pipeline\n",
    "ner = nlp.get_pipe('ner')\n",
    "from spacy.training import Example\n",
    "from pathlib import Path\n",
    "\n",
    "# Add new entity labels to 'ner'\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for start,end,label in annotations.get('entities'):\n",
    "        print(label)\n",
    "        ner.add_label(str(label))\n",
    "\n",
    "\n",
    "# Disable other pipes during training\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "\n",
    "# Begin training\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    if starting_fresh:\n",
    "        nlp.begin_training()\n",
    "\n",
    "    for itn in range(100):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "\n",
    "        # Batch up the examples using spaCy's minibatch\n",
    "        for batch in minibatch(TRAIN_DATA, size=2):\n",
    "            examples = []\n",
    "            for text, annotations in batch:\n",
    "                # Create a Spacy Doc from the text\n",
    "                doc = nlp.make_doc(text)\n",
    "                # Create an Example using the annotations\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                examples.append(example)\n",
    "\n",
    "            # Update the model\n",
    "            nlp.update(\n",
    "                examples,\n",
    "                drop=0.5,  # Dropout - make it harder to memorize data\n",
    "                losses=losses\n",
    "            )\n",
    "        print(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to /Users/nyzy/nitzmali/job_transition_pathway/models/skills_tag_spacy_nlp_model\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "output_dir = Path('/Users/nyzy/nitzmali/job_transition_pathway/models/skills_tag_spacy_nlp_model')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.scorer import Scorer\n",
    "from pathlib import Path\n",
    "def train_test_val_split(data, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    # Calculate actual validation set size of the remaining data after test split\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    # Split off test set from available data\n",
    "    train_val_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    # Split remaining data into training and validation sets\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=val_size_adjusted, random_state=random_state)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load the model you want to evaluate\n",
    "nlp = spacy.load('/Users/nyzy/nitzmali/job_transition_pathway/models/skills_tag_spacy_nlp_model')  # replace with your model name\n",
    "\n",
    "# Split your TRAIN_DATA into train, validate and test sets\n",
    "train_data, val_data, test_data = train_test_val_split(TRAIN_DATA, test_size=0.2, val_size=0.25)\n",
    "\n",
    "# Convert the validation data to spaCy's Example format\n",
    "examples = []\n",
    "for text, annots in TRAIN_DATA:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annots)\n",
    "    examples.append(example)\n",
    "\n",
    "\n",
    "# Use the Scorer to score the examples\n",
    "scorer = Scorer(nlp)\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "\n",
    "precision = scores['ents_p']\n",
    "recall = scores['ents_r']\n",
    "f_score = scores['ents_f']\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'analytics': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'visualization': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data engineering': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'sql': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data pipeline': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'airflow': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data infrastructure': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application data': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data lake': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'computer science': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data curation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'mysql': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'big data': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'nosql': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'artificial intelligence': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data warehousing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache beam': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'sql azure': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database development': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data security': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data system': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'build process': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'computer engineering': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'ci cd': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'databricks': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'java': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud development': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'continuous integration': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache spark': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'dashboard': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data quality': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data exchange': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'project management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data store': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'algorithm': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'product lifecycle management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application lifecycle management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database query': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'azure data factory': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database programming': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'statistical analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data driven decision making': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'power bi': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data retrieval': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business intelligence tool': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data mining': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'quantitative analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data acquisition': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data modeling': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database design': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data normalization': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data cleansing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data visualization': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data pipeline management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data integration': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business intelligence': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database management system': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data center': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'amazon': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'http': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'problem solving': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data processing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'amazon web service': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud computing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'technical analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'azure databricks': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data architecture': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'azure data lake': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data storage': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data mart': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data science': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data strategy': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data domain': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data manipulation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data structure': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'digital data': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data validation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'code review': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'pyspark': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database system': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data transformation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'graph database': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'bigtable': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'aws glue': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache kafka': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'batch processing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data governance': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business requirement': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache airflow': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'devops': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data sharing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data fusion': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application design': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'unit testing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'agile methodology': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data flow diagram': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'relational database management system': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'predictive analytics': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'amazon redshift': {'p': 0.0, 'r': 0.0, 'f': 0.0}}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the validation data to spaCy's Example format\n",
    "examples = []\n",
    "for text, annots in val_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annots)\n",
    "    examples.append(example)\n",
    "\n",
    "\n",
    "# Use the Scorer to score the examples\n",
    "scorer = Scorer(nlp)\n",
    "scores = scorer.score(examples)\n",
    "scores['ents_per_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3146176248.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[310], line 21\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"\\n\" + \"-\" * 50 + \"\\n)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "examples[0]\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('/Users/nyzy/nitzmali/job_transition_pathway/models/skills_tag_spacy_nlp_model')  # replace with your model name\n",
    "\n",
    "# Assume 'nlp' is your loaded NLP model\n",
    "for text, annots in val_data:  # Let's use val_data as an example\n",
    "    doc = nlp(text)  # Process the text to predict entities\n",
    "    print(\"Predictions by model:\")\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "    # Now print the correct data for comparison\n",
    "    print(\"\\nCorrect labels:\")\n",
    "    for start, end, label in annots['entities']:\n",
    "        print(text[start:end], start, end, label)\n",
    "\n",
    "    # You can use displacy here as well if you prefer visual comparison\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "    # Adding a separation for readability between different examples\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'power bi': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data conversion': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'sql': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data processing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'bigquery': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business requirement': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'java': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data validation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data mining': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data pipeline': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data cleansing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data engineering': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data visualization': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'relational database management system': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business intelligence': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'http': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data governance': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data architecture': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data science': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data strategy': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data integration': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'analytics': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'problem solving': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data domain': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data manipulation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data structure': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'algorithm': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'digital data': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'build process': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'computer engineering': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'ci cd': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'computer science': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'databricks': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud development': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'big data': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'continuous integration': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache spark': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data acquisition': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data extraction': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'nosql': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'dataflow': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data integrity': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data transformation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data lake': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data infrastructure': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'sql azure': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database development': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data security': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data system': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'airflow': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'agile methodology': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'c': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'amazon redshift': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data modeling': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'project management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data link': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data quality': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database design': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data normalization': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data pipeline management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application planning': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database schema': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data redundancy': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'acceptance testing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data feed': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'azure devops': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'dashboard': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'visualization': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'requirement analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'test data': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'artificial intelligence': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data warehousing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data center': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'postgresql': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache airflow': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'ad hoc analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud technology': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data ingestion': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business intelligence tool': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'business analytics': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data access': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data profiling': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'oracle database': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database query': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data storage': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'pl sql': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'code review': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'azure data factory': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'pyspark': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'mysql': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database system': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'graph database': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'bigtable': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'aws glue': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application data': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data mart': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'technology solution': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'devops': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'microsoft azure': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data wrangling': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'azure data lake': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data classification': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data exploitation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'visual analytics': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data store': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data streaming': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'agile software development': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'build tool': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data driven decision making': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data exchange': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'product lifecycle management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application lifecycle management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud database': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'statistical analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'predictive modeling': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'azure databricks': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data migration': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'bitbucket': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'aws lambda': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud migration': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database architecture': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'dataset': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud infrastructure': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'object oriented programming language': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'unit testing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud computing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database software': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database tuning': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'asp net': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data collection': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'distributed computing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application deployment': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache camel': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data hub': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data management platform': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application development': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'financial data management': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'risk analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data mapping': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data retrieval': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'cloud application': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database administration': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data curation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache beam': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data sharing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data fusion': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database programming': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'quantitative analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'system lifecycle': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database application': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'programming concept': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'predictive analytics': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'batch processing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'big data analytics': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application design': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data flow diagram': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'technical analysis': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'machine learning algorithm': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'aws cloudformation': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'apache kafka': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'amazon': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'amazon web service': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'customer analytics': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'data capture': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'bokeh': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'database management system': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'jupyter notebook': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'parallel processing': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       " 'application programming interface api': {'p': 0.0, 'r': 0.0, 'f': 0.0}}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access and print scores\n",
    "scores['ents_per_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted entities: [('database', 845, 853, 'database'), ('analytics', 984, 993, 'analytics'), ('data pipeline', 1430, 1443, 'data pipeline'), ('airflow', 1457, 1464, 'airflow'), ('application data', 1480, 1496, 'application data'), ('data engineering', 1684, 1700, 'data engineering'), ('data engineering', 1733, 1749, 'data engineering'), ('data pipeline', 1818, 1831, 'data pipeline'), ('data pipeline', 1949, 1962, 'data pipeline'), ('sql', 2008, 2011, 'sql'), ('sql', 2025, 2028, 'sql'), ('data infrastructure', 2077, 2096, 'data infrastructure'), ('analytics', 2097, 2106, 'analytics'), ('visualization', 2107, 2120, 'visualization'), ('c', 2747, 2748, 'c')]\n",
      "Gold entities: (database, analytics, data pipeline, airflow, application data, data engineering, data engineering, data pipeline, data pipeline, sql, sql, data infrastructure, analytics, visualization, c)\n",
      "Matched entity: database (database)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: airflow (airflow)\n",
      "Matched entity: application data (application data)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: data infrastructure (data infrastructure)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: visualization (visualization)\n",
      "Matched entity: c (c)\n",
      "Predicted entities: [('artificial intelligence', 388, 411, 'artificial intelligence'), ('data curation', 1269, 1282, 'data curation'), ('data warehousing', 1428, 1444, 'data warehousing'), ('data lake', 1445, 1454, 'data lake'), ('big data', 1455, 1463, 'big data'), ('computer science', 1597, 1613, 'computer science'), ('big data', 1857, 1865, 'big data'), ('data pipeline', 1959, 1972, 'data pipeline'), ('airflow', 1978, 1985, 'airflow'), ('apache beam', 1986, 1997, 'apache beam'), ('nosql', 2166, 2171, 'nosql'), ('database', 2172, 2180, 'database'), ('database', 2218, 2226, 'database'), ('mysql', 2234, 2239, 'mysql')]\n",
      "Gold entities: (artificial intelligence, data curation, data warehousing, data lake, big data, computer science, big data, data pipeline, airflow, apache beam, nosql, database, database, mysql)\n",
      "Matched entity: artificial intelligence (artificial intelligence)\n",
      "Matched entity: data curation (data curation)\n",
      "Matched entity: data warehousing (data warehousing)\n",
      "Matched entity: data lake (data lake)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: airflow (airflow)\n",
      "Matched entity: apache beam (apache beam)\n",
      "Matched entity: nosql (nosql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: database (database)\n",
      "Matched entity: mysql (mysql)\n",
      "Predicted entities: [('data pipeline', 121, 134, 'data pipeline'), ('data infrastructure', 332, 351, 'data infrastructure'), ('data pipeline', 500, 513, 'data pipeline'), ('big data', 532, 540, 'big data'), ('data engineering', 672, 688, 'data engineering'), ('data engineering', 782, 798, 'data engineering'), ('sql azure', 810, 819, 'sql azure'), ('database development', 852, 872, 'database development'), ('computer science', 932, 948, 'computer science'), ('data pipeline', 1002, 1015, 'data pipeline'), ('data infrastructure', 1068, 1087, 'data infrastructure'), ('analytics', 1098, 1107, 'analytics'), ('data security', 1175, 1188, 'data security'), ('analytics', 1207, 1216, 'analytics'), ('data system', 1225, 1236, 'data system')]\n",
      "Gold entities: (data pipeline, data infrastructure, data pipeline, big data, data engineering, data engineering, sql azure, database development, computer science, data pipeline, data infrastructure, analytics, data security, analytics, data system)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: data infrastructure (data infrastructure)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: sql azure (sql azure)\n",
      "Matched entity: database development (database development)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: data infrastructure (data infrastructure)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: data security (data security)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: data system (data system)\n",
      "Predicted entities: [('apache spark', 227, 239, 'apache spark'), ('data engineering', 245, 261, 'data engineering'), ('databricks', 313, 323, 'databricks'), ('java', 360, 364, 'java'), ('big data', 449, 457, 'big data'), ('cloud development', 561, 578, 'cloud development'), ('ci cd', 694, 699, 'ci cd'), ('build process', 700, 713, 'build process'), ('continuous integration', 849, 871, 'continuous integration'), ('computer science', 1209, 1225, 'computer science'), ('computer engineering', 1226, 1246, 'computer engineering')]\n",
      "Gold entities: (apache spark, data engineering, databricks, java, big data, cloud development, ci cd, build process, continuous integration, computer science, computer engineering)\n",
      "Matched entity: apache spark (apache spark)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: databricks (databricks)\n",
      "Matched entity: java (java)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: cloud development (cloud development)\n",
      "Matched entity: ci cd (ci cd)\n",
      "Matched entity: build process (build process)\n",
      "Matched entity: continuous integration (continuous integration)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: computer engineering (computer engineering)\n",
      "Predicted entities: [('data pipeline', 440, 453, 'data pipeline'), ('data system', 553, 564, 'data system'), ('data infrastructure', 609, 628, 'data infrastructure'), ('database', 849, 857, 'database'), ('data pipeline', 867, 880, 'data pipeline'), ('dashboard', 916, 925, 'dashboard'), ('data exchange', 1052, 1065, 'data exchange'), ('visualization', 1192, 1205, 'visualization'), ('data quality', 1371, 1383, 'data quality'), ('data engineering', 1461, 1477, 'data engineering'), ('algorithm', 1561, 1570, 'algorithm'), ('data system', 2189, 2200, 'data system'), ('computer science', 2254, 2270, 'computer science'), ('big data', 2339, 2347, 'big data'), ('sql', 2499, 2502, 'sql'), ('database query', 2543, 2557, 'database query'), ('database', 2596, 2604, 'database'), ('big data', 2745, 2753, 'big data'), ('data pipeline', 2754, 2767, 'data pipeline'), ('big data', 2859, 2867, 'big data'), ('data store', 2868, 2878, 'data store'), ('ci cd', 2918, 2923, 'ci cd'), ('project management', 2938, 2956, 'project management'), ('apache', 3318, 3324, 'apache'), ('big data', 3325, 3333, 'big data'), ('airflow', 3339, 3346, 'airflow'), ('application lifecycle management', 3408, 3440, 'application lifecycle management'), ('product lifecycle management', 3445, 3473, 'product lifecycle management')]\n",
      "Gold entities: (data pipeline, data system, data infrastructure, database, data pipeline, dashboard, data exchange, visualization, data quality, data engineering, algorithm, data system, computer science, big data, sql, database query, database, big data, data pipeline, big data, data store, ci cd, project management, apache, big data, airflow, application lifecycle management, product lifecycle management)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: data system (data system)\n",
      "Matched entity: data infrastructure (data infrastructure)\n",
      "Matched entity: database (database)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: dashboard (dashboard)\n",
      "Matched entity: data exchange (data exchange)\n",
      "Matched entity: visualization (visualization)\n",
      "Matched entity: data quality (data quality)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: algorithm (algorithm)\n",
      "Matched entity: data system (data system)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database query (database query)\n",
      "Matched entity: database (database)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: data store (data store)\n",
      "Matched entity: ci cd (ci cd)\n",
      "Matched entity: project management (project management)\n",
      "Matched entity: apache (apache)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: airflow (airflow)\n",
      "Matched entity: application lifecycle management (application lifecycle management)\n",
      "Matched entity: product lifecycle management (product lifecycle management)\n",
      "Predicted entities: [('data retrieval', 149, 163, 'data retrieval'), ('big data', 566, 574, 'big data'), ('algorithm', 855, 864, 'algorithm'), ('database', 889, 897, 'database'), ('statistical analysis', 936, 956, 'statistical analysis'), ('data pipeline', 1038, 1051, 'data pipeline'), ('data driven decision making', 1219, 1246, 'data driven decision making'), ('data driven decision making', 1263, 1290, 'data driven decision making'), ('data engineering', 1765, 1781, 'data engineering'), ('analytics', 1782, 1791, 'analytics'), ('quantitative analysis', 1893, 1914, 'quantitative analysis'), ('data mining', 1915, 1926, 'data mining'), ('computer science', 2102, 2118, 'computer science'), ('data engineering', 2184, 2200, 'data engineering'), ('database', 2492, 2500, 'database'), ('sql', 2615, 2618, 'sql'), ('database programming', 2644, 2664, 'database programming'), ('java', 2722, 2726, 'java'), ('business intelligence tool', 2749, 2775, 'business intelligence tool'), ('power bi', 2791, 2799, 'power bi'), ('azure data factory', 2840, 2858, 'azure data factory')]\n",
      "Gold entities: (data retrieval, big data, algorithm, database, statistical analysis, data pipeline, data driven decision making, data driven decision making, data engineering, analytics, quantitative analysis, data mining, computer science, data engineering, database, sql, database programming, java, business intelligence tool, power bi, azure data factory)\n",
      "Matched entity: data retrieval (data retrieval)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: algorithm (algorithm)\n",
      "Matched entity: database (database)\n",
      "Matched entity: statistical analysis (statistical analysis)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: data driven decision making (data driven decision making)\n",
      "Matched entity: data driven decision making (data driven decision making)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: quantitative analysis (quantitative analysis)\n",
      "Matched entity: data mining (data mining)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: database (database)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database programming (database programming)\n",
      "Matched entity: java (java)\n",
      "Matched entity: business intelligence tool (business intelligence tool)\n",
      "Matched entity: power bi (power bi)\n",
      "Matched entity: azure data factory (azure data factory)\n",
      "Predicted entities: [('data pipeline', 789, 802, 'data pipeline'), ('data system', 823, 834, 'data system'), ('data acquisition', 910, 926, 'data acquisition'), ('data modeling', 1039, 1052, 'data modeling'), ('algorithm', 1062, 1071, 'algorithm'), ('data cleansing', 1082, 1096, 'data cleansing'), ('cloud management', 1097, 1113, 'cloud management'), ('data quality', 1198, 1210, 'data quality'), ('computer science', 1356, 1372, 'computer science'), ('data mining', 1648, 1659, 'data mining'), ('data normalization', 1701, 1719, 'data normalization'), ('sql', 1753, 1756, 'sql'), ('java', 1757, 1761, 'java'), ('sql', 1790, 1793, 'sql'), ('database design', 1794, 1809, 'database design'), ('analytics', 1894, 1903, 'analytics'), ('data modeling', 1921, 1934, 'data modeling'), ('data visualization', 1935, 1953, 'data visualization')]\n",
      "Gold entities: (data pipeline management, data system, data acquisition, data modeling, algorithm, data cleansing, cloud management, data quality, computer science, data mining, data normalization, sql, java, sql, database design, analytics, data modeling, data visualization)\n",
      "Matched entity: data system (data system)\n",
      "Matched entity: data acquisition (data acquisition)\n",
      "Matched entity: data modeling (data modeling)\n",
      "Matched entity: algorithm (algorithm)\n",
      "Matched entity: data cleansing (data cleansing)\n",
      "Matched entity: cloud management (cloud management)\n",
      "Matched entity: data quality (data quality)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: data mining (data mining)\n",
      "Matched entity: data normalization (data normalization)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: java (java)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database design (database design)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: data modeling (data modeling)\n",
      "Matched entity: data visualization (data visualization)\n",
      "Predicted entities: [('business intelligence', 416, 437, 'business intelligence'), ('business intelligence', 556, 577, 'business intelligence'), ('data engineering', 965, 981, 'data engineering'), ('business intelligence', 1342, 1363, 'business intelligence'), ('data engineering', 1418, 1434, 'data engineering'), ('java', 1568, 1572, 'java'), ('sql', 1586, 1589, 'sql'), ('database management system', 1601, 1627, 'database management system'), ('sql', 1638, 1641, 'sql'), ('mysql', 1649, 1654, 'mysql'), ('data integration', 1666, 1682, 'data integration'), ('data visualisation', 1719, 1737, 'data visualization'), ('analytics', 1779, 1788, 'analytics'), ('apache spark', 1796, 1808, 'apache spark')]\n",
      "Gold entities: (business intelligence, business intelligence, data engineering, business intelligence, data engineering, java, sql, database management system, sql, mysql, data integration, analytics, apache spark)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: java (java)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database management system (database management system)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: mysql (mysql)\n",
      "Matched entity: data integration (data integration)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: apache spark (apache spark)\n",
      "Predicted entities: [('business intelligence', 379, 400, 'business intelligence'), ('business intelligence', 469, 490, 'business intelligence'), ('data quality', 533, 545, 'data quality'), ('algorithm', 630, 639, 'algorithm'), ('data mining', 801, 812, 'data mining'), ('data integration', 851, 867, 'data integration'), ('java', 884, 888, 'java'), ('sql', 889, 892, 'sql'), ('database', 1180, 1188, 'database'), ('business intelligence', 1266, 1287, 'business intelligence')]\n",
      "Gold entities: (business intelligence, business intelligence, data quality, algorithm, data mining, data integration, java, sql, database, business intelligence)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: data quality (data quality)\n",
      "Matched entity: algorithm (algorithm)\n",
      "Matched entity: data mining (data mining)\n",
      "Matched entity: data integration (data integration)\n",
      "Matched entity: java (java)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Predicted entities: [('amazon', 0, 6, 'amazon'), ('data center', 53, 64, 'data center'), ('data center', 475, 486, 'data center'), ('data center', 584, 595, 'data center'), ('amazon', 848, 854, 'amazon'), ('data center', 866, 877, 'data center'), ('amazon', 1043, 1049, 'amazon'), ('data center', 1139, 1150, 'data center'), ('data center', 1566, 1577, 'data center'), ('data center', 1606, 1617, 'data center'), ('amazon', 1863, 1869, 'amazon'), ('cloud computing', 1947, 1962, 'cloud computing'), ('data center', 2833, 2844, 'data center'), ('problem solving', 2993, 3008, 'problem solving'), ('data processing', 3070, 3085, 'data processing'), ('data center', 3183, 3194, 'data center'), ('data center', 3645, 3656, 'data center'), ('amazon', 4509, 4515, 'amazon'), ('amazon', 4567, 4573, 'amazon'), ('amazon', 4617, 4623, 'amazon'), ('amazon', 4662, 4668, 'amazon'), ('http', 4898, 4902, 'http'), ('amazon', 4907, 4913, 'amazon')]\n",
      "Gold entities: (amazon, data center, data center, data center, amazon, data center, amazon, data center, data center, data center, amazon web service, cloud computing, data center, problem solving, data processing, data center, data center, amazon, amazon, amazon, amazon, http, amazon)\n",
      "Matched entity: amazon (amazon)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: amazon (amazon)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: amazon (amazon)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: cloud computing (cloud computing)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: problem solving (problem solving)\n",
      "Matched entity: data processing (data processing)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: amazon (amazon)\n",
      "Matched entity: amazon (amazon)\n",
      "Matched entity: amazon (amazon)\n",
      "Matched entity: amazon (amazon)\n",
      "Matched entity: http (http)\n",
      "Matched entity: amazon (amazon)\n",
      "Predicted entities: [('data acquisition', 209, 225, 'data acquisition'), ('data management', 441, 456, 'data management'), ('data management', 602, 617, 'data management'), ('data processing', 717, 732, 'data processing'), ('data modeling', 909, 922, 'data modeling'), ('data management', 1026, 1041, 'data management'), ('computer science', 1823, 1839, 'computer science'), ('java', 2142, 2146, 'java'), ('c', 2165, 2166, 'c'), ('data pipeline', 2207, 2220, 'data pipeline'), ('technical analysis', 2323, 2341, 'technical analysis'), ('problem solving', 2362, 2377, 'problem solving')]\n",
      "Gold entities: (data acquisition, data management, data management, data processing, data modeling, data management, computer science, java, c, data pipeline, technical analysis, problem solving)\n",
      "Matched entity: data acquisition (data acquisition)\n",
      "Matched entity: data management (data management)\n",
      "Matched entity: data management (data management)\n",
      "Matched entity: data processing (data processing)\n",
      "Matched entity: data modeling (data modeling)\n",
      "Matched entity: data management (data management)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: java (java)\n",
      "Matched entity: c (c)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: technical analysis (technical analysis)\n",
      "Matched entity: problem solving (problem solving)\n",
      "Predicted entities: [('data architecture', 88, 105, 'data architecture'), ('data infrastructure', 168, 187, 'data infrastructure'), ('data pipeline', 368, 381, 'data pipeline'), ('azure data factory', 382, 400, 'azure data factory'), ('azure data factory', 497, 515, 'azure data factory'), ('azure data lake', 534, 549, 'azure data lake'), ('analytics', 550, 559, 'analytics'), ('data storage', 638, 650, 'data storage'), ('sql', 666, 669, 'sql'), ('database', 670, 678, 'database'), ('analytics', 693, 702, 'analytics'), ('data mart', 899, 908, 'data mart'), ('azure data factory', 1050, 1068, 'azure data factory'), ('sql', 1089, 1092, 'sql'), ('azure data lake', 1093, 1108, 'azure data lake'), ('data pipeline', 1155, 1168, 'data pipeline'), ('ci cd', 1246, 1251, 'ci cd'), ('azure databricks', 1292, 1308, 'azure databricks'), ('analytics', 1354, 1363, 'analytics'), ('power', 1364, 1369, 'bigtable'), ('sql', 1396, 1399, 'sql'), ('database', 1400, 1408, 'database'), ('azure data factory', 1409, 1427, 'azure data factory'), ('data store', 1461, 1471, 'data store'), ('c', 1482, 1483, 'c'), ('computer science', 1585, 1601, 'computer science'), ('azure data factory', 1655, 1673, 'azure data factory'), ('data modeling', 1770, 1783, 'data modeling'), ('database management', 1796, 1815, 'database management'), ('sql azure', 1828, 1837, 'sql azure'), ('sql', 1838, 1841, 'sql'), ('azure data lake', 1842, 1857, 'azure data lake'), ('data warehousing', 1878, 1894, 'data warehousing'), ('problem solving', 1918, 1933, 'problem solving'), ('project management', 1961, 1979, 'project management')]\n",
      "Gold entities: (data architecture, data infrastructure, data pipeline, azure data factory, azure data factory, azure data lake, analytics, data storage, sql, database, analytics, data mart, azure data factory, sql, azure data lake, data pipeline, ci cd, azure databricks, analytics, power bi, sql, database, azure data factory, data store, c, computer science, azure data factory, data modeling, database management, sql azure, sql, azure data lake, data warehousing, problem solving, project management)\n",
      "Matched entity: data architecture (data architecture)\n",
      "Matched entity: data infrastructure (data infrastructure)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: azure data factory (azure data factory)\n",
      "Matched entity: azure data factory (azure data factory)\n",
      "Matched entity: azure data lake (azure data lake)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: data storage (data storage)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: data mart (data mart)\n",
      "Matched entity: azure data factory (azure data factory)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: azure data lake (azure data lake)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: ci cd (ci cd)\n",
      "Matched entity: azure databricks (azure databricks)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: azure data factory (azure data factory)\n",
      "Matched entity: data store (data store)\n",
      "Matched entity: c (c)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: azure data factory (azure data factory)\n",
      "Matched entity: data modeling (data modeling)\n",
      "Matched entity: database management (database management)\n",
      "Matched entity: sql azure (sql azure)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: azure data lake (azure data lake)\n",
      "Matched entity: data warehousing (data warehousing)\n",
      "Matched entity: problem solving (problem solving)\n",
      "Matched entity: project management (project management)\n",
      "Predicted entities: [('business analysis', 398, 415, 'business analysis'), ('business analysis', 613, 630, 'business analysis'), ('data management', 631, 646, 'data management'), ('data science', 647, 659, 'data science'), ('analytics', 776, 785, 'analytics'), ('digital data', 844, 856, 'digital data'), ('data science', 1243, 1255, 'data science'), ('analytics', 1261, 1270, 'analytics'), ('data engineering', 1271, 1287, 'data engineering'), ('data management', 1288, 1303, 'data management'), ('data processing', 1480, 1495, 'data processing'), ('data processing', 1502, 1517, 'data processing'), ('data analysis', 1663, 1676, 'data analysis'), ('data cleansing', 1677, 1691, 'data cleansing'), ('data domain', 1707, 1718, 'data domain'), ('data integration', 1729, 1745, 'data integration'), ('data management', 1746, 1761, 'data management'), ('data manipulation', 1762, 1779, 'data manipulation'), ('data strategy', 1794, 1807, 'data strategy'), ('data structure', 1819, 1833, 'data structure'), ('algorithm', 1834, 1843, 'algorithm'), ('data visualization', 1844, 1862, 'data visualization'), ('problem solving', 1918, 1933, 'problem solving')]\n",
      "Gold entities: (business analysis, business analysis, data management, data science, analytics, digital data, data science, analytics, data engineering, data management, data processing, data processing, data analysis, data cleansing, data domain, data integration, data management, data manipulation, data strategy, data structure, algorithm, data visualization, problem solving)\n",
      "Matched entity: business analysis (business analysis)\n",
      "Matched entity: business analysis (business analysis)\n",
      "Matched entity: data management (data management)\n",
      "Matched entity: data science (data science)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: digital data (digital data)\n",
      "Matched entity: data science (data science)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: data management (data management)\n",
      "Matched entity: data processing (data processing)\n",
      "Matched entity: data processing (data processing)\n",
      "Matched entity: data analysis (data analysis)\n",
      "Matched entity: data cleansing (data cleansing)\n",
      "Matched entity: data domain (data domain)\n",
      "Matched entity: data integration (data integration)\n",
      "Matched entity: data management (data management)\n",
      "Matched entity: data manipulation (data manipulation)\n",
      "Matched entity: data strategy (data strategy)\n",
      "Matched entity: data structure (data structure)\n",
      "Matched entity: algorithm (algorithm)\n",
      "Matched entity: data visualization (data visualization)\n",
      "Matched entity: problem solving (problem solving)\n",
      "Predicted entities: [('business intelligence', 379, 400, 'business intelligence'), ('business intelligence', 469, 490, 'business intelligence'), ('data quality', 533, 545, 'data quality'), ('algorithm', 630, 639, 'algorithm'), ('data mining', 801, 812, 'data mining'), ('data integration', 851, 867, 'data integration'), ('java', 884, 888, 'java'), ('sql', 889, 892, 'sql'), ('database', 1180, 1188, 'database'), ('business intelligence', 1266, 1287, 'business intelligence')]\n",
      "Gold entities: (business intelligence, business intelligence, data quality, algorithm, data mining, data integration, java, sql, database, business intelligence)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: data quality (data quality)\n",
      "Matched entity: algorithm (algorithm)\n",
      "Matched entity: data mining (data mining)\n",
      "Matched entity: data integration (data integration)\n",
      "Matched entity: java (java)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Predicted entities: [('problem solving', 278, 293, 'problem solving'), ('data system', 426, 437, 'data system'), ('data processing', 438, 453, 'data processing'), ('data transformation', 454, 473, 'data transformation'), ('data pipeline', 1113, 1126, 'data pipeline'), ('database', 1243, 1251, 'database'), ('nosql', 1292, 1297, 'nosql'), ('database', 1298, 1306, 'database'), ('bigtable', 1329, 1337, 'bigtable'), ('database system', 1364, 1379, 'database system'), ('sql', 1382, 1385, 'sql'), ('mysql', 1400, 1405, 'mysql'), ('data pipeline', 1430, 1443, 'data pipeline'), ('azure data factory', 1492, 1510, 'azure data factory'), ('data modeling', 1636, 1649, 'data modeling'), ('continuous integration', 1732, 1754, 'continuous integration'), ('data modeling', 1755, 1768, 'data modeling'), ('nosql', 1802, 1807, 'nosql'), ('graph database', 1819, 1833, 'graph database'), ('data lake', 1849, 1858, 'data lake'), ('data processing', 1859, 1874, 'data processing'), ('sql', 1887, 1890, 'sql'), ('pyspark', 1948, 1955, 'pyspark'), ('java', 1956, 1960, 'java'), ('data validation', 2164, 2179, 'data validation'), ('aws glue', 2327, 2335, 'aws glue'), ('code review', 2372, 2383, 'code review'), ('computer science', 2411, 2427, 'computer science'), ('problem solving', 3153, 3168, 'problem solving')]\n",
      "Gold entities: (problem solving, data system, data processing, data transformation, data pipeline, database, nosql, database, bigtable, database system, sql, mysql, data pipeline, azure data factory, data modeling, continuous integration, data modeling, nosql, graph database, data lake, data processing, sql, pyspark, java, data validation, aws glue, code review, computer science, problem solving)\n",
      "Matched entity: problem solving (problem solving)\n",
      "Matched entity: data system (data system)\n",
      "Matched entity: data processing (data processing)\n",
      "Matched entity: data transformation (data transformation)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: database (database)\n",
      "Matched entity: nosql (nosql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: bigtable (bigtable)\n",
      "Matched entity: database system (database system)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: mysql (mysql)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: azure data factory (azure data factory)\n",
      "Matched entity: data modeling (data modeling)\n",
      "Matched entity: continuous integration (continuous integration)\n",
      "Matched entity: data modeling (data modeling)\n",
      "Matched entity: nosql (nosql)\n",
      "Matched entity: graph database (graph database)\n",
      "Matched entity: data lake (data lake)\n",
      "Matched entity: data processing (data processing)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: pyspark (pyspark)\n",
      "Matched entity: java (java)\n",
      "Matched entity: data validation (data validation)\n",
      "Matched entity: aws glue (aws glue)\n",
      "Matched entity: code review (code review)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: problem solving (problem solving)\n",
      "Predicted entities: [('power bi', 672, 680, 'power bi'), ('business intelligence', 768, 789, 'business intelligence'), ('power bi', 831, 839, 'power bi')]\n",
      "Gold entities: (power bi, business intelligence, power bi)\n",
      "Matched entity: power bi (power bi)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: power bi (power bi)\n",
      "Predicted entities: [('c', 594, 595, 'c'), ('c', 596, 597, 'c'), ('data engineering', 768, 784, 'data engineering'), ('computer science', 2031, 2047, 'computer science'), ('java', 2197, 2201, 'java'), ('sql', 2227, 2230, 'sql'), ('data structure', 2394, 2408, 'data structure'), ('batch processing', 2537, 2553, 'batch processing'), ('apache kafka', 2565, 2577, 'apache kafka'), ('ci cd', 2645, 2650, 'ci cd'), ('data pipeline', 2855, 2868, 'data pipeline')]\n",
      "Gold entities: (c, c, data engineering, computer science, java, sql, data structure, batch processing, apache kafka, ci cd, data pipeline)\n",
      "Matched entity: c (c)\n",
      "Matched entity: c (c)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: computer science (computer science)\n",
      "Matched entity: java (java)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: data structure (data structure)\n",
      "Matched entity: batch processing (batch processing)\n",
      "Matched entity: apache kafka (apache kafka)\n",
      "Matched entity: ci cd (ci cd)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Predicted entities: [('application data', 1411, 1427, 'application data'), ('big data', 1727, 1735, 'big data'), ('data quality', 1806, 1818, 'data quality'), ('analytics', 1847, 1856, 'analytics'), ('business intelligence tool', 1886, 1912, 'business intelligence tool'), ('data security', 1921, 1934, 'data security'), ('data governance', 1956, 1971, 'data governance'), ('continuous integration', 2011, 2033, 'continuous integration'), ('data architecture', 2336, 2353, 'data architecture'), ('data quality', 2472, 2484, 'data quality'), ('sql', 2547, 2550, 'sql'), ('databricks', 2612, 2622, 'databricks'), ('airflow', 2623, 2630, 'airflow'), ('batch processing', 2631, 2647, 'batch processing'), ('sql', 2648, 2651, 'sql'), ('data pipeline', 2697, 2710, 'data pipeline'), ('databricks', 2806, 2816, 'databricks'), ('database system', 2850, 2865, 'database system'), ('data store', 2935, 2945, 'data store'), ('data lake', 3002, 3011, 'data lake')]\n",
      "Gold entities: (application data, big data, data quality, analytics, business intelligence tool, data security, data governance, continuous integration, data architecture, data quality, sql, databricks, airflow, batch processing, sql, data pipeline, databricks, database system, data store, data lake)\n",
      "Matched entity: application data (application data)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: data quality (data quality)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: business intelligence tool (business intelligence tool)\n",
      "Matched entity: data security (data security)\n",
      "Matched entity: data governance (data governance)\n",
      "Matched entity: continuous integration (continuous integration)\n",
      "Matched entity: data architecture (data architecture)\n",
      "Matched entity: data quality (data quality)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: databricks (databricks)\n",
      "Matched entity: airflow (airflow)\n",
      "Matched entity: batch processing (batch processing)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: databricks (databricks)\n",
      "Matched entity: database system (database system)\n",
      "Matched entity: data store (data store)\n",
      "Matched entity: data lake (data lake)\n",
      "Predicted entities: [('data fusion', 240, 251, 'data fusion'), ('artificial intelligence', 252, 275, 'artificial intelligence'), ('artificial intelligence', 1935, 1958, 'artificial intelligence'), ('devops', 2017, 2023, 'devops'), ('business requirement', 2247, 2267, 'business requirement'), ('data pipeline', 2457, 2470, 'data pipeline'), ('cloud computing', 2678, 2693, 'cloud computing'), ('database system', 2753, 2768, 'database system'), ('data sharing', 2779, 2791, 'data sharing'), ('database', 2840, 2848, 'database'), ('database', 2890, 2898, 'database'), ('data mining', 2936, 2947, 'data mining'), ('data warehousing', 2948, 2964, 'data warehousing'), ('data integration', 3056, 3072, 'data integration'), ('data quality', 3073, 3085, 'data quality'), ('sql', 3173, 3176, 'sql'), ('data modeling', 3177, 3190, 'data modeling'), ('database', 3270, 3278, 'database'), ('java', 3314, 3318, 'java'), ('data engineering', 3377, 3393, 'data engineering'), ('big data', 3403, 3411, 'big data'), ('apache airflow', 3445, 3459, 'apache airflow'), ('sql', 3480, 3483, 'sql'), ('database', 3484, 3492, 'database'), ('data engineering', 3493, 3509, 'data engineering'), ('nosql', 3525, 3530, 'nosql'), ('data science', 3568, 3580, 'data science')]\n",
      "Gold entities: (data fusion, artificial intelligence, artificial intelligence, devops, business requirement, data pipeline, cloud computing, database system, data sharing, database, database, data mining, data warehousing, data integration, data quality, sql, data modeling, database, java, data engineering, big data, apache airflow, sql, database, data engineering, nosql, data science)\n",
      "Matched entity: data fusion (data fusion)\n",
      "Matched entity: artificial intelligence (artificial intelligence)\n",
      "Matched entity: artificial intelligence (artificial intelligence)\n",
      "Matched entity: devops (devops)\n",
      "Matched entity: business requirement (business requirement)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: cloud computing (cloud computing)\n",
      "Matched entity: database system (database system)\n",
      "Matched entity: data sharing (data sharing)\n",
      "Matched entity: database (database)\n",
      "Matched entity: database (database)\n",
      "Matched entity: data mining (data mining)\n",
      "Matched entity: data warehousing (data warehousing)\n",
      "Matched entity: data integration (data integration)\n",
      "Matched entity: data quality (data quality)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: data modeling (data modeling)\n",
      "Matched entity: database (database)\n",
      "Matched entity: java (java)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: big data (big data)\n",
      "Matched entity: apache airflow (apache airflow)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: nosql (nosql)\n",
      "Matched entity: data science (data science)\n",
      "Predicted entities: [('data center', 700, 711, 'data center'), ('data center', 1673, 1684, 'data center'), ('data center', 2015, 2026, 'data center'), ('data center', 2405, 2416, 'data center')]\n",
      "Gold entities: (data center, data center, data center, data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Predicted entities: [('data management', 560, 575, 'data management'), ('unit testing', 757, 769, 'unit testing'), ('data flow diagram', 1172, 1189, 'data flow diagram'), ('application design', 1214, 1232, 'application design'), ('code review', 1258, 1269, 'code review'), ('problem solving', 1582, 1597, 'problem solving'), ('agile methodology', 2236, 2253, 'agile methodology'), ('data management', 2352, 2367, 'data management'), ('analytics', 2373, 2382, 'analytics'), ('database', 2415, 2423, 'database'), ('sql', 2453, 2456, 'sql')]\n",
      "Gold entities: (data management, unit testing, data flow diagram, application design, code review, problem solving, agile methodology, data management, analytics, database, sql)\n",
      "Matched entity: data management (data management)\n",
      "Matched entity: unit testing (unit testing)\n",
      "Matched entity: data flow diagram (data flow diagram)\n",
      "Matched entity: application design (application design)\n",
      "Matched entity: code review (code review)\n",
      "Matched entity: problem solving (problem solving)\n",
      "Matched entity: agile methodology (agile methodology)\n",
      "Matched entity: data management (data management)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: database (database)\n",
      "Matched entity: sql (sql)\n",
      "Predicted entities: [('data architecture', 542, 559, 'data architecture'), ('data pipeline', 1145, 1158, 'data pipeline'), ('data engineering', 1456, 1472, 'data engineering'), ('data governance', 1473, 1488, 'data governance'), ('relational database management system year experience etl product experience informatica cloud plus year experience scripting language', 1589, 1723, 'relational database management system'), ('data visualization', 1778, 1796, 'data visualization'), ('business intelligence', 1797, 1818, 'business intelligence'), ('http', 2404, 2408, 'http')]\n",
      "Gold entities: (data architecture, data pipeline, data engineering, data governance, relational database management system, data visualization, business intelligence, http)\n",
      "Matched entity: data architecture (data architecture)\n",
      "Matched entity: data pipeline (data pipeline)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: data governance (data governance)\n",
      "Matched entity: data visualization (data visualization)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: http (http)\n",
      "Predicted entities: [('data center', 0, 11, 'data center'), ('data center', 128, 139, 'data center'), ('data center', 479, 490, 'data center'), ('data center', 760, 771, 'data center'), ('data center', 872, 883, 'data center'), ('data center', 1291, 1302, 'data center'), ('data center', 1303, 1314, 'data center'), ('data center', 2503, 2514, 'data center'), ('data center', 2579, 2590, 'data center'), ('data center', 2839, 2850, 'data center'), ('data center', 3024, 3035, 'data center'), ('data center', 3428, 3439, 'data center'), ('data center', 3601, 3612, 'data center'), ('data center', 3669, 3680, 'data center'), ('data center', 4438, 4449, 'data center'), ('data center', 4589, 4600, 'data center'), ('data center', 4826, 4837, 'data center'), ('data center', 5429, 5440, 'data center'), ('data center', 5612, 5623, 'data center'), ('problem solving', 5781, 5796, 'problem solving')]\n",
      "Gold entities: (data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, data center, problem solving)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: data center (data center)\n",
      "Matched entity: problem solving (problem solving)\n",
      "Predicted entities: [('statistical analysis', 628, 648, 'statistical analysis'), ('predictive analytics', 659, 679, 'predictive analytics'), ('data manipulation', 680, 697, 'data manipulation'), ('quantitative analysis', 921, 942, 'quantitative analysis'), ('business intelligence tool', 1418, 1444, 'business intelligence tool'), ('sql', 1619, 1622, 'sql'), ('database', 1629, 1637, 'database'), ('c', 2217, 2218, 'c'), ('c', 2276, 2277, 'c')]\n",
      "Gold entities: (statistical analysis, predictive analytics, data manipulation, quantitative analysis, business intelligence tool, sql, database, c, c)\n",
      "Matched entity: statistical analysis (statistical analysis)\n",
      "Matched entity: predictive analytics (predictive analytics)\n",
      "Matched entity: data manipulation (data manipulation)\n",
      "Matched entity: quantitative analysis (quantitative analysis)\n",
      "Matched entity: business intelligence tool (business intelligence tool)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: c (c)\n",
      "Matched entity: c (c)\n",
      "Predicted entities: [('analytics', 404, 413, 'analytics'), ('business intelligence', 426, 447, 'business intelligence'), ('data engineering', 566, 582, 'data engineering'), ('database', 628, 636, 'database'), ('data engineering', 658, 674, 'data engineering'), ('data engineering', 690, 706, 'data engineering'), ('database', 785, 793, 'database'), ('data science', 926, 938, 'data science'), ('c', 949, 950, 'c'), ('sql', 1086, 1089, 'sql'), ('database', 1159, 1167, 'database'), ('amazon redshift', 1178, 1193, 'amazon redshift'), ('sql', 1204, 1207, 'sql'), ('airflow', 1312, 1319, 'airflow'), ('agile methodology', 1372, 1389, 'agile methodology')]\n",
      "Gold entities: (analytics, business intelligence, data engineering, database, data engineering, data engineering, database, data science, c, sql, database, amazon redshift, sql, airflow, agile methodology)\n",
      "Matched entity: analytics (analytics)\n",
      "Matched entity: business intelligence (business intelligence)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: database (database)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: data engineering (data engineering)\n",
      "Matched entity: database (database)\n",
      "Matched entity: data science (data science)\n",
      "Matched entity: c (c)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: database (database)\n",
      "Matched entity: amazon redshift (amazon redshift)\n",
      "Matched entity: sql (sql)\n",
      "Matched entity: airflow (airflow)\n",
      "Matched entity: agile methodology (agile methodology)\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'val_data' is available and properly formatted as per spaCy requirements\n",
    "\n",
    "\n",
    "# Checking entities on new or unseen text\n",
    "for text, annots in val_data:\n",
    "    doc_gold_text = nlp.make_doc(text)  # the document text\n",
    "    gold = Example.from_dict(doc_gold_text, annots)  # the gold-standard Example object\n",
    "\n",
    "    # Using the model to predict the outcome on the same text\n",
    "    doc_pred = nlp(text)  # the predicted document\n",
    "    pred_ents = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc_pred.ents]  # predicted entities\n",
    "\n",
    "    # Comparing predicted entities with the gold standard\n",
    "    print(\"Predicted entities:\", pred_ents)\n",
    "    print(\"Gold entities:\", gold.reference.ents)\n",
    "    \n",
    "    # Check if there are any predicted entities that match the gold standard\n",
    "    for pred in doc_pred.ents:\n",
    "        for gold_span in gold.reference.ents:\n",
    "            if pred.text == gold_span.text and pred.label_ == gold_span.label_ and pred.start_char == gold_span.start_char and pred.end_char == gold_span.end_char:\n",
    "                print(f\"Matched entity: {pred.text} ({pred.label_})\")\n",
    "    \n",
    "    # If the loop above never finds a match, you may want to check your training and annotation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted entities with positions and labels: [('database', 845, 853, 'database'), ('analytics', 984, 993, 'analytics'), ('data pipeline', 1430, 1443, 'data pipeline'), ('airflow', 1457, 1464, 'airflow'), ('application data', 1480, 1496, 'application data'), ('data engineering', 1684, 1700, 'data engineering'), ('data engineering', 1733, 1749, 'data engineering'), ('data pipeline', 1818, 1831, 'data pipeline'), ('data pipeline', 1949, 1962, 'data pipeline'), ('sql', 2008, 2011, 'sql'), ('sql', 2025, 2028, 'sql'), ('data infrastructure', 2077, 2096, 'data infrastructure'), ('analytics', 2097, 2106, 'analytics'), ('visualization', 2107, 2120, 'visualization'), ('c', 2747, 2748, 'c')]\n",
      "Gold standard entities with positions and labels: [('database', 845, 853, 'database'), ('analytics', 984, 993, 'analytics'), ('data pipeline', 1430, 1443, 'data pipeline'), ('airflow', 1457, 1464, 'airflow'), ('application data', 1480, 1496, 'application data'), ('data engineering', 1684, 1700, 'data engineering'), ('data engineering', 1733, 1749, 'data engineering'), ('data pipeline', 1818, 1831, 'data pipeline'), ('data pipeline', 1949, 1962, 'data pipeline'), ('sql', 2008, 2011, 'sql'), ('sql', 2025, 2028, 'sql'), ('data infrastructure', 2077, 2096, 'data infrastructure'), ('analytics', 2097, 2106, 'analytics'), ('visualization', 2107, 2120, 'visualization'), ('c', 2747, 2748, 'c')]\n",
      "Matched entities: [('database', 845, 853, 'database'), ('analytics', 984, 993, 'analytics'), ('data pipeline', 1430, 1443, 'data pipeline'), ('airflow', 1457, 1464, 'airflow'), ('application data', 1480, 1496, 'application data'), ('data engineering', 1684, 1700, 'data engineering'), ('data engineering', 1733, 1749, 'data engineering'), ('data pipeline', 1818, 1831, 'data pipeline'), ('data pipeline', 1949, 1962, 'data pipeline'), ('sql', 2008, 2011, 'sql'), ('sql', 2025, 2028, 'sql'), ('data infrastructure', 2077, 2096, 'data infrastructure'), ('analytics', 2097, 2106, 'analytics'), ('visualization', 2107, 2120, 'visualization'), ('c', 2747, 2748, 'c')]\n",
      "Predicted entities with positions and labels: [('artificial intelligence', 388, 411, 'artificial intelligence'), ('data curation', 1269, 1282, 'data curation'), ('data warehousing', 1428, 1444, 'data warehousing'), ('data lake', 1445, 1454, 'data lake'), ('big data', 1455, 1463, 'big data'), ('computer science', 1597, 1613, 'computer science'), ('big data', 1857, 1865, 'big data'), ('data pipeline', 1959, 1972, 'data pipeline'), ('airflow', 1978, 1985, 'airflow'), ('apache beam', 1986, 1997, 'apache beam'), ('nosql', 2166, 2171, 'nosql'), ('database', 2172, 2180, 'database'), ('database', 2218, 2226, 'database'), ('mysql', 2234, 2239, 'mysql')]\n",
      "Gold standard entities with positions and labels: [('artificial intelligence', 388, 411, 'artificial intelligence'), ('data curation', 1269, 1282, 'data curation'), ('data warehousing', 1428, 1444, 'data warehousing'), ('data lake', 1445, 1454, 'data lake'), ('big data', 1455, 1463, 'big data'), ('computer science', 1597, 1613, 'computer science'), ('big data', 1857, 1865, 'big data'), ('data pipeline', 1959, 1972, 'data pipeline'), ('airflow', 1978, 1985, 'airflow'), ('apache beam', 1986, 1997, 'apache beam'), ('nosql', 2166, 2171, 'nosql'), ('database', 2172, 2180, 'database'), ('database', 2218, 2226, 'database'), ('mysql', 2234, 2239, 'mysql')]\n",
      "Matched entities: [('artificial intelligence', 388, 411, 'artificial intelligence'), ('data curation', 1269, 1282, 'data curation'), ('data warehousing', 1428, 1444, 'data warehousing'), ('data lake', 1445, 1454, 'data lake'), ('big data', 1455, 1463, 'big data'), ('computer science', 1597, 1613, 'computer science'), ('big data', 1857, 1865, 'big data'), ('data pipeline', 1959, 1972, 'data pipeline'), ('airflow', 1978, 1985, 'airflow'), ('apache beam', 1986, 1997, 'apache beam'), ('nosql', 2166, 2171, 'nosql'), ('database', 2172, 2180, 'database'), ('database', 2218, 2226, 'database'), ('mysql', 2234, 2239, 'mysql')]\n",
      "Predicted entities with positions and labels: [('data pipeline', 121, 134, 'data pipeline'), ('data infrastructure', 332, 351, 'data infrastructure'), ('data pipeline', 500, 513, 'data pipeline'), ('big data', 532, 540, 'big data'), ('data engineering', 672, 688, 'data engineering'), ('data engineering', 782, 798, 'data engineering'), ('sql azure', 810, 819, 'sql azure'), ('database development', 852, 872, 'database development'), ('computer science', 932, 948, 'computer science'), ('data pipeline', 1002, 1015, 'data pipeline'), ('data infrastructure', 1068, 1087, 'data infrastructure'), ('analytics', 1098, 1107, 'analytics'), ('data security', 1175, 1188, 'data security'), ('analytics', 1207, 1216, 'analytics'), ('data system', 1225, 1236, 'data system')]\n",
      "Gold standard entities with positions and labels: [('data pipeline', 121, 134, 'data pipeline'), ('data infrastructure', 332, 351, 'data infrastructure'), ('data pipeline', 500, 513, 'data pipeline'), ('big data', 532, 540, 'big data'), ('data engineering', 672, 688, 'data engineering'), ('data engineering', 782, 798, 'data engineering'), ('sql azure', 810, 819, 'sql azure'), ('database development', 852, 872, 'database development'), ('computer science', 932, 948, 'computer science'), ('data pipeline', 1002, 1015, 'data pipeline'), ('data infrastructure', 1068, 1087, 'data infrastructure'), ('analytics', 1098, 1107, 'analytics'), ('data security', 1175, 1188, 'data security'), ('analytics', 1207, 1216, 'analytics'), ('data system', 1225, 1236, 'data system')]\n",
      "Matched entities: [('data pipeline', 121, 134, 'data pipeline'), ('data infrastructure', 332, 351, 'data infrastructure'), ('data pipeline', 500, 513, 'data pipeline'), ('big data', 532, 540, 'big data'), ('data engineering', 672, 688, 'data engineering'), ('data engineering', 782, 798, 'data engineering'), ('sql azure', 810, 819, 'sql azure'), ('database development', 852, 872, 'database development'), ('computer science', 932, 948, 'computer science'), ('data pipeline', 1002, 1015, 'data pipeline'), ('data infrastructure', 1068, 1087, 'data infrastructure'), ('analytics', 1098, 1107, 'analytics'), ('data security', 1175, 1188, 'data security'), ('analytics', 1207, 1216, 'analytics'), ('data system', 1225, 1236, 'data system')]\n",
      "Predicted entities with positions and labels: [('apache spark', 227, 239, 'apache spark'), ('data engineering', 245, 261, 'data engineering'), ('databricks', 313, 323, 'databricks'), ('java', 360, 364, 'java'), ('big data', 449, 457, 'big data'), ('cloud development', 561, 578, 'cloud development'), ('ci cd', 694, 699, 'ci cd'), ('build process', 700, 713, 'build process'), ('continuous integration', 849, 871, 'continuous integration'), ('computer science', 1209, 1225, 'computer science'), ('computer engineering', 1226, 1246, 'computer engineering')]\n",
      "Gold standard entities with positions and labels: [('apache spark', 227, 239, 'apache spark'), ('data engineering', 245, 261, 'data engineering'), ('databricks', 313, 323, 'databricks'), ('java', 360, 364, 'java'), ('big data', 449, 457, 'big data'), ('cloud development', 561, 578, 'cloud development'), ('ci cd', 694, 699, 'ci cd'), ('build process', 700, 713, 'build process'), ('continuous integration', 849, 871, 'continuous integration'), ('computer science', 1209, 1225, 'computer science'), ('computer engineering', 1226, 1246, 'computer engineering')]\n",
      "Matched entities: [('apache spark', 227, 239, 'apache spark'), ('data engineering', 245, 261, 'data engineering'), ('databricks', 313, 323, 'databricks'), ('java', 360, 364, 'java'), ('big data', 449, 457, 'big data'), ('cloud development', 561, 578, 'cloud development'), ('ci cd', 694, 699, 'ci cd'), ('build process', 700, 713, 'build process'), ('continuous integration', 849, 871, 'continuous integration'), ('computer science', 1209, 1225, 'computer science'), ('computer engineering', 1226, 1246, 'computer engineering')]\n",
      "Predicted entities with positions and labels: [('data pipeline', 440, 453, 'data pipeline'), ('data system', 553, 564, 'data system'), ('data infrastructure', 609, 628, 'data infrastructure'), ('database', 849, 857, 'database'), ('data pipeline', 867, 880, 'data pipeline'), ('dashboard', 916, 925, 'dashboard'), ('data exchange', 1052, 1065, 'data exchange'), ('visualization', 1192, 1205, 'visualization'), ('data quality', 1371, 1383, 'data quality'), ('data engineering', 1461, 1477, 'data engineering'), ('algorithm', 1561, 1570, 'algorithm'), ('data system', 2189, 2200, 'data system'), ('computer science', 2254, 2270, 'computer science'), ('big data', 2339, 2347, 'big data'), ('sql', 2499, 2502, 'sql'), ('database query', 2543, 2557, 'database query'), ('database', 2596, 2604, 'database'), ('big data', 2745, 2753, 'big data'), ('data pipeline', 2754, 2767, 'data pipeline'), ('big data', 2859, 2867, 'big data'), ('data store', 2868, 2878, 'data store'), ('ci cd', 2918, 2923, 'ci cd'), ('project management', 2938, 2956, 'project management'), ('apache', 3318, 3324, 'apache'), ('big data', 3325, 3333, 'big data'), ('airflow', 3339, 3346, 'airflow'), ('application lifecycle management', 3408, 3440, 'application lifecycle management'), ('product lifecycle management', 3445, 3473, 'product lifecycle management')]\n",
      "Gold standard entities with positions and labels: [('data pipeline', 440, 453, 'data pipeline'), ('data system', 553, 564, 'data system'), ('data infrastructure', 609, 628, 'data infrastructure'), ('database', 849, 857, 'database'), ('data pipeline', 867, 880, 'data pipeline'), ('dashboard', 916, 925, 'dashboard'), ('data exchange', 1052, 1065, 'data exchange'), ('visualization', 1192, 1205, 'visualization'), ('data quality', 1371, 1383, 'data quality'), ('data engineering', 1461, 1477, 'data engineering'), ('algorithm', 1561, 1570, 'algorithm'), ('data system', 2189, 2200, 'data system'), ('computer science', 2254, 2270, 'computer science'), ('big data', 2339, 2347, 'big data'), ('sql', 2499, 2502, 'sql'), ('database query', 2543, 2557, 'database query'), ('database', 2596, 2604, 'database'), ('big data', 2745, 2753, 'big data'), ('data pipeline', 2754, 2767, 'data pipeline'), ('big data', 2859, 2867, 'big data'), ('data store', 2868, 2878, 'data store'), ('ci cd', 2918, 2923, 'ci cd'), ('project management', 2938, 2956, 'project management'), ('apache', 3318, 3324, 'apache'), ('big data', 3325, 3333, 'big data'), ('airflow', 3339, 3346, 'airflow'), ('application lifecycle management', 3408, 3440, 'application lifecycle management'), ('product lifecycle management', 3445, 3473, 'product lifecycle management')]\n",
      "Matched entities: [('data pipeline', 440, 453, 'data pipeline'), ('data system', 553, 564, 'data system'), ('data infrastructure', 609, 628, 'data infrastructure'), ('database', 849, 857, 'database'), ('data pipeline', 867, 880, 'data pipeline'), ('dashboard', 916, 925, 'dashboard'), ('data exchange', 1052, 1065, 'data exchange'), ('visualization', 1192, 1205, 'visualization'), ('data quality', 1371, 1383, 'data quality'), ('data engineering', 1461, 1477, 'data engineering'), ('algorithm', 1561, 1570, 'algorithm'), ('data system', 2189, 2200, 'data system'), ('computer science', 2254, 2270, 'computer science'), ('big data', 2339, 2347, 'big data'), ('sql', 2499, 2502, 'sql'), ('database query', 2543, 2557, 'database query'), ('database', 2596, 2604, 'database'), ('big data', 2745, 2753, 'big data'), ('data pipeline', 2754, 2767, 'data pipeline'), ('big data', 2859, 2867, 'big data'), ('data store', 2868, 2878, 'data store'), ('ci cd', 2918, 2923, 'ci cd'), ('project management', 2938, 2956, 'project management'), ('apache', 3318, 3324, 'apache'), ('big data', 3325, 3333, 'big data'), ('airflow', 3339, 3346, 'airflow'), ('application lifecycle management', 3408, 3440, 'application lifecycle management'), ('product lifecycle management', 3445, 3473, 'product lifecycle management')]\n",
      "Predicted entities with positions and labels: [('data retrieval', 149, 163, 'data retrieval'), ('big data', 566, 574, 'big data'), ('algorithm', 855, 864, 'algorithm'), ('database', 889, 897, 'database'), ('statistical analysis', 936, 956, 'statistical analysis'), ('data pipeline', 1038, 1051, 'data pipeline'), ('data driven decision making', 1219, 1246, 'data driven decision making'), ('data driven decision making', 1263, 1290, 'data driven decision making'), ('data engineering', 1765, 1781, 'data engineering'), ('analytics', 1782, 1791, 'analytics'), ('quantitative analysis', 1893, 1914, 'quantitative analysis'), ('data mining', 1915, 1926, 'data mining'), ('computer science', 2102, 2118, 'computer science'), ('data engineering', 2184, 2200, 'data engineering'), ('database', 2492, 2500, 'database'), ('sql', 2615, 2618, 'sql'), ('database programming', 2644, 2664, 'database programming'), ('java', 2722, 2726, 'java'), ('business intelligence tool', 2749, 2775, 'business intelligence tool'), ('power bi', 2791, 2799, 'power bi'), ('azure data factory', 2840, 2858, 'azure data factory')]\n",
      "Gold standard entities with positions and labels: [('data retrieval', 149, 163, 'data retrieval'), ('big data', 566, 574, 'big data'), ('algorithm', 855, 864, 'algorithm'), ('database', 889, 897, 'database'), ('statistical analysis', 936, 956, 'statistical analysis'), ('data pipeline', 1038, 1051, 'data pipeline'), ('data driven decision making', 1219, 1246, 'data driven decision making'), ('data driven decision making', 1263, 1290, 'data driven decision making'), ('data engineering', 1765, 1781, 'data engineering'), ('analytics', 1782, 1791, 'analytics'), ('quantitative analysis', 1893, 1914, 'quantitative analysis'), ('data mining', 1915, 1926, 'data mining'), ('computer science', 2102, 2118, 'computer science'), ('data engineering', 2184, 2200, 'data engineering'), ('database', 2492, 2500, 'database'), ('sql', 2615, 2618, 'sql'), ('database programming', 2644, 2664, 'database programming'), ('java', 2722, 2726, 'java'), ('business intelligence tool', 2749, 2775, 'business intelligence tool'), ('power bi', 2791, 2799, 'power bi'), ('azure data factory', 2840, 2858, 'azure data factory')]\n",
      "Matched entities: [('data retrieval', 149, 163, 'data retrieval'), ('big data', 566, 574, 'big data'), ('algorithm', 855, 864, 'algorithm'), ('database', 889, 897, 'database'), ('statistical analysis', 936, 956, 'statistical analysis'), ('data pipeline', 1038, 1051, 'data pipeline'), ('data driven decision making', 1219, 1246, 'data driven decision making'), ('data driven decision making', 1263, 1290, 'data driven decision making'), ('data engineering', 1765, 1781, 'data engineering'), ('analytics', 1782, 1791, 'analytics'), ('quantitative analysis', 1893, 1914, 'quantitative analysis'), ('data mining', 1915, 1926, 'data mining'), ('computer science', 2102, 2118, 'computer science'), ('data engineering', 2184, 2200, 'data engineering'), ('database', 2492, 2500, 'database'), ('sql', 2615, 2618, 'sql'), ('database programming', 2644, 2664, 'database programming'), ('java', 2722, 2726, 'java'), ('business intelligence tool', 2749, 2775, 'business intelligence tool'), ('power bi', 2791, 2799, 'power bi'), ('azure data factory', 2840, 2858, 'azure data factory')]\n",
      "Predicted entities with positions and labels: [('data pipeline', 789, 802, 'data pipeline'), ('data system', 823, 834, 'data system'), ('data acquisition', 910, 926, 'data acquisition'), ('data modeling', 1039, 1052, 'data modeling'), ('algorithm', 1062, 1071, 'algorithm'), ('data cleansing', 1082, 1096, 'data cleansing'), ('cloud management', 1097, 1113, 'cloud management'), ('data quality', 1198, 1210, 'data quality'), ('computer science', 1356, 1372, 'computer science'), ('data mining', 1648, 1659, 'data mining'), ('data normalization', 1701, 1719, 'data normalization'), ('sql', 1753, 1756, 'sql'), ('java', 1757, 1761, 'java'), ('sql', 1790, 1793, 'sql'), ('database design', 1794, 1809, 'database design'), ('analytics', 1894, 1903, 'analytics'), ('data modeling', 1921, 1934, 'data modeling'), ('data visualization', 1935, 1953, 'data visualization')]\n",
      "Gold standard entities with positions and labels: [('data pipeline management', 789, 813, 'data pipeline management'), ('data system', 823, 834, 'data system'), ('data acquisition', 910, 926, 'data acquisition'), ('data modeling', 1039, 1052, 'data modeling'), ('algorithm', 1062, 1071, 'algorithm'), ('data cleansing', 1082, 1096, 'data cleansing'), ('cloud management', 1097, 1113, 'cloud management'), ('data quality', 1198, 1210, 'data quality'), ('computer science', 1356, 1372, 'computer science'), ('data mining', 1648, 1659, 'data mining'), ('data normalization', 1701, 1719, 'data normalization'), ('sql', 1753, 1756, 'sql'), ('java', 1757, 1761, 'java'), ('sql', 1790, 1793, 'sql'), ('database design', 1794, 1809, 'database design'), ('analytics', 1894, 1903, 'analytics'), ('data modeling', 1921, 1934, 'data modeling'), ('data visualization', 1935, 1953, 'data visualization')]\n",
      "Matched entities: [('data system', 823, 834, 'data system'), ('data acquisition', 910, 926, 'data acquisition'), ('data modeling', 1039, 1052, 'data modeling'), ('algorithm', 1062, 1071, 'algorithm'), ('data cleansing', 1082, 1096, 'data cleansing'), ('cloud management', 1097, 1113, 'cloud management'), ('data quality', 1198, 1210, 'data quality'), ('computer science', 1356, 1372, 'computer science'), ('data mining', 1648, 1659, 'data mining'), ('data normalization', 1701, 1719, 'data normalization'), ('sql', 1753, 1756, 'sql'), ('java', 1757, 1761, 'java'), ('sql', 1790, 1793, 'sql'), ('database design', 1794, 1809, 'database design'), ('analytics', 1894, 1903, 'analytics'), ('data modeling', 1921, 1934, 'data modeling'), ('data visualization', 1935, 1953, 'data visualization')]\n",
      "Predicted entities with positions and labels: [('business intelligence', 416, 437, 'business intelligence'), ('business intelligence', 556, 577, 'business intelligence'), ('data engineering', 965, 981, 'data engineering'), ('business intelligence', 1342, 1363, 'business intelligence'), ('data engineering', 1418, 1434, 'data engineering'), ('java', 1568, 1572, 'java'), ('sql', 1586, 1589, 'sql'), ('database management system', 1601, 1627, 'database management system'), ('sql', 1638, 1641, 'sql'), ('mysql', 1649, 1654, 'mysql'), ('data integration', 1666, 1682, 'data integration'), ('data visualisation', 1719, 1737, 'data visualization'), ('analytics', 1779, 1788, 'analytics'), ('apache spark', 1796, 1808, 'apache spark')]\n",
      "Gold standard entities with positions and labels: [('business intelligence', 416, 437, 'business intelligence'), ('business intelligence', 556, 577, 'business intelligence'), ('data engineering', 965, 981, 'data engineering'), ('business intelligence', 1342, 1363, 'business intelligence'), ('data engineering', 1418, 1434, 'data engineering'), ('java', 1568, 1572, 'java'), ('sql', 1586, 1589, 'sql'), ('database management system', 1601, 1627, 'database management system'), ('sql', 1638, 1641, 'sql'), ('mysql', 1649, 1654, 'mysql'), ('data integration', 1666, 1682, 'data integration'), ('analytics', 1779, 1788, 'analytics'), ('apache spark', 1796, 1808, 'apache spark')]\n",
      "Matched entities: [('business intelligence', 416, 437, 'business intelligence'), ('business intelligence', 556, 577, 'business intelligence'), ('data engineering', 965, 981, 'data engineering'), ('business intelligence', 1342, 1363, 'business intelligence'), ('data engineering', 1418, 1434, 'data engineering'), ('java', 1568, 1572, 'java'), ('sql', 1586, 1589, 'sql'), ('database management system', 1601, 1627, 'database management system'), ('sql', 1638, 1641, 'sql'), ('mysql', 1649, 1654, 'mysql'), ('data integration', 1666, 1682, 'data integration'), ('analytics', 1779, 1788, 'analytics'), ('apache spark', 1796, 1808, 'apache spark')]\n",
      "Predicted entities with positions and labels: [('business intelligence', 379, 400, 'business intelligence'), ('business intelligence', 469, 490, 'business intelligence'), ('data quality', 533, 545, 'data quality'), ('algorithm', 630, 639, 'algorithm'), ('data mining', 801, 812, 'data mining'), ('data integration', 851, 867, 'data integration'), ('java', 884, 888, 'java'), ('sql', 889, 892, 'sql'), ('database', 1180, 1188, 'database'), ('business intelligence', 1266, 1287, 'business intelligence')]\n",
      "Gold standard entities with positions and labels: [('business intelligence', 379, 400, 'business intelligence'), ('business intelligence', 469, 490, 'business intelligence'), ('data quality', 533, 545, 'data quality'), ('algorithm', 630, 639, 'algorithm'), ('data mining', 801, 812, 'data mining'), ('data integration', 851, 867, 'data integration'), ('java', 884, 888, 'java'), ('sql', 889, 892, 'sql'), ('database', 1180, 1188, 'database'), ('business intelligence', 1266, 1287, 'business intelligence')]\n",
      "Matched entities: [('business intelligence', 379, 400, 'business intelligence'), ('business intelligence', 469, 490, 'business intelligence'), ('data quality', 533, 545, 'data quality'), ('algorithm', 630, 639, 'algorithm'), ('data mining', 801, 812, 'data mining'), ('data integration', 851, 867, 'data integration'), ('java', 884, 888, 'java'), ('sql', 889, 892, 'sql'), ('database', 1180, 1188, 'database'), ('business intelligence', 1266, 1287, 'business intelligence')]\n",
      "Predicted entities with positions and labels: [('amazon', 0, 6, 'amazon'), ('data center', 53, 64, 'data center'), ('data center', 475, 486, 'data center'), ('data center', 584, 595, 'data center'), ('amazon', 848, 854, 'amazon'), ('data center', 866, 877, 'data center'), ('amazon', 1043, 1049, 'amazon'), ('data center', 1139, 1150, 'data center'), ('data center', 1566, 1577, 'data center'), ('data center', 1606, 1617, 'data center'), ('amazon', 1863, 1869, 'amazon'), ('cloud computing', 1947, 1962, 'cloud computing'), ('data center', 2833, 2844, 'data center'), ('problem solving', 2993, 3008, 'problem solving'), ('data processing', 3070, 3085, 'data processing'), ('data center', 3183, 3194, 'data center'), ('data center', 3645, 3656, 'data center'), ('amazon', 4509, 4515, 'amazon'), ('amazon', 4567, 4573, 'amazon'), ('amazon', 4617, 4623, 'amazon'), ('amazon', 4662, 4668, 'amazon'), ('http', 4898, 4902, 'http'), ('amazon', 4907, 4913, 'amazon')]\n",
      "Gold standard entities with positions and labels: [('amazon', 0, 6, 'amazon'), ('data center', 53, 64, 'data center'), ('data center', 475, 486, 'data center'), ('data center', 584, 595, 'data center'), ('amazon', 848, 854, 'amazon'), ('data center', 866, 877, 'data center'), ('amazon', 1043, 1049, 'amazon'), ('data center', 1139, 1150, 'data center'), ('data center', 1566, 1577, 'data center'), ('data center', 1606, 1617, 'data center'), ('amazon web service', 1863, 1881, 'amazon web service'), ('cloud computing', 1947, 1962, 'cloud computing'), ('data center', 2833, 2844, 'data center'), ('problem solving', 2993, 3008, 'problem solving'), ('data processing', 3070, 3085, 'data processing'), ('data center', 3183, 3194, 'data center'), ('data center', 3645, 3656, 'data center'), ('amazon', 4509, 4515, 'amazon'), ('amazon', 4567, 4573, 'amazon'), ('amazon', 4617, 4623, 'amazon'), ('amazon', 4662, 4668, 'amazon'), ('http', 4898, 4902, 'http'), ('amazon', 4907, 4913, 'amazon')]\n",
      "Matched entities: [('amazon', 0, 6, 'amazon'), ('data center', 53, 64, 'data center'), ('data center', 475, 486, 'data center'), ('data center', 584, 595, 'data center'), ('amazon', 848, 854, 'amazon'), ('data center', 866, 877, 'data center'), ('amazon', 1043, 1049, 'amazon'), ('data center', 1139, 1150, 'data center'), ('data center', 1566, 1577, 'data center'), ('data center', 1606, 1617, 'data center'), ('cloud computing', 1947, 1962, 'cloud computing'), ('data center', 2833, 2844, 'data center'), ('problem solving', 2993, 3008, 'problem solving'), ('data processing', 3070, 3085, 'data processing'), ('data center', 3183, 3194, 'data center'), ('data center', 3645, 3656, 'data center'), ('amazon', 4509, 4515, 'amazon'), ('amazon', 4567, 4573, 'amazon'), ('amazon', 4617, 4623, 'amazon'), ('amazon', 4662, 4668, 'amazon'), ('http', 4898, 4902, 'http'), ('amazon', 4907, 4913, 'amazon')]\n",
      "Predicted entities with positions and labels: [('data acquisition', 209, 225, 'data acquisition'), ('data management', 441, 456, 'data management'), ('data management', 602, 617, 'data management'), ('data processing', 717, 732, 'data processing'), ('data modeling', 909, 922, 'data modeling'), ('data management', 1026, 1041, 'data management'), ('computer science', 1823, 1839, 'computer science'), ('java', 2142, 2146, 'java'), ('c', 2165, 2166, 'c'), ('data pipeline', 2207, 2220, 'data pipeline'), ('technical analysis', 2323, 2341, 'technical analysis'), ('problem solving', 2362, 2377, 'problem solving')]\n",
      "Gold standard entities with positions and labels: [('data acquisition', 209, 225, 'data acquisition'), ('data management', 441, 456, 'data management'), ('data management', 602, 617, 'data management'), ('data processing', 717, 732, 'data processing'), ('data modeling', 909, 922, 'data modeling'), ('data management', 1026, 1041, 'data management'), ('computer science', 1823, 1839, 'computer science'), ('java', 2142, 2146, 'java'), ('c', 2165, 2166, 'c'), ('data pipeline', 2207, 2220, 'data pipeline'), ('technical analysis', 2323, 2341, 'technical analysis'), ('problem solving', 2362, 2377, 'problem solving')]\n",
      "Matched entities: [('data acquisition', 209, 225, 'data acquisition'), ('data management', 441, 456, 'data management'), ('data management', 602, 617, 'data management'), ('data processing', 717, 732, 'data processing'), ('data modeling', 909, 922, 'data modeling'), ('data management', 1026, 1041, 'data management'), ('computer science', 1823, 1839, 'computer science'), ('java', 2142, 2146, 'java'), ('c', 2165, 2166, 'c'), ('data pipeline', 2207, 2220, 'data pipeline'), ('technical analysis', 2323, 2341, 'technical analysis'), ('problem solving', 2362, 2377, 'problem solving')]\n",
      "Predicted entities with positions and labels: [('data architecture', 88, 105, 'data architecture'), ('data infrastructure', 168, 187, 'data infrastructure'), ('data pipeline', 368, 381, 'data pipeline'), ('azure data factory', 382, 400, 'azure data factory'), ('azure data factory', 497, 515, 'azure data factory'), ('azure data lake', 534, 549, 'azure data lake'), ('analytics', 550, 559, 'analytics'), ('data storage', 638, 650, 'data storage'), ('sql', 666, 669, 'sql'), ('database', 670, 678, 'database'), ('analytics', 693, 702, 'analytics'), ('data mart', 899, 908, 'data mart'), ('azure data factory', 1050, 1068, 'azure data factory'), ('sql', 1089, 1092, 'sql'), ('azure data lake', 1093, 1108, 'azure data lake'), ('data pipeline', 1155, 1168, 'data pipeline'), ('ci cd', 1246, 1251, 'ci cd'), ('azure databricks', 1292, 1308, 'azure databricks'), ('analytics', 1354, 1363, 'analytics'), ('power', 1364, 1369, 'bigtable'), ('sql', 1396, 1399, 'sql'), ('database', 1400, 1408, 'database'), ('azure data factory', 1409, 1427, 'azure data factory'), ('data store', 1461, 1471, 'data store'), ('c', 1482, 1483, 'c'), ('computer science', 1585, 1601, 'computer science'), ('azure data factory', 1655, 1673, 'azure data factory'), ('data modeling', 1770, 1783, 'data modeling'), ('database management', 1796, 1815, 'database management'), ('sql azure', 1828, 1837, 'sql azure'), ('sql', 1838, 1841, 'sql'), ('azure data lake', 1842, 1857, 'azure data lake'), ('data warehousing', 1878, 1894, 'data warehousing'), ('problem solving', 1918, 1933, 'problem solving'), ('project management', 1961, 1979, 'project management')]\n",
      "Gold standard entities with positions and labels: [('data architecture', 88, 105, 'data architecture'), ('data infrastructure', 168, 187, 'data infrastructure'), ('data pipeline', 368, 381, 'data pipeline'), ('azure data factory', 382, 400, 'azure data factory'), ('azure data factory', 497, 515, 'azure data factory'), ('azure data lake', 534, 549, 'azure data lake'), ('analytics', 550, 559, 'analytics'), ('data storage', 638, 650, 'data storage'), ('sql', 666, 669, 'sql'), ('database', 670, 678, 'database'), ('analytics', 693, 702, 'analytics'), ('data mart', 899, 908, 'data mart'), ('azure data factory', 1050, 1068, 'azure data factory'), ('sql', 1089, 1092, 'sql'), ('azure data lake', 1093, 1108, 'azure data lake'), ('data pipeline', 1155, 1168, 'data pipeline'), ('ci cd', 1246, 1251, 'ci cd'), ('azure databricks', 1292, 1308, 'azure databricks'), ('analytics', 1354, 1363, 'analytics'), ('power bi', 1364, 1372, 'power bi'), ('sql', 1396, 1399, 'sql'), ('database', 1400, 1408, 'database'), ('azure data factory', 1409, 1427, 'azure data factory'), ('data store', 1461, 1471, 'data store'), ('c', 1482, 1483, 'c'), ('computer science', 1585, 1601, 'computer science'), ('azure data factory', 1655, 1673, 'azure data factory'), ('data modeling', 1770, 1783, 'data modeling'), ('database management', 1796, 1815, 'database management'), ('sql azure', 1828, 1837, 'sql azure'), ('sql', 1838, 1841, 'sql'), ('azure data lake', 1842, 1857, 'azure data lake'), ('data warehousing', 1878, 1894, 'data warehousing'), ('problem solving', 1918, 1933, 'problem solving'), ('project management', 1961, 1979, 'project management')]\n",
      "Matched entities: [('data architecture', 88, 105, 'data architecture'), ('data infrastructure', 168, 187, 'data infrastructure'), ('data pipeline', 368, 381, 'data pipeline'), ('azure data factory', 382, 400, 'azure data factory'), ('azure data factory', 497, 515, 'azure data factory'), ('azure data lake', 534, 549, 'azure data lake'), ('analytics', 550, 559, 'analytics'), ('data storage', 638, 650, 'data storage'), ('sql', 666, 669, 'sql'), ('database', 670, 678, 'database'), ('analytics', 693, 702, 'analytics'), ('data mart', 899, 908, 'data mart'), ('azure data factory', 1050, 1068, 'azure data factory'), ('sql', 1089, 1092, 'sql'), ('azure data lake', 1093, 1108, 'azure data lake'), ('data pipeline', 1155, 1168, 'data pipeline'), ('ci cd', 1246, 1251, 'ci cd'), ('azure databricks', 1292, 1308, 'azure databricks'), ('analytics', 1354, 1363, 'analytics'), ('sql', 1396, 1399, 'sql'), ('database', 1400, 1408, 'database'), ('azure data factory', 1409, 1427, 'azure data factory'), ('data store', 1461, 1471, 'data store'), ('c', 1482, 1483, 'c'), ('computer science', 1585, 1601, 'computer science'), ('azure data factory', 1655, 1673, 'azure data factory'), ('data modeling', 1770, 1783, 'data modeling'), ('database management', 1796, 1815, 'database management'), ('sql azure', 1828, 1837, 'sql azure'), ('sql', 1838, 1841, 'sql'), ('azure data lake', 1842, 1857, 'azure data lake'), ('data warehousing', 1878, 1894, 'data warehousing'), ('problem solving', 1918, 1933, 'problem solving'), ('project management', 1961, 1979, 'project management')]\n",
      "Predicted entities with positions and labels: [('business analysis', 398, 415, 'business analysis'), ('business analysis', 613, 630, 'business analysis'), ('data management', 631, 646, 'data management'), ('data science', 647, 659, 'data science'), ('analytics', 776, 785, 'analytics'), ('digital data', 844, 856, 'digital data'), ('data science', 1243, 1255, 'data science'), ('analytics', 1261, 1270, 'analytics'), ('data engineering', 1271, 1287, 'data engineering'), ('data management', 1288, 1303, 'data management'), ('data processing', 1480, 1495, 'data processing'), ('data processing', 1502, 1517, 'data processing'), ('data analysis', 1663, 1676, 'data analysis'), ('data cleansing', 1677, 1691, 'data cleansing'), ('data domain', 1707, 1718, 'data domain'), ('data integration', 1729, 1745, 'data integration'), ('data management', 1746, 1761, 'data management'), ('data manipulation', 1762, 1779, 'data manipulation'), ('data strategy', 1794, 1807, 'data strategy'), ('data structure', 1819, 1833, 'data structure'), ('algorithm', 1834, 1843, 'algorithm'), ('data visualization', 1844, 1862, 'data visualization'), ('problem solving', 1918, 1933, 'problem solving')]\n",
      "Gold standard entities with positions and labels: [('business analysis', 398, 415, 'business analysis'), ('business analysis', 613, 630, 'business analysis'), ('data management', 631, 646, 'data management'), ('data science', 647, 659, 'data science'), ('analytics', 776, 785, 'analytics'), ('digital data', 844, 856, 'digital data'), ('data science', 1243, 1255, 'data science'), ('analytics', 1261, 1270, 'analytics'), ('data engineering', 1271, 1287, 'data engineering'), ('data management', 1288, 1303, 'data management'), ('data processing', 1480, 1495, 'data processing'), ('data processing', 1502, 1517, 'data processing'), ('data analysis', 1663, 1676, 'data analysis'), ('data cleansing', 1677, 1691, 'data cleansing'), ('data domain', 1707, 1718, 'data domain'), ('data integration', 1729, 1745, 'data integration'), ('data management', 1746, 1761, 'data management'), ('data manipulation', 1762, 1779, 'data manipulation'), ('data strategy', 1794, 1807, 'data strategy'), ('data structure', 1819, 1833, 'data structure'), ('algorithm', 1834, 1843, 'algorithm'), ('data visualization', 1844, 1862, 'data visualization'), ('problem solving', 1918, 1933, 'problem solving')]\n",
      "Matched entities: [('business analysis', 398, 415, 'business analysis'), ('business analysis', 613, 630, 'business analysis'), ('data management', 631, 646, 'data management'), ('data science', 647, 659, 'data science'), ('analytics', 776, 785, 'analytics'), ('digital data', 844, 856, 'digital data'), ('data science', 1243, 1255, 'data science'), ('analytics', 1261, 1270, 'analytics'), ('data engineering', 1271, 1287, 'data engineering'), ('data management', 1288, 1303, 'data management'), ('data processing', 1480, 1495, 'data processing'), ('data processing', 1502, 1517, 'data processing'), ('data analysis', 1663, 1676, 'data analysis'), ('data cleansing', 1677, 1691, 'data cleansing'), ('data domain', 1707, 1718, 'data domain'), ('data integration', 1729, 1745, 'data integration'), ('data management', 1746, 1761, 'data management'), ('data manipulation', 1762, 1779, 'data manipulation'), ('data strategy', 1794, 1807, 'data strategy'), ('data structure', 1819, 1833, 'data structure'), ('algorithm', 1834, 1843, 'algorithm'), ('data visualization', 1844, 1862, 'data visualization'), ('problem solving', 1918, 1933, 'problem solving')]\n",
      "Predicted entities with positions and labels: [('business intelligence', 379, 400, 'business intelligence'), ('business intelligence', 469, 490, 'business intelligence'), ('data quality', 533, 545, 'data quality'), ('algorithm', 630, 639, 'algorithm'), ('data mining', 801, 812, 'data mining'), ('data integration', 851, 867, 'data integration'), ('java', 884, 888, 'java'), ('sql', 889, 892, 'sql'), ('database', 1180, 1188, 'database'), ('business intelligence', 1266, 1287, 'business intelligence')]\n",
      "Gold standard entities with positions and labels: [('business intelligence', 379, 400, 'business intelligence'), ('business intelligence', 469, 490, 'business intelligence'), ('data quality', 533, 545, 'data quality'), ('algorithm', 630, 639, 'algorithm'), ('data mining', 801, 812, 'data mining'), ('data integration', 851, 867, 'data integration'), ('java', 884, 888, 'java'), ('sql', 889, 892, 'sql'), ('database', 1180, 1188, 'database'), ('business intelligence', 1266, 1287, 'business intelligence')]\n",
      "Matched entities: [('business intelligence', 379, 400, 'business intelligence'), ('business intelligence', 469, 490, 'business intelligence'), ('data quality', 533, 545, 'data quality'), ('algorithm', 630, 639, 'algorithm'), ('data mining', 801, 812, 'data mining'), ('data integration', 851, 867, 'data integration'), ('java', 884, 888, 'java'), ('sql', 889, 892, 'sql'), ('database', 1180, 1188, 'database'), ('business intelligence', 1266, 1287, 'business intelligence')]\n",
      "Predicted entities with positions and labels: [('problem solving', 278, 293, 'problem solving'), ('data system', 426, 437, 'data system'), ('data processing', 438, 453, 'data processing'), ('data transformation', 454, 473, 'data transformation'), ('data pipeline', 1113, 1126, 'data pipeline'), ('database', 1243, 1251, 'database'), ('nosql', 1292, 1297, 'nosql'), ('database', 1298, 1306, 'database'), ('bigtable', 1329, 1337, 'bigtable'), ('database system', 1364, 1379, 'database system'), ('sql', 1382, 1385, 'sql'), ('mysql', 1400, 1405, 'mysql'), ('data pipeline', 1430, 1443, 'data pipeline'), ('azure data factory', 1492, 1510, 'azure data factory'), ('data modeling', 1636, 1649, 'data modeling'), ('continuous integration', 1732, 1754, 'continuous integration'), ('data modeling', 1755, 1768, 'data modeling'), ('nosql', 1802, 1807, 'nosql'), ('graph database', 1819, 1833, 'graph database'), ('data lake', 1849, 1858, 'data lake'), ('data processing', 1859, 1874, 'data processing'), ('sql', 1887, 1890, 'sql'), ('pyspark', 1948, 1955, 'pyspark'), ('java', 1956, 1960, 'java'), ('data validation', 2164, 2179, 'data validation'), ('aws glue', 2327, 2335, 'aws glue'), ('code review', 2372, 2383, 'code review'), ('computer science', 2411, 2427, 'computer science'), ('problem solving', 3153, 3168, 'problem solving')]\n",
      "Gold standard entities with positions and labels: [('problem solving', 278, 293, 'problem solving'), ('data system', 426, 437, 'data system'), ('data processing', 438, 453, 'data processing'), ('data transformation', 454, 473, 'data transformation'), ('data pipeline', 1113, 1126, 'data pipeline'), ('database', 1243, 1251, 'database'), ('nosql', 1292, 1297, 'nosql'), ('database', 1298, 1306, 'database'), ('bigtable', 1329, 1337, 'bigtable'), ('database system', 1364, 1379, 'database system'), ('sql', 1382, 1385, 'sql'), ('mysql', 1400, 1405, 'mysql'), ('data pipeline', 1430, 1443, 'data pipeline'), ('azure data factory', 1492, 1510, 'azure data factory'), ('data modeling', 1636, 1649, 'data modeling'), ('continuous integration', 1732, 1754, 'continuous integration'), ('data modeling', 1755, 1768, 'data modeling'), ('nosql', 1802, 1807, 'nosql'), ('graph database', 1819, 1833, 'graph database'), ('data lake', 1849, 1858, 'data lake'), ('data processing', 1859, 1874, 'data processing'), ('sql', 1887, 1890, 'sql'), ('pyspark', 1948, 1955, 'pyspark'), ('java', 1956, 1960, 'java'), ('data validation', 2164, 2179, 'data validation'), ('aws glue', 2327, 2335, 'aws glue'), ('code review', 2372, 2383, 'code review'), ('computer science', 2411, 2427, 'computer science'), ('problem solving', 3153, 3168, 'problem solving')]\n",
      "Matched entities: [('problem solving', 278, 293, 'problem solving'), ('data system', 426, 437, 'data system'), ('data processing', 438, 453, 'data processing'), ('data transformation', 454, 473, 'data transformation'), ('data pipeline', 1113, 1126, 'data pipeline'), ('database', 1243, 1251, 'database'), ('nosql', 1292, 1297, 'nosql'), ('database', 1298, 1306, 'database'), ('bigtable', 1329, 1337, 'bigtable'), ('database system', 1364, 1379, 'database system'), ('sql', 1382, 1385, 'sql'), ('mysql', 1400, 1405, 'mysql'), ('data pipeline', 1430, 1443, 'data pipeline'), ('azure data factory', 1492, 1510, 'azure data factory'), ('data modeling', 1636, 1649, 'data modeling'), ('continuous integration', 1732, 1754, 'continuous integration'), ('data modeling', 1755, 1768, 'data modeling'), ('nosql', 1802, 1807, 'nosql'), ('graph database', 1819, 1833, 'graph database'), ('data lake', 1849, 1858, 'data lake'), ('data processing', 1859, 1874, 'data processing'), ('sql', 1887, 1890, 'sql'), ('pyspark', 1948, 1955, 'pyspark'), ('java', 1956, 1960, 'java'), ('data validation', 2164, 2179, 'data validation'), ('aws glue', 2327, 2335, 'aws glue'), ('code review', 2372, 2383, 'code review'), ('computer science', 2411, 2427, 'computer science'), ('problem solving', 3153, 3168, 'problem solving')]\n",
      "Predicted entities with positions and labels: [('power bi', 672, 680, 'power bi'), ('business intelligence', 768, 789, 'business intelligence'), ('power bi', 831, 839, 'power bi')]\n",
      "Gold standard entities with positions and labels: [('power bi', 672, 680, 'power bi'), ('business intelligence', 768, 789, 'business intelligence'), ('power bi', 831, 839, 'power bi')]\n",
      "Matched entities: [('power bi', 672, 680, 'power bi'), ('business intelligence', 768, 789, 'business intelligence'), ('power bi', 831, 839, 'power bi')]\n",
      "Predicted entities with positions and labels: [('c', 594, 595, 'c'), ('c', 596, 597, 'c'), ('data engineering', 768, 784, 'data engineering'), ('computer science', 2031, 2047, 'computer science'), ('java', 2197, 2201, 'java'), ('sql', 2227, 2230, 'sql'), ('data structure', 2394, 2408, 'data structure'), ('batch processing', 2537, 2553, 'batch processing'), ('apache kafka', 2565, 2577, 'apache kafka'), ('ci cd', 2645, 2650, 'ci cd'), ('data pipeline', 2855, 2868, 'data pipeline')]\n",
      "Gold standard entities with positions and labels: [('c', 594, 595, 'c'), ('c', 596, 597, 'c'), ('data engineering', 768, 784, 'data engineering'), ('computer science', 2031, 2047, 'computer science'), ('java', 2197, 2201, 'java'), ('sql', 2227, 2230, 'sql'), ('data structure', 2394, 2408, 'data structure'), ('batch processing', 2537, 2553, 'batch processing'), ('apache kafka', 2565, 2577, 'apache kafka'), ('ci cd', 2645, 2650, 'ci cd'), ('data pipeline', 2855, 2868, 'data pipeline')]\n",
      "Matched entities: [('c', 594, 595, 'c'), ('c', 596, 597, 'c'), ('data engineering', 768, 784, 'data engineering'), ('computer science', 2031, 2047, 'computer science'), ('java', 2197, 2201, 'java'), ('sql', 2227, 2230, 'sql'), ('data structure', 2394, 2408, 'data structure'), ('batch processing', 2537, 2553, 'batch processing'), ('apache kafka', 2565, 2577, 'apache kafka'), ('ci cd', 2645, 2650, 'ci cd'), ('data pipeline', 2855, 2868, 'data pipeline')]\n",
      "Predicted entities with positions and labels: [('application data', 1411, 1427, 'application data'), ('big data', 1727, 1735, 'big data'), ('data quality', 1806, 1818, 'data quality'), ('analytics', 1847, 1856, 'analytics'), ('business intelligence tool', 1886, 1912, 'business intelligence tool'), ('data security', 1921, 1934, 'data security'), ('data governance', 1956, 1971, 'data governance'), ('continuous integration', 2011, 2033, 'continuous integration'), ('data architecture', 2336, 2353, 'data architecture'), ('data quality', 2472, 2484, 'data quality'), ('sql', 2547, 2550, 'sql'), ('databricks', 2612, 2622, 'databricks'), ('airflow', 2623, 2630, 'airflow'), ('batch processing', 2631, 2647, 'batch processing'), ('sql', 2648, 2651, 'sql'), ('data pipeline', 2697, 2710, 'data pipeline'), ('databricks', 2806, 2816, 'databricks'), ('database system', 2850, 2865, 'database system'), ('data store', 2935, 2945, 'data store'), ('data lake', 3002, 3011, 'data lake')]\n",
      "Gold standard entities with positions and labels: [('application data', 1411, 1427, 'application data'), ('big data', 1727, 1735, 'big data'), ('data quality', 1806, 1818, 'data quality'), ('analytics', 1847, 1856, 'analytics'), ('business intelligence tool', 1886, 1912, 'business intelligence tool'), ('data security', 1921, 1934, 'data security'), ('data governance', 1956, 1971, 'data governance'), ('continuous integration', 2011, 2033, 'continuous integration'), ('data architecture', 2336, 2353, 'data architecture'), ('data quality', 2472, 2484, 'data quality'), ('sql', 2547, 2550, 'sql'), ('databricks', 2612, 2622, 'databricks'), ('airflow', 2623, 2630, 'airflow'), ('batch processing', 2631, 2647, 'batch processing'), ('sql', 2648, 2651, 'sql'), ('data pipeline', 2697, 2710, 'data pipeline'), ('databricks', 2806, 2816, 'databricks'), ('database system', 2850, 2865, 'database system'), ('data store', 2935, 2945, 'data store'), ('data lake', 3002, 3011, 'data lake')]\n",
      "Matched entities: [('application data', 1411, 1427, 'application data'), ('big data', 1727, 1735, 'big data'), ('data quality', 1806, 1818, 'data quality'), ('analytics', 1847, 1856, 'analytics'), ('business intelligence tool', 1886, 1912, 'business intelligence tool'), ('data security', 1921, 1934, 'data security'), ('data governance', 1956, 1971, 'data governance'), ('continuous integration', 2011, 2033, 'continuous integration'), ('data architecture', 2336, 2353, 'data architecture'), ('data quality', 2472, 2484, 'data quality'), ('sql', 2547, 2550, 'sql'), ('databricks', 2612, 2622, 'databricks'), ('airflow', 2623, 2630, 'airflow'), ('batch processing', 2631, 2647, 'batch processing'), ('sql', 2648, 2651, 'sql'), ('data pipeline', 2697, 2710, 'data pipeline'), ('databricks', 2806, 2816, 'databricks'), ('database system', 2850, 2865, 'database system'), ('data store', 2935, 2945, 'data store'), ('data lake', 3002, 3011, 'data lake')]\n",
      "Predicted entities with positions and labels: [('data fusion', 240, 251, 'data fusion'), ('artificial intelligence', 252, 275, 'artificial intelligence'), ('artificial intelligence', 1935, 1958, 'artificial intelligence'), ('devops', 2017, 2023, 'devops'), ('business requirement', 2247, 2267, 'business requirement'), ('data pipeline', 2457, 2470, 'data pipeline'), ('cloud computing', 2678, 2693, 'cloud computing'), ('database system', 2753, 2768, 'database system'), ('data sharing', 2779, 2791, 'data sharing'), ('database', 2840, 2848, 'database'), ('database', 2890, 2898, 'database'), ('data mining', 2936, 2947, 'data mining'), ('data warehousing', 2948, 2964, 'data warehousing'), ('data integration', 3056, 3072, 'data integration'), ('data quality', 3073, 3085, 'data quality'), ('sql', 3173, 3176, 'sql'), ('data modeling', 3177, 3190, 'data modeling'), ('database', 3270, 3278, 'database'), ('java', 3314, 3318, 'java'), ('data engineering', 3377, 3393, 'data engineering'), ('big data', 3403, 3411, 'big data'), ('apache airflow', 3445, 3459, 'apache airflow'), ('sql', 3480, 3483, 'sql'), ('database', 3484, 3492, 'database'), ('data engineering', 3493, 3509, 'data engineering'), ('nosql', 3525, 3530, 'nosql'), ('data science', 3568, 3580, 'data science')]\n",
      "Gold standard entities with positions and labels: [('data fusion', 240, 251, 'data fusion'), ('artificial intelligence', 252, 275, 'artificial intelligence'), ('artificial intelligence', 1935, 1958, 'artificial intelligence'), ('devops', 2017, 2023, 'devops'), ('business requirement', 2247, 2267, 'business requirement'), ('data pipeline', 2457, 2470, 'data pipeline'), ('cloud computing', 2678, 2693, 'cloud computing'), ('database system', 2753, 2768, 'database system'), ('data sharing', 2779, 2791, 'data sharing'), ('database', 2840, 2848, 'database'), ('database', 2890, 2898, 'database'), ('data mining', 2936, 2947, 'data mining'), ('data warehousing', 2948, 2964, 'data warehousing'), ('data integration', 3056, 3072, 'data integration'), ('data quality', 3073, 3085, 'data quality'), ('sql', 3173, 3176, 'sql'), ('data modeling', 3177, 3190, 'data modeling'), ('database', 3270, 3278, 'database'), ('java', 3314, 3318, 'java'), ('data engineering', 3377, 3393, 'data engineering'), ('big data', 3403, 3411, 'big data'), ('apache airflow', 3445, 3459, 'apache airflow'), ('sql', 3480, 3483, 'sql'), ('database', 3484, 3492, 'database'), ('data engineering', 3493, 3509, 'data engineering'), ('nosql', 3525, 3530, 'nosql'), ('data science', 3568, 3580, 'data science')]\n",
      "Matched entities: [('data fusion', 240, 251, 'data fusion'), ('artificial intelligence', 252, 275, 'artificial intelligence'), ('artificial intelligence', 1935, 1958, 'artificial intelligence'), ('devops', 2017, 2023, 'devops'), ('business requirement', 2247, 2267, 'business requirement'), ('data pipeline', 2457, 2470, 'data pipeline'), ('cloud computing', 2678, 2693, 'cloud computing'), ('database system', 2753, 2768, 'database system'), ('data sharing', 2779, 2791, 'data sharing'), ('database', 2840, 2848, 'database'), ('database', 2890, 2898, 'database'), ('data mining', 2936, 2947, 'data mining'), ('data warehousing', 2948, 2964, 'data warehousing'), ('data integration', 3056, 3072, 'data integration'), ('data quality', 3073, 3085, 'data quality'), ('sql', 3173, 3176, 'sql'), ('data modeling', 3177, 3190, 'data modeling'), ('database', 3270, 3278, 'database'), ('java', 3314, 3318, 'java'), ('data engineering', 3377, 3393, 'data engineering'), ('big data', 3403, 3411, 'big data'), ('apache airflow', 3445, 3459, 'apache airflow'), ('sql', 3480, 3483, 'sql'), ('database', 3484, 3492, 'database'), ('data engineering', 3493, 3509, 'data engineering'), ('nosql', 3525, 3530, 'nosql'), ('data science', 3568, 3580, 'data science')]\n",
      "Predicted entities with positions and labels: [('data center', 700, 711, 'data center'), ('data center', 1673, 1684, 'data center'), ('data center', 2015, 2026, 'data center'), ('data center', 2405, 2416, 'data center')]\n",
      "Gold standard entities with positions and labels: [('data center', 700, 711, 'data center'), ('data center', 1673, 1684, 'data center'), ('data center', 2015, 2026, 'data center'), ('data center', 2405, 2416, 'data center')]\n",
      "Matched entities: [('data center', 700, 711, 'data center'), ('data center', 1673, 1684, 'data center'), ('data center', 2015, 2026, 'data center'), ('data center', 2405, 2416, 'data center')]\n",
      "Predicted entities with positions and labels: [('data management', 560, 575, 'data management'), ('unit testing', 757, 769, 'unit testing'), ('data flow diagram', 1172, 1189, 'data flow diagram'), ('application design', 1214, 1232, 'application design'), ('code review', 1258, 1269, 'code review'), ('problem solving', 1582, 1597, 'problem solving'), ('agile methodology', 2236, 2253, 'agile methodology'), ('data management', 2352, 2367, 'data management'), ('analytics', 2373, 2382, 'analytics'), ('database', 2415, 2423, 'database'), ('sql', 2453, 2456, 'sql')]\n",
      "Gold standard entities with positions and labels: [('data management', 560, 575, 'data management'), ('unit testing', 757, 769, 'unit testing'), ('data flow diagram', 1172, 1189, 'data flow diagram'), ('application design', 1214, 1232, 'application design'), ('code review', 1258, 1269, 'code review'), ('problem solving', 1582, 1597, 'problem solving'), ('agile methodology', 2236, 2253, 'agile methodology'), ('data management', 2352, 2367, 'data management'), ('analytics', 2373, 2382, 'analytics'), ('database', 2415, 2423, 'database'), ('sql', 2453, 2456, 'sql')]\n",
      "Matched entities: [('data management', 560, 575, 'data management'), ('unit testing', 757, 769, 'unit testing'), ('data flow diagram', 1172, 1189, 'data flow diagram'), ('application design', 1214, 1232, 'application design'), ('code review', 1258, 1269, 'code review'), ('problem solving', 1582, 1597, 'problem solving'), ('agile methodology', 2236, 2253, 'agile methodology'), ('data management', 2352, 2367, 'data management'), ('analytics', 2373, 2382, 'analytics'), ('database', 2415, 2423, 'database'), ('sql', 2453, 2456, 'sql')]\n",
      "Predicted entities with positions and labels: [('data architecture', 542, 559, 'data architecture'), ('data pipeline', 1145, 1158, 'data pipeline'), ('data engineering', 1456, 1472, 'data engineering'), ('data governance', 1473, 1488, 'data governance'), ('relational database management system year experience etl product experience informatica cloud plus year experience scripting language', 1589, 1723, 'relational database management system'), ('data visualization', 1778, 1796, 'data visualization'), ('business intelligence', 1797, 1818, 'business intelligence'), ('http', 2404, 2408, 'http')]\n",
      "Gold standard entities with positions and labels: [('data architecture', 542, 559, 'data architecture'), ('data pipeline', 1145, 1158, 'data pipeline'), ('data engineering', 1456, 1472, 'data engineering'), ('data governance', 1473, 1488, 'data governance'), ('relational database management system', 1589, 1626, 'relational database management system'), ('data visualization', 1778, 1796, 'data visualization'), ('business intelligence', 1797, 1818, 'business intelligence'), ('http', 2404, 2408, 'http')]\n",
      "Matched entities: [('data architecture', 542, 559, 'data architecture'), ('data pipeline', 1145, 1158, 'data pipeline'), ('data engineering', 1456, 1472, 'data engineering'), ('data governance', 1473, 1488, 'data governance'), ('data visualization', 1778, 1796, 'data visualization'), ('business intelligence', 1797, 1818, 'business intelligence'), ('http', 2404, 2408, 'http')]\n",
      "Predicted entities with positions and labels: [('data center', 0, 11, 'data center'), ('data center', 128, 139, 'data center'), ('data center', 479, 490, 'data center'), ('data center', 760, 771, 'data center'), ('data center', 872, 883, 'data center'), ('data center', 1291, 1302, 'data center'), ('data center', 1303, 1314, 'data center'), ('data center', 2503, 2514, 'data center'), ('data center', 2579, 2590, 'data center'), ('data center', 2839, 2850, 'data center'), ('data center', 3024, 3035, 'data center'), ('data center', 3428, 3439, 'data center'), ('data center', 3601, 3612, 'data center'), ('data center', 3669, 3680, 'data center'), ('data center', 4438, 4449, 'data center'), ('data center', 4589, 4600, 'data center'), ('data center', 4826, 4837, 'data center'), ('data center', 5429, 5440, 'data center'), ('data center', 5612, 5623, 'data center'), ('problem solving', 5781, 5796, 'problem solving')]\n",
      "Gold standard entities with positions and labels: [('data center', 0, 11, 'data center'), ('data center', 128, 139, 'data center'), ('data center', 479, 490, 'data center'), ('data center', 760, 771, 'data center'), ('data center', 872, 883, 'data center'), ('data center', 1291, 1302, 'data center'), ('data center', 1303, 1314, 'data center'), ('data center', 2503, 2514, 'data center'), ('data center', 2579, 2590, 'data center'), ('data center', 2839, 2850, 'data center'), ('data center', 3024, 3035, 'data center'), ('data center', 3428, 3439, 'data center'), ('data center', 3601, 3612, 'data center'), ('data center', 3669, 3680, 'data center'), ('data center', 4438, 4449, 'data center'), ('data center', 4589, 4600, 'data center'), ('data center', 4826, 4837, 'data center'), ('data center', 5429, 5440, 'data center'), ('data center', 5612, 5623, 'data center'), ('problem solving', 5781, 5796, 'problem solving')]\n",
      "Matched entities: [('data center', 0, 11, 'data center'), ('data center', 128, 139, 'data center'), ('data center', 479, 490, 'data center'), ('data center', 760, 771, 'data center'), ('data center', 872, 883, 'data center'), ('data center', 1291, 1302, 'data center'), ('data center', 1303, 1314, 'data center'), ('data center', 2503, 2514, 'data center'), ('data center', 2579, 2590, 'data center'), ('data center', 2839, 2850, 'data center'), ('data center', 3024, 3035, 'data center'), ('data center', 3428, 3439, 'data center'), ('data center', 3601, 3612, 'data center'), ('data center', 3669, 3680, 'data center'), ('data center', 4438, 4449, 'data center'), ('data center', 4589, 4600, 'data center'), ('data center', 4826, 4837, 'data center'), ('data center', 5429, 5440, 'data center'), ('data center', 5612, 5623, 'data center'), ('problem solving', 5781, 5796, 'problem solving')]\n",
      "Predicted entities with positions and labels: [('statistical analysis', 628, 648, 'statistical analysis'), ('predictive analytics', 659, 679, 'predictive analytics'), ('data manipulation', 680, 697, 'data manipulation'), ('quantitative analysis', 921, 942, 'quantitative analysis'), ('business intelligence tool', 1418, 1444, 'business intelligence tool'), ('sql', 1619, 1622, 'sql'), ('database', 1629, 1637, 'database'), ('c', 2217, 2218, 'c'), ('c', 2276, 2277, 'c')]\n",
      "Gold standard entities with positions and labels: [('statistical analysis', 628, 648, 'statistical analysis'), ('predictive analytics', 659, 679, 'predictive analytics'), ('data manipulation', 680, 697, 'data manipulation'), ('quantitative analysis', 921, 942, 'quantitative analysis'), ('business intelligence tool', 1418, 1444, 'business intelligence tool'), ('sql', 1619, 1622, 'sql'), ('database', 1629, 1637, 'database'), ('c', 2217, 2218, 'c'), ('c', 2276, 2277, 'c')]\n",
      "Matched entities: [('statistical analysis', 628, 648, 'statistical analysis'), ('predictive analytics', 659, 679, 'predictive analytics'), ('data manipulation', 680, 697, 'data manipulation'), ('quantitative analysis', 921, 942, 'quantitative analysis'), ('business intelligence tool', 1418, 1444, 'business intelligence tool'), ('sql', 1619, 1622, 'sql'), ('database', 1629, 1637, 'database'), ('c', 2217, 2218, 'c'), ('c', 2276, 2277, 'c')]\n",
      "Predicted entities with positions and labels: [('analytics', 404, 413, 'analytics'), ('business intelligence', 426, 447, 'business intelligence'), ('data engineering', 566, 582, 'data engineering'), ('database', 628, 636, 'database'), ('data engineering', 658, 674, 'data engineering'), ('data engineering', 690, 706, 'data engineering'), ('database', 785, 793, 'database'), ('data science', 926, 938, 'data science'), ('c', 949, 950, 'c'), ('sql', 1086, 1089, 'sql'), ('database', 1159, 1167, 'database'), ('amazon redshift', 1178, 1193, 'amazon redshift'), ('sql', 1204, 1207, 'sql'), ('airflow', 1312, 1319, 'airflow'), ('agile methodology', 1372, 1389, 'agile methodology')]\n",
      "Gold standard entities with positions and labels: [('analytics', 404, 413, 'analytics'), ('business intelligence', 426, 447, 'business intelligence'), ('data engineering', 566, 582, 'data engineering'), ('database', 628, 636, 'database'), ('data engineering', 658, 674, 'data engineering'), ('data engineering', 690, 706, 'data engineering'), ('database', 785, 793, 'database'), ('data science', 926, 938, 'data science'), ('c', 949, 950, 'c'), ('sql', 1086, 1089, 'sql'), ('database', 1159, 1167, 'database'), ('amazon redshift', 1178, 1193, 'amazon redshift'), ('sql', 1204, 1207, 'sql'), ('airflow', 1312, 1319, 'airflow'), ('agile methodology', 1372, 1389, 'agile methodology')]\n",
      "Matched entities: [('analytics', 404, 413, 'analytics'), ('business intelligence', 426, 447, 'business intelligence'), ('data engineering', 566, 582, 'data engineering'), ('database', 628, 636, 'database'), ('data engineering', 658, 674, 'data engineering'), ('data engineering', 690, 706, 'data engineering'), ('database', 785, 793, 'database'), ('data science', 926, 938, 'data science'), ('c', 949, 950, 'c'), ('sql', 1086, 1089, 'sql'), ('database', 1159, 1167, 'database'), ('amazon redshift', 1178, 1193, 'amazon redshift'), ('sql', 1204, 1207, 'sql'), ('airflow', 1312, 1319, 'airflow'), ('agile methodology', 1372, 1389, 'agile methodology')]\n"
     ]
    }
   ],
   "source": [
    "for text, annots in val_data:\n",
    "    doc_gold_text = nlp.make_doc(text)  # the document text\n",
    "    gold = Example.from_dict(doc_gold_text, annots)  # the gold-standard Example object\n",
    "\n",
    "    # Using the model to predict the outcome on the same text\n",
    "    doc_pred = nlp(text)  # the predicted document\n",
    "\n",
    "    # Print predicted entities with their span info\n",
    "    pred_ents = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc_pred.ents]\n",
    "    print(\"Predicted entities with positions and labels:\", pred_ents)\n",
    "\n",
    "    # Print the gold standard entity annotations for comparison\n",
    "    gold_ents = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in gold.reference.ents]\n",
    "    print(\"Gold standard entities with positions and labels:\", gold_ents)\n",
    "\n",
    "    # Find matched entities\n",
    "    matched_entities = []\n",
    "    for pred in doc_pred.ents:\n",
    "        match_found = any(\n",
    "            pred.text == gold_ent.text and pred.label_ == gold_ent.label_ and pred.start_char == gold_ent.start_char and pred.end_char == gold_ent.end_char\n",
    "            for gold_ent in gold.reference.ents\n",
    "        )\n",
    "        if match_found:\n",
    "            matched_entities.append((pred.text, pred.start_char, pred.end_char, pred.label_))\n",
    "\n",
    "    print(\"Matched entities:\", matched_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the validation data to spaCy's Example format\n",
    "examples = []\n",
    "for text, annots in train_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annots)\n",
    "    examples.append(example)\n",
    "\n",
    "\n",
    "# Use the Scorer to score the examples\n",
    "scorer = Scorer(nlp)\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "\n",
    "precision = scores['ents_p']\n",
    "recall = scores['ents_r']\n",
    "f_score = scores['ents_f']\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data infrastructure data infrastructure\n",
      "data system data system\n",
      "data pipeline data pipeline\n",
      "data system data system\n",
      "ad hoc analysis ad hoc analysis\n",
      "database database\n",
      "data extraction data extraction\n",
      "computer science computer science\n",
      "data quality data quality\n",
      "data warehousing data warehousing\n",
      "data modeling data modeling\n",
      "analytics analytics\n",
      "database database\n",
      "postgresql postgresql\n",
      "sql sql\n",
      "data infrastructure data infrastructure\n",
      "apache airflow apache airflow\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(job_description)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'tag_acc': None,\n",
       " 'sents_p': None,\n",
       " 'sents_r': None,\n",
       " 'sents_f': None,\n",
       " 'dep_uas': None,\n",
       " 'dep_las': None,\n",
       " 'dep_las_per_type': None,\n",
       " 'pos_acc': None,\n",
       " 'morph_acc': None,\n",
       " 'morph_micro_p': None,\n",
       " 'morph_micro_r': None,\n",
       " 'morph_micro_f': None,\n",
       " 'morph_per_feat': None,\n",
       " 'lemma_acc': None,\n",
       " 'ents_p': 0.9941451990632318,\n",
       " 'ents_r': 0.991822429906542,\n",
       " 'ents_f': 0.9929824561403509,\n",
       " 'ents_per_type': {'analytics': {'p': 1.0,\n",
       "   'r': 0.9803921568627451,\n",
       "   'f': 0.99009900990099},\n",
       "  'data engineering': {'p': 1.0, 'r': 0.975, 'f': 0.9873417721518987},\n",
       "  'cloud technology': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data modeling': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'sql': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'c': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data visualization': {'p': 0.9375, 'r': 1.0, 'f': 0.967741935483871},\n",
       "  'data infrastructure': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'business requirement': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'predictive modeling': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'statistical analysis': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'problem solving': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data manipulation': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data hub': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data ingestion': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data collection': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data management': {'p': 0.8571428571428571,\n",
       "   'r': 1.0,\n",
       "   'f': 0.923076923076923},\n",
       "  'cloud application': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data integration': {'p': 0.8571428571428571,\n",
       "   'r': 1.0,\n",
       "   'f': 0.923076923076923},\n",
       "  'data governance': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data pipeline': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data strategy': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data quality': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'aws glue': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data processing': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'risk analysis': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'financial data management': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'computer science': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data structure': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database administration': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data architecture': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data mapping': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'application development': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'java': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'microsoft azure': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data migration': {'p': 0.8, 'r': 1.0, 'f': 0.888888888888889},\n",
       "  'mysql': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data streaming': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'nosql': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data warehousing': {'p': 1.0,\n",
       "   'r': 0.9166666666666666,\n",
       "   'f': 0.9565217391304348},\n",
       "  'project management': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'http': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data management platform': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'data retrieval': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'business intelligence': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database management system': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'apache spark': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'visualization': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database application': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'programming concept': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'big data': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'continuous integration': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data science': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'bokeh': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data center': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'databricks': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'power bi': {'p': 1.0, 'r': 0.9, 'f': 0.9473684210526316},\n",
       "  'data driven decision making': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'computer engineering': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data lake': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'airflow': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'devops': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database query': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data feed': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'algorithm': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data storage': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data mining': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database design': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data analysis': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'dashboard': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'business analysis': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data transformation': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'business intelligence tool': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'business analytics': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'agile methodology': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database schema': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'acceptance testing': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data redundancy': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'application planning': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'technology solution': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data system': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'code review': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'build process': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'distributed computing': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'agile software development': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'batch processing': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data extraction': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'big data analytics': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'object oriented programming language': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database system': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'unit testing': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'cloud database': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'pyspark': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'relational database management system': {'p': 0.6666666666666666,\n",
       "   'r': 0.6666666666666666,\n",
       "   'f': 0.6666666666666666},\n",
       "  'database management': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database architecture': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'azure data factory': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'ci cd': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'azure devops': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data integrity': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'ad hoc analysis': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'postgresql': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'apache airflow': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'application programming interface api': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'application design': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'application lifecycle management': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'test data': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'database tuning': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'parallel processing': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'pl sql': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'jupyter notebook': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'cloud development': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data mart': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data cleansing': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data acquisition': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'aws cloudformation': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'requirement analysis': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'artificial intelligence': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data validation': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data conversion': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'bigquery': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data exploitation': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'visual analytics': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data store': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'asp net': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'dataflow': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data wrangling': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'azure data lake': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'system lifecycle': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'customer analytics': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data capture': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'application deployment': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'cloud computing': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data profiling': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'data access': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'apache camel': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'dataset': {'p': 1.0, 'r': 1.0, 'f': 1.0},\n",
       "  'cloud infrastructure': {'p': 1.0, 'r': 1.0, 'f': 1.0}}}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scorer = Scorer(nlp)\n",
    "example = []\n",
    "for input_, annotations in train_data:\n",
    "    pred = nlp(input_)\n",
    "    #print(pred,annotations)\n",
    "    temp = Example.from_dict(pred, annotations)\n",
    "    example.append(temp)\n",
    "scores = scorer.score(example)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': None}"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.fromkeys(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [(404, 420, data engineering),\n",
       "  (444, 460, data acquisition),\n",
       "  (465, 472, dataset),\n",
       "  (771, 791, cloud infrastructure),\n",
       "  (931, 944, data pipeline),\n",
       "  (962, 982, business requirement),\n",
       "  (1514, 1529, data governance),\n",
       "  (1598, 1614, data engineering),\n",
       "  (1638, 1654, data engineering),\n",
       "  (1659, 1666, dataset),\n",
       "  (1706, 1710, java),\n",
       "  (1835, 1841, devops),\n",
       "  (1856, 1861, ci cd),\n",
       "  (1937, 1953, computer science)]}"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'department engineering location cary north carolina united state product epic online service company epic game requisition id r make epic core epic success talented passionate people epic pride creating collaborative welcoming creative environment building award winning game crafting engine technology enables visually stunning interactive experience innovating epic mean team continually strives right community user constantly innovating raise bar engine game development ecosec ecosec team provides safer experience epic user work multiple product service improve technology craft transparent policy player user positive experience platform responsible designing building maintaining data infrastructure ensure reliability efficiency data system ecosystem security team role include building maintaining data pipeline transform load data product managing aws infrastructure machine learning platform additionally work engineer product manager data scientist design implement robust scalable data service support ecosystem security mission ensuring user privacy work directly combat bad actor platform safe user role interact product team understand safety system interact data system design implement automated end end etl process prepare data machine learning ad hoc analysis including data anonymization manage scale tool technology use label data run aws devise database structure technology storing efficiently accessing large data set million record different type text image video etc use implement data extraction apis support data versioning strategy automated tool dvc support devising strategy labeling new data human looking strong analytical background bsc msc computer science software engineering related subject candidate degree welcome long proven extensive hand experience experience etl technical design automated data quality testing qa documentation data warehousing data modeling experience python interaction web service e g rest postman experience developing data apis experience aws snowflake comparable large scale analytics platform experience monitoring managing database use elasticsearch mongodb postgresql experience sql experience data versioning tool experience developing maintaining data infrastructure etl pipeline apache airflow role open multiple location north america europe excluding ny wa epic job epic benefit epic life intent cover thing medically necessary improve quality life pay premium dependent coverage includes medical dental vision hra long term disability life insurance amp k competitive match offer robust mental program modern health provides free therapy coaching employee amp dependent year celebrate employee event company wide paid break offer unlimited pto sick time recognize individual year employment paid sabbatical epic game span country studio employee globally year making award winning game engine technology empowers visually stunning game content bring environment life like epic award winning unreal engine technology provides game developer ability build high fidelity interactive experience pc console mobile vr tool embraced content creator variety industry medium entertainment automotive architectural design continue build engine technology develop remarkable game strive build team world class talent like hear come epic epic game deeply value diverse team inclusive work culture proud equal opportunity employer note recruitment agency epic accept unsolicited resume approach unauthorized party including recruitment placement agency e party negotiated validly executed agreement pay fee unauthorized party'"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-data pipeline', 'L-data pipeline', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-business requirement', 'L-business requirement', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-data processing', 'L-data processing', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['description', 'data', 'engineer', 'responsible', 'creation', 'ongoing', 'maintenance', 'internal', 'external', 'data', 'pipeline', 'service', 'includes', 'timely', 'resolution', 'data', 'issue', 'customer', 'ad', 'hoc', 'data', 'request', 'assigned', 'data', 'architect', 'leadership', 'team', 'collaborate', 'product', 'owner', 'operation', 'process', 'owner', 'identify', 'opportunity', 'define', 'business', 'requirement', 'assist', 'designing', 'implementing', 'solution', 'assist', 'reporting', 'critical', 'process', 'system', 'responsibility', 'life', 'cycle', 'project', 'analysis', 'development', 'delivery', 'solution', 'modify', 'existing', 'process', 'improve', 'operational', 'efficiency', 'entire', 'data', 'processing', 'team', 'perform', 'data', 'mining', 'aggregation', 'combining', 'multiple', 'datasets', 'develop', 'streamlined', 'efficient', 'solution', 'internal', 'external', 'party', 'perform', 'data', 'validation', 'analysis', 'ensure', 'accuracy', 'quality', 'data', 'handle', 'data', 'conversion', 'data', 'cleansing', 'report', 'generation', 'scheduled', 'data', 'delivery', 'standard', 'ability', 'deliver', 'data', 'feature', 'based', 'operation', 'leadership', 'requirement', 'requirement', 'required', 'experience', 'year', 'data', 'engineering', 'experience', 'relative', 'analytical', 'engineering', 'experience', 'year', 'sql', 'mssql', 'relative', 'sql', 'syntax', 'advanced', 'trigger', 'procedure', 'managed', 'statement', 'understanding', 'dimensional', 'programming', 'year', 'python', 'java', 'experience', 'experience', 'power', 'bi', 'google', 'data', 'studio', 'looker', 'tableau', 'experience', 'version', 'control', 'git', 'experience', 'automated', 'infrastructure', 'infrastructure', 'code', 'e', 'ansible', 'terraform', 'understanding', 'sdlc', 'secure', 'programming', 'skill', 'strong', 'understanding', 'cloud', 'platform', 'azure', 'amp', 'gcp', 'google', 'cloud', 'platform', 'experience', 'data', 'warehouse', 'environment', 'azure', 'synapse', 'google', 'bigquery', 'aws', 'redshift', 'snowflake', 'etc', 'education', 'bachelor', 'degree', 'related', 'field', 'equivalent', 'experience', 'required', 'preferred', 'experience', 'skill', 'ability', 'experience', 'machine', 'learning', 'infrastructure', 'modeling', 'experience', 'training', 'routine', 'creation', 'physical', 'technical', 'environment', 'noise', 'level', 'work', 'environment', 'moderate', 'ability', 'maintain', 'focus', 'high', 'level', 'pressure', 'multiple', 'priority'], 'SPACY': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable spacy.training.example.Example object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[289], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     examples\u001b[39m.\u001b[39mappend(example)\n\u001b[1;32m     14\u001b[0m scorer \u001b[39m=\u001b[39m Scorer()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m input_, annot \u001b[39min\u001b[39;00m examples:\n\u001b[1;32m     17\u001b[0m     doc_gold_text \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39mmake_doc(input_)\n\u001b[1;32m     18\u001b[0m     gold \u001b[39m=\u001b[39m GoldParse(doc_gold_text, entities\u001b[39m=\u001b[39mannot[\u001b[39m'\u001b[39m\u001b[39mentities\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable spacy.training.example.Example object"
     ]
    }
   ],
   "source": [
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "# Convert the test data to spaCy's Example format\n",
    "examples = []\n",
    "for text, annots in tes:\n",
    "    doc = nlp.make_doc(text)  # Create a Doc object from the text\n",
    "    example = Example.from_dict(doc, annots)  # Create an Example object from the Doc and annotations\n",
    "    examples.append(example)\n",
    "\n",
    "scorer = Scorer()\n",
    "\n",
    "for input_, annot in examples:\n",
    "    doc_gold_text = nlp.make_doc(input_)\n",
    "    gold = GoldParse(doc_gold_text, entities=annot['entities'])\n",
    "    pred_value = nlp(input_)\n",
    "    scorer.score(pred_value, gold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-data pipeline', 'L-data pipeline', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-business requirement', 'L-business requirement', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-data processing', 'L-data processing', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['description', 'data', 'engineer', 'responsible', 'creation', 'ongoing', 'maintenance', 'internal', 'external', 'data', 'pipeline', 'service', 'includes', 'timely', 'resolution', 'data', 'issue', 'customer', 'ad', 'hoc', 'data', 'request', 'assigned', 'data', 'architect', 'leadership', 'team', 'collaborate', 'product', 'owner', 'operation', 'process', 'owner', 'identify', 'opportunity', 'define', 'business', 'requirement', 'assist', 'designing', 'implementing', 'solution', 'assist', 'reporting', 'critical', 'process', 'system', 'responsibility', 'life', 'cycle', 'project', 'analysis', 'development', 'delivery', 'solution', 'modify', 'existing', 'process', 'improve', 'operational', 'efficiency', 'entire', 'data', 'processing', 'team', 'perform', 'data', 'mining', 'aggregation', 'combining', 'multiple', 'datasets', 'develop', 'streamlined', 'efficient', 'solution', 'internal', 'external', 'party', 'perform', 'data', 'validation', 'analysis', 'ensure', 'accuracy', 'quality', 'data', 'handle', 'data', 'conversion', 'data', 'cleansing', 'report', 'generation', 'scheduled', 'data', 'delivery', 'standard', 'ability', 'deliver', 'data', 'feature', 'based', 'operation', 'leadership', 'requirement', 'requirement', 'required', 'experience', 'year', 'data', 'engineering', 'experience', 'relative', 'analytical', 'engineering', 'experience', 'year', 'sql', 'mssql', 'relative', 'sql', 'syntax', 'advanced', 'trigger', 'procedure', 'managed', 'statement', 'understanding', 'dimensional', 'programming', 'year', 'python', 'java', 'experience', 'experience', 'power', 'bi', 'google', 'data', 'studio', 'looker', 'tableau', 'experience', 'version', 'control', 'git', 'experience', 'automated', 'infrastructure', 'infrastructure', 'code', 'e', 'ansible', 'terraform', 'understanding', 'sdlc', 'secure', 'programming', 'skill', 'strong', 'understanding', 'cloud', 'platform', 'azure', 'amp', 'gcp', 'google', 'cloud', 'platform', 'experience', 'data', 'warehouse', 'environment', 'azure', 'synapse', 'google', 'bigquery', 'aws', 'redshift', 'snowflake', 'etc', 'education', 'bachelor', 'degree', 'related', 'field', 'equivalent', 'experience', 'required', 'preferred', 'experience', 'skill', 'ability', 'experience', 'machine', 'learning', 'infrastructure', 'modeling', 'experience', 'training', 'routine', 'creation', 'physical', 'technical', 'environment', 'noise', 'level', 'work', 'environment', 'moderate', 'ability', 'maintain', 'focus', 'high', 'level', 'pressure', 'multiple', 'priority'], 'SPACY': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0,\n",
       " 'token_p': 1.0,\n",
       " 'token_r': 1.0,\n",
       " 'token_f': 1.0,\n",
       " 'sents_p': None,\n",
       " 'sents_r': None,\n",
       " 'sents_f': None,\n",
       " 'tag_acc': None,\n",
       " 'pos_acc': None,\n",
       " 'morph_acc': None,\n",
       " 'morph_micro_p': None,\n",
       " 'morph_micro_r': None,\n",
       " 'morph_micro_f': None,\n",
       " 'morph_per_feat': None,\n",
       " 'dep_uas': None,\n",
       " 'dep_las': None,\n",
       " 'dep_las_per_type': None,\n",
       " 'ents_p': 0.0,\n",
       " 'ents_r': 0.0,\n",
       " 'ents_f': 0.0,\n",
       " 'ents_per_type': {'business requirement': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'data pipeline': {'p': 0.0, 'r': 0.0, 'f': 0.0},\n",
       "  'data processing': {'p': 0.0, 'r': 0.0, 'f': 0.0}},\n",
       " 'cats_score': 0.0,\n",
       " 'cats_score_desc': 'macro F',\n",
       " 'cats_micro_p': 0.0,\n",
       " 'cats_micro_r': 0.0,\n",
       " 'cats_micro_f': 0.0,\n",
       " 'cats_macro_p': 0.0,\n",
       " 'cats_macro_r': 0.0,\n",
       " 'cats_macro_f': 0.0,\n",
       " 'cats_macro_auc': 0.0,\n",
       " 'cats_f_per_type': {},\n",
       " 'cats_auc_per_type': {}}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-data pipeline', 'L-data pipeline', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-business requirement', 'L-business requirement', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-data processing', 'L-data processing', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['description', 'data', 'engineer', 'responsible', 'creation', 'ongoing', 'maintenance', 'internal', 'external', 'data', 'pipeline', 'service', 'includes', 'timely', 'resolution', 'data', 'issue', 'customer', 'ad', 'hoc', 'data', 'request', 'assigned', 'data', 'architect', 'leadership', 'team', 'collaborate', 'product', 'owner', 'operation', 'process', 'owner', 'identify', 'opportunity', 'define', 'business', 'requirement', 'assist', 'designing', 'implementing', 'solution', 'assist', 'reporting', 'critical', 'process', 'system', 'responsibility', 'life', 'cycle', 'project', 'analysis', 'development', 'delivery', 'solution', 'modify', 'existing', 'process', 'improve', 'operational', 'efficiency', 'entire', 'data', 'processing', 'team', 'perform', 'data', 'mining', 'aggregation', 'combining', 'multiple', 'datasets', 'develop', 'streamlined', 'efficient', 'solution', 'internal', 'external', 'party', 'perform', 'data', 'validation', 'analysis', 'ensure', 'accuracy', 'quality', 'data', 'handle', 'data', 'conversion', 'data', 'cleansing', 'report', 'generation', 'scheduled', 'data', 'delivery', 'standard', 'ability', 'deliver', 'data', 'feature', 'based', 'operation', 'leadership', 'requirement', 'requirement', 'required', 'experience', 'year', 'data', 'engineering', 'experience', 'relative', 'analytical', 'engineering', 'experience', 'year', 'sql', 'mssql', 'relative', 'sql', 'syntax', 'advanced', 'trigger', 'procedure', 'managed', 'statement', 'understanding', 'dimensional', 'programming', 'year', 'python', 'java', 'experience', 'experience', 'power', 'bi', 'google', 'data', 'studio', 'looker', 'tableau', 'experience', 'version', 'control', 'git', 'experience', 'automated', 'infrastructure', 'infrastructure', 'code', 'e', 'ansible', 'terraform', 'understanding', 'sdlc', 'secure', 'programming', 'skill', 'strong', 'understanding', 'cloud', 'platform', 'azure', 'amp', 'gcp', 'google', 'cloud', 'platform', 'experience', 'data', 'warehouse', 'environment', 'azure', 'synapse', 'google', 'bigquery', 'aws', 'redshift', 'snowflake', 'etc', 'education', 'bachelor', 'degree', 'related', 'field', 'equivalent', 'experience', 'required', 'preferred', 'experience', 'skill', 'ability', 'experience', 'machine', 'learning', 'infrastructure', 'modeling', 'experience', 'training', 'routine', 'creation', 'physical', 'technical', 'environment', 'noise', 'level', 'work', 'environment', 'moderate', 'ability', 'maintain', 'focus', 'high', 'level', 'pressure', 'multiple', 'priority'], 'SPACY': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[276], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred_value\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_value' is not defined"
     ]
    }
   ],
   "source": [
    "pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m scorer\u001b[39m.\u001b[39mscores\n\u001b[1;32m     11\u001b[0m \u001b[39m# Prepare examples as a list of Example objects\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m examples \u001b[39m=\u001b[39m [Example\u001b[39m.\u001b[39mfrom_dict(nlp\u001b[39m.\u001b[39mmake_doc(text), annot) \u001b[39mfor\u001b[39;00m text, annot \u001b[39min\u001b[39;00m train_data[\u001b[39m0\u001b[39m]]\n\u001b[1;32m     14\u001b[0m \u001b[39m# Evaluate the model with the examples\u001b[39;00m\n\u001b[1;32m     15\u001b[0m scores \u001b[39m=\u001b[39m evaluate(nlp, examples)\n",
      "Cell \u001b[0;32mIn[245], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m scorer\u001b[39m.\u001b[39mscores\n\u001b[1;32m     11\u001b[0m \u001b[39m# Prepare examples as a list of Example objects\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m examples \u001b[39m=\u001b[39m [Example\u001b[39m.\u001b[39mfrom_dict(nlp\u001b[39m.\u001b[39mmake_doc(text), annot) \u001b[39mfor\u001b[39;00m text, annot \u001b[39min\u001b[39;00m train_data[\u001b[39m0\u001b[39m]]\n\u001b[1;32m     14\u001b[0m \u001b[39m# Evaluate the model with the examples\u001b[39;00m\n\u001b[1;32m     15\u001b[0m scores \u001b[39m=\u001b[39m evaluate(nlp, examples)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()  # Scorer does not take model as argument\n",
    "    for example in examples:\n",
    "        # Score the example\n",
    "        scorer.score(example)\n",
    "    return scorer.scores\n",
    "\n",
    "# Prepare examples as a list of Example objects\n",
    "examples = [Example.from_dict(nlp.make_doc(text), annot) for text, annot in train_data[0]]\n",
    "\n",
    "# Evaluate the model with the examples\n",
    "scores = evaluate(nlp, examples)\n",
    "\n",
    "# Print out the scores\n",
    "print(f\"Precision: {scores['ents_p']:.3f}\")\n",
    "print(f\"Recall: {scores['ents_r']:.3f}\")\n",
    "print(f\"F-score: {scores['ents_f']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('direct hire opportunity client hybrid role located minneapolis mn candidate able work sponsorship seeking highly motivated detail oriented data engineer join dynamic team ideal candidate responsible developing maintaining data solution drive business objective forward passion working data technical background desire contribute project leverage advanced analytics role excellent opportunity meaningful impact work collaboratively team create innovative data driven solution focus data engineering cloud technology data analytics product advanced analytics ready career level difference data driven world encourage apply responsibility data solution development design develop implement data solution address business need leveraging understanding data modeling sql proficiency coding skill language python c javascript continuous improvement identify seize opportunity enhance existing data solution optimizing performance scalability efficiency data visualization collaborate cross functional team create engaging data visualization provide actionable insight stakeholder data engineering cloud keen interest data engineering cloud technology actively participating related project support data infrastructure architecture data analytics product contribute development maintenance data analytics product ensuring meet highest quality standard satisfy business requirement advanced analytics work project related advanced analytics including predictive modeling machine learning statistical analysis help drive informed decision making collaborative teamwork act effective team member participating collaborative project sharing knowledge supporting colleague achieving project goal project documentation assist creation high quality project documentation project artifact ensuring aspect solution documented future reference audit qualification bachelor degree technical field equivalent practical experience ability understand implement solution business need effectively bridging gap technical knowledge business objective excellent written oral communication skill convey complex technical concept non technical stakeholder exceptional attention detail note taking skill precise problem solving documentation proficiency sql data manipulation analysis experience coding language like python c javascript data driven mindset strong analytical orientation demonstrated interest data visualization data engineering cloud technology data analytics product advanced analytics willingness contribute collaborative team member project fostering culture knowledge sharing innovation vetting process emergent software work hard find data engineer right fit client step vetting process position application minute online assessment minute initial phone interview minute interview client job offer job type time pay year benefit k k matching dental insurance health insurance paid time vision insurance compensation package bonus opportunity schedule hour shift monday friday work location person',\n",
       " {'entities': [(355, 364, analytics),\n",
       "   (481, 497, data engineering),\n",
       "   (498, 514, cloud technology),\n",
       "   (520, 529, analytics),\n",
       "   (547, 556, analytics),\n",
       "   (748, 761, data modeling),\n",
       "   (762, 765, sql),\n",
       "   (807, 808, c),\n",
       "   (947, 965, data visualization),\n",
       "   (1016, 1034, data visualization),\n",
       "   (1074, 1090, data engineering),\n",
       "   (1111, 1127, data engineering),\n",
       "   (1128, 1144, cloud technology),\n",
       "   (1192, 1211, data infrastructure),\n",
       "   (1230, 1239, analytics),\n",
       "   (1288, 1297, analytics),\n",
       "   (1353, 1373, business requirement),\n",
       "   (1383, 1392, analytics),\n",
       "   (1423, 1432, analytics),\n",
       "   (1443, 1462, predictive modeling),\n",
       "   (1480, 1500, statistical analysis),\n",
       "   (2184, 2199, problem solving),\n",
       "   (2226, 2229, sql),\n",
       "   (2230, 2247, data manipulation),\n",
       "   (2296, 2297, c),\n",
       "   (2381, 2399, data visualization),\n",
       "   (2400, 2416, data engineering),\n",
       "   (2417, 2433, cloud technology),\n",
       "   (2439, 2448, analytics),\n",
       "   (2466, 2475, analytics)]})"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('direct hire opportunity client hybrid role located minneapolis mn candidate able work sponsorship seeking highly motivated detail oriented data engineer join dynamic team ideal candidate responsible developing maintaining data solution drive business objective forward passion working data technical background desire contribute project leverage advanced analytics role excellent opportunity meaningful impact work collaboratively team create innovative data driven solution focus data engineering cloud technology data analytics product advanced analytics ready career level difference data driven world encourage apply responsibility data solution development design develop implement data solution address business need leveraging understanding data modeling sql proficiency coding skill language python c javascript continuous improvement identify seize opportunity enhance existing data solution optimizing performance scalability efficiency data visualization collaborate cross functional team create engaging data visualization provide actionable insight stakeholder data engineering cloud keen interest data engineering cloud technology actively participating related project support data infrastructure architecture data analytics product contribute development maintenance data analytics product ensuring meet highest quality standard satisfy business requirement advanced analytics work project related advanced analytics including predictive modeling machine learning statistical analysis help drive informed decision making collaborative teamwork act effective team member participating collaborative project sharing knowledge supporting colleague achieving project goal project documentation assist creation high quality project documentation project artifact ensuring aspect solution documented future reference audit qualification bachelor degree technical field equivalent practical experience ability understand implement solution business need effectively bridging gap technical knowledge business objective excellent written oral communication skill convey complex technical concept non technical stakeholder exceptional attention detail note taking skill precise problem solving documentation proficiency sql data manipulation analysis experience coding language like python c javascript data driven mindset strong analytical orientation demonstrated interest data visualization data engineering cloud technology data analytics product advanced analytics willingness contribute collaborative team member project fostering culture knowledge sharing innovation vetting process emergent software work hard find data engineer right fit client step vetting process position application minute online assessment minute initial phone interview minute interview client job offer job type time pay year benefit k k matching dental insurance health insurance paid time vision insurance compensation package bonus opportunity schedule hour shift monday friday work location person',\n",
       " {'entities': [(355, 364, analytics),\n",
       "   (481, 497, data engineering),\n",
       "   (498, 514, cloud technology),\n",
       "   (520, 529, analytics),\n",
       "   (547, 556, analytics),\n",
       "   (748, 761, data modeling),\n",
       "   (762, 765, sql),\n",
       "   (807, 808, c),\n",
       "   (947, 965, data visualization),\n",
       "   (1016, 1034, data visualization),\n",
       "   (1074, 1090, data engineering),\n",
       "   (1111, 1127, data engineering),\n",
       "   (1128, 1144, cloud technology),\n",
       "   (1192, 1211, data infrastructure),\n",
       "   (1230, 1239, analytics),\n",
       "   (1288, 1297, analytics),\n",
       "   (1353, 1373, business requirement),\n",
       "   (1383, 1392, analytics),\n",
       "   (1423, 1432, analytics),\n",
       "   (1443, 1462, predictive modeling),\n",
       "   (1480, 1500, statistical analysis),\n",
       "   (2184, 2199, problem solving),\n",
       "   (2226, 2229, sql),\n",
       "   (2230, 2247, data manipulation),\n",
       "   (2296, 2297, c),\n",
       "   (2381, 2399, data visualization),\n",
       "   (2400, 2416, data engineering),\n",
       "   (2417, 2433, cloud technology),\n",
       "   (2439, 2448, analytics),\n",
       "   (2466, 2475, analytics)]})"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visualization',\n",
       " 'b testing',\n",
       " 'ansi c',\n",
       " 'api gateway',\n",
       " 'api management',\n",
       " 'apl programming language',\n",
       " 'asp net',\n",
       " 'asp net core',\n",
       " 'asp net core mvc',\n",
       " 'asp net extension ajax',\n",
       " 'asp net fundamental',\n",
       " 'asp net identity',\n",
       " 'asp net mvc',\n",
       " 'asp net mvc framework',\n",
       " 'asp net razor',\n",
       " 'asp net web api',\n",
       " 'awk programming language',\n",
       " 'aws amplify',\n",
       " 'aws app mesh',\n",
       " 'aws appsync',\n",
       " 'aws auto scaling',\n",
       " 'aws backup',\n",
       " 'aws batch',\n",
       " 'aws cli command line interface',\n",
       " 'aws cloud development kit cdk',\n",
       " 'aws cloudformation',\n",
       " 'aws cloudhsm',\n",
       " 'aws cloudtrail',\n",
       " 'aws codebuild',\n",
       " 'aws codecommit',\n",
       " 'aws codedeploy',\n",
       " 'aws codepipeline',\n",
       " 'aws cost management',\n",
       " 'aws directory service',\n",
       " 'aws elastic beanstalk',\n",
       " 'aws elastic mapreduce emr',\n",
       " 'aws fargate',\n",
       " 'aws glue',\n",
       " 'aws identity access management iam',\n",
       " 'aws inferentia',\n",
       " 'aws internet thing iot',\n",
       " 'aws key management service km',\n",
       " 'aws kinesis',\n",
       " 'aws lambda',\n",
       " 'aws opsworks',\n",
       " 'aws outpost',\n",
       " 'aws sdk',\n",
       " 'aws sagemaker',\n",
       " 'aws serverless',\n",
       " 'aws user pool',\n",
       " 'abstract class',\n",
       " 'abstract data type',\n",
       " 'acceptance testing',\n",
       " 'activepython python package',\n",
       " 'ad hoc analysis',\n",
       " 'ad hoc marketing',\n",
       " 'ad hoc reporting',\n",
       " 'ad hoc testing',\n",
       " 'ada programming support environment apse',\n",
       " 'adaboost adaptive boosting',\n",
       " 'adhoc query',\n",
       " 'aggregation analysis',\n",
       " 'agile management',\n",
       " 'agile methodology',\n",
       " 'agile project management',\n",
       " 'agile project',\n",
       " 'agile software development',\n",
       " 'agile testing',\n",
       " 'airflow',\n",
       " 'ajax net',\n",
       " 'alation data catalog',\n",
       " 'algebra',\n",
       " 'algebraic modeling language',\n",
       " 'algorithm analysis',\n",
       " 'algorithm design',\n",
       " 'algorithmic trading',\n",
       " 'algorithm',\n",
       " 'alteryx',\n",
       " 'amazon api gateway',\n",
       " 'amazon alexa',\n",
       " 'amazon appstore',\n",
       " 'amazon appstream',\n",
       " 'amazon athena',\n",
       " 'amazon aurora',\n",
       " 'amazon cloud directory',\n",
       " 'amazon cloud drive',\n",
       " 'amazon cloudsearch',\n",
       " 'amazon cloudwatch',\n",
       " 'amazon cloudfront',\n",
       " 'amazon cognito',\n",
       " 'amazon comprehend',\n",
       " 'amazon connect',\n",
       " 'amazon data pipeline',\n",
       " 'amazon documentdb',\n",
       " 'amazon dynamodb',\n",
       " 'amazon elasticache',\n",
       " 'amazon elastic block store',\n",
       " 'amazon elastic compute cloud',\n",
       " 'amazon elastic container registry',\n",
       " 'amazon elastic container service',\n",
       " 'amazon elastic file system',\n",
       " 'amazon elastic kubernetes service',\n",
       " 'amazon elasticsearch service',\n",
       " 'amazon flexible payment service',\n",
       " 'amazon forecast',\n",
       " 'amazon guardduty',\n",
       " 'amazon inspector',\n",
       " 'amazon lex',\n",
       " 'amazon lightsail',\n",
       " 'amazon linux amazon machine image ami',\n",
       " 'amazon lumberyard',\n",
       " 'amazon mq',\n",
       " 'amazon macie',\n",
       " 'amazon managed blockchain',\n",
       " 'amazon managed streaming apache kafka amazon msk',\n",
       " 'amazon marketplace',\n",
       " 'amazon mechanical turk',\n",
       " 'amazon neptune',\n",
       " 'amazon personalize',\n",
       " 'amazon polly',\n",
       " 'amazon product advertising api',\n",
       " 'amazon quantum ledger database qldb',\n",
       " 'amazon quicksight',\n",
       " 'amazon redshift',\n",
       " 'amazon rekognition',\n",
       " 'amazon relational database service',\n",
       " 'amazon route',\n",
       " 'amazon',\n",
       " 'amazon bucket',\n",
       " 'amazon glacier',\n",
       " 'amazon simple email service s',\n",
       " 'amazon simple notification service sn',\n",
       " 'amazon simple queue service',\n",
       " 'amazon simple workflow service swf',\n",
       " 'amazon simpledb',\n",
       " 'amazon textract',\n",
       " 'amazon timestream',\n",
       " 'amazon transcribe',\n",
       " 'amazon translate',\n",
       " 'amazon virtual private cloud vpc',\n",
       " 'amazon web service',\n",
       " 'amazon workspace',\n",
       " 'analysis variance anova',\n",
       " 'analytic application',\n",
       " 'analytical dashboard',\n",
       " 'analytical technique',\n",
       " 'analytical testing',\n",
       " 'analytics',\n",
       " 'analytics j javascript library',\n",
       " 'android software development',\n",
       " 'android studio',\n",
       " 'android testing',\n",
       " 'android ui',\n",
       " 'angelscript',\n",
       " 'angular web framework',\n",
       " 'angular cli',\n",
       " 'angular component',\n",
       " 'angular material',\n",
       " 'angular reactive form',\n",
       " 'angular ui',\n",
       " 'ansi sql',\n",
       " 'apache',\n",
       " 'apache accumulo',\n",
       " 'apache activemq',\n",
       " 'apache administration',\n",
       " 'apache airflow',\n",
       " 'apache ambari',\n",
       " 'apache ant',\n",
       " 'apache apex',\n",
       " 'apache archiva',\n",
       " 'apache atlas',\n",
       " 'apache avro',\n",
       " 'apache axis',\n",
       " 'apache beam',\n",
       " 'apache beehive',\n",
       " 'apache cxf',\n",
       " 'apache camel',\n",
       " 'apache cassandra',\n",
       " 'apache cloudstack',\n",
       " 'apache common ognl',\n",
       " 'apache cordova',\n",
       " 'apache derby',\n",
       " 'apache directory',\n",
       " 'apache drill',\n",
       " 'apache druid',\n",
       " 'apache felix',\n",
       " 'apache flex',\n",
       " 'apache flink',\n",
       " 'apache flume',\n",
       " 'apache fop',\n",
       " 'apache giraph',\n",
       " 'apache hbase',\n",
       " 'apache http server',\n",
       " 'apache hadoop',\n",
       " 'apache hive',\n",
       " 'apache ibatis',\n",
       " 'apache iceberg',\n",
       " 'apache ignite',\n",
       " 'apache impala',\n",
       " 'apache jmeter',\n",
       " 'apache jserv protocol',\n",
       " 'apache jackrabbit',\n",
       " 'apache kafka',\n",
       " 'apache karaf',\n",
       " 'apache lucene',\n",
       " 'apache lucene net',\n",
       " 'apache madlib',\n",
       " 'apache mxnet',\n",
       " 'apache mahout',\n",
       " 'apache maven',\n",
       " 'apache mesos',\n",
       " 'apache module',\n",
       " 'apache myfaces',\n",
       " 'apache nifi',\n",
       " 'apache oozie',\n",
       " 'apache opennlp',\n",
       " 'apache openoffice',\n",
       " 'apache openjpa',\n",
       " 'apache openoffice calc',\n",
       " 'apache pdfbox',\n",
       " 'apache poi',\n",
       " 'apache parquet',\n",
       " 'apache phoenix',\n",
       " 'apache pig',\n",
       " 'apache pulsar',\n",
       " 'apache ranger',\n",
       " 'apache singa',\n",
       " 'apache samza',\n",
       " 'apache servicemix',\n",
       " 'apache shiro',\n",
       " 'apache sling',\n",
       " 'apache solr',\n",
       " 'apache spark',\n",
       " 'apache storm',\n",
       " 'apache strut',\n",
       " 'apache subversion',\n",
       " 'apache thrift',\n",
       " 'apache tika',\n",
       " 'apache tile',\n",
       " 'apache tinkerpop',\n",
       " 'apache tomee',\n",
       " 'apache tomcat',\n",
       " 'apache torque',\n",
       " 'apache traffic server',\n",
       " 'apache turbine',\n",
       " 'apache velocity',\n",
       " 'apache wicket',\n",
       " 'apache yarn',\n",
       " 'apache zeppelin',\n",
       " 'apollo graphql',\n",
       " 'application data',\n",
       " 'application deployment',\n",
       " 'application design',\n",
       " 'application development',\n",
       " 'application development language',\n",
       " 'application development system online adso',\n",
       " 'application environment',\n",
       " 'application firewall',\n",
       " 'application foundation class',\n",
       " 'application framework',\n",
       " 'application integration architecture',\n",
       " 'application interface framework',\n",
       " 'application kit',\n",
       " 'application layer',\n",
       " 'application level gateway',\n",
       " 'application level multicast infrastructure almi',\n",
       " 'application lifecycle management',\n",
       " 'application lifecycle management alm software',\n",
       " 'application monitoring',\n",
       " 'application note',\n",
       " 'application packaging',\n",
       " 'application performance management',\n",
       " 'application planning',\n",
       " 'application portfolio management',\n",
       " 'application programming interface api',\n",
       " 'application release automation',\n",
       " 'application remediation',\n",
       " 'application restart',\n",
       " 'application retirement',\n",
       " 'application security',\n",
       " 'application security testing',\n",
       " 'application server',\n",
       " 'application service',\n",
       " 'application setting',\n",
       " 'application specific instruction set processor',\n",
       " 'application specific integrated circuit',\n",
       " 'application streaming',\n",
       " 'application virtualization',\n",
       " 'application xml',\n",
       " 'application level gateway',\n",
       " 'application specific information',\n",
       " 'application architecture',\n",
       " 'application artificial intelligence',\n",
       " 'applied behavior analysis',\n",
       " 'applied business technology',\n",
       " 'applied science',\n",
       " 'applied statistic',\n",
       " 'aqua data studio',\n",
       " 'arangodb',\n",
       " 'architect engineer contract administration support system',\n",
       " 'architecture flow diagram',\n",
       " 'argo cd',\n",
       " 'artificial general intelligence',\n",
       " 'artificial intelligence',\n",
       " 'artificial intelligence development',\n",
       " 'artificial intelligence markup language aiml',\n",
       " 'artificial intelligence system',\n",
       " 'artificial neural network',\n",
       " 'assembly language',\n",
       " 'assessment basic language learning skill',\n",
       " 'association management',\n",
       " 'association rule learning',\n",
       " 'asynchronous javascript',\n",
       " 'asynchronous javascript xml ajax',\n",
       " 'asynchronous learning',\n",
       " 'asynchronous module definition',\n",
       " 'asynchronous serial communication',\n",
       " 'asynchronous transfer mode atm',\n",
       " 'attention mechanism',\n",
       " 'autocad',\n",
       " 'autocad architecture',\n",
       " 'autocad civil',\n",
       " 'autocad dxf',\n",
       " 'autocad plant',\n",
       " 'autoencoders',\n",
       " 'automated code review',\n",
       " 'automated machine learning',\n",
       " 'automatic data processing equipment',\n",
       " 'automatic data processing software adp',\n",
       " 'autoregressive integrated moving average arima',\n",
       " 'autoregressive model',\n",
       " 'average cost method',\n",
       " 'azure api apps',\n",
       " 'azure api management',\n",
       " 'azure active directory',\n",
       " 'azure application insight',\n",
       " 'azure automation',\n",
       " 'azure batch',\n",
       " 'azure blob storage',\n",
       " 'azure blueprint',\n",
       " 'azure cloud service',\n",
       " 'azure cognitive service',\n",
       " 'azure command line interface azure cli',\n",
       " 'azure content delivery network azure cdn',\n",
       " 'azure cosmos db',\n",
       " 'azure cost management',\n",
       " 'azure data catalog',\n",
       " 'azure data explorer kusto',\n",
       " 'azure data factory',\n",
       " 'azure data lake',\n",
       " 'azure databricks',\n",
       " 'azure devops',\n",
       " 'azure firewall',\n",
       " 'azure internet thing iot',\n",
       " 'azure kubernetes service',\n",
       " 'azure load balancer',\n",
       " 'azure logic apps',\n",
       " 'azure mfa',\n",
       " 'azure machine learning',\n",
       " 'azure monitor',\n",
       " 'azure pipeline',\n",
       " 'azure policy',\n",
       " 'azure security',\n",
       " 'azure sentinel',\n",
       " 'azure service bus',\n",
       " 'azure service fabric',\n",
       " 'azure web apps',\n",
       " 'b tree file system btrfs',\n",
       " 'bert nlp model',\n",
       " 'bgfs algorithm',\n",
       " 'end software engineering',\n",
       " 'backbone j javascript library',\n",
       " 'backpropagation',\n",
       " 'batch message processing',\n",
       " 'batch processing',\n",
       " 'batch production',\n",
       " 'bayes classifier',\n",
       " 'bayes estimator',\n",
       " 'bayesian inference',\n",
       " 'bayesian modeling',\n",
       " 'bayesian network',\n",
       " 'bayesian probability',\n",
       " 'bayesian statistic',\n",
       " 'beautifulsoup',\n",
       " 'behavioral analytics',\n",
       " 'behavioral segmentation',\n",
       " 'big data',\n",
       " 'big data analytics',\n",
       " 'big ip',\n",
       " 'big notation',\n",
       " 'bigmachines query language bmql',\n",
       " 'bigquery',\n",
       " 'bigtable',\n",
       " 'binary search algorithm',\n",
       " 'binary search tree',\n",
       " 'binary space partitioning',\n",
       " 'binary system',\n",
       " 'binary tree',\n",
       " 'bit ly',\n",
       " 'bitbucket',\n",
       " 'blockchain',\n",
       " 'blockchain indexing',\n",
       " 'blockchain security',\n",
       " 'bokeh',\n",
       " 'boltzmann machine',\n",
       " 'boolean expression',\n",
       " 'boolean network',\n",
       " 'boolean search',\n",
       " 'boost c library',\n",
       " 'bootstrap end framework',\n",
       " 'bootstrap protocol',\n",
       " 'bootstrapping',\n",
       " 'build automation',\n",
       " 'build event',\n",
       " 'build management',\n",
       " 'build pipeline',\n",
       " 'build process',\n",
       " 'build time',\n",
       " 'build tool',\n",
       " 'build v buy analysis',\n",
       " 'building information modeling',\n",
       " 'business analysis',\n",
       " 'business analysis body knowledge babok',\n",
       " 'business analytics',\n",
       " 'business architecture',\n",
       " 'business case analysis',\n",
       " 'business communication',\n",
       " 'business computer system',\n",
       " 'business development',\n",
       " 'business integration software',\n",
       " 'business intelligence',\n",
       " 'business intelligence architecture',\n",
       " 'business intelligence data modeling',\n",
       " 'business intelligence development',\n",
       " 'business intelligence development studio',\n",
       " 'business intelligence reporting',\n",
       " 'business intelligence testing',\n",
       " 'business intelligence tool',\n",
       " 'business performance management',\n",
       " 'business reporting',\n",
       " 'business requirement',\n",
       " 'business requirement documentation',\n",
       " 'c programming language',\n",
       " 'c compiler',\n",
       " 'c data type c programming language',\n",
       " 'c dynamic memory allocation',\n",
       " 'c file input output',\n",
       " 'c graphic',\n",
       " 'c mathematical function c standard library',\n",
       " 'c preprocessor',\n",
       " 'c sharp software',\n",
       " 'c sharp syntax',\n",
       " 'c shell',\n",
       " 'c standard library',\n",
       " 'c programming language',\n",
       " 'c fundamental',\n",
       " 'c programming language',\n",
       " 'c concept',\n",
       " 'c fundamental',\n",
       " 'c module',\n",
       " 'c server page',\n",
       " 'c cli',\n",
       " 'c',\n",
       " 'c based programming language',\n",
       " 'c treeace',\n",
       " 'c c standard library',\n",
       " 'c family',\n",
       " 'c j javascript library',\n",
       " 'cgi scripting',\n",
       " 'chi squared automatic interaction detection chaid',\n",
       " 'ci cd',\n",
       " 'cpython python package',\n",
       " 'cs animation',\n",
       " 'cs code',\n",
       " 'cs flex box layout',\n",
       " 'cs framework',\n",
       " 'cs grid',\n",
       " 'calculus',\n",
       " 'candidate key',\n",
       " 'canva software',\n",
       " 'cascading style sheet cs',\n",
       " 'chart j javascript library',\n",
       " 'chatgpt',\n",
       " 'chatbot',\n",
       " 'chi squared test',\n",
       " 'class diagram',\n",
       " 'class hierarchy',\n",
       " 'client server application language c al',\n",
       " 'cloud application',\n",
       " 'cloud automation',\n",
       " 'cloud collaboration',\n",
       " 'cloud computing',\n",
       " 'cloud computing architecture',\n",
       " 'cloud data management interface',\n",
       " 'cloud database',\n",
       " 'cloud development',\n",
       " 'cloud engineering',\n",
       " 'cloud infrastructure',\n",
       " 'cloud infrastructure management interface cimi',\n",
       " 'cloud management',\n",
       " 'cloud management platform',\n",
       " 'cloud migration',\n",
       " 'cloud operation',\n",
       " 'cloud penetration testing',\n",
       " 'cloud platform system',\n",
       " 'cloud storage',\n",
       " 'cloud strategy',\n",
       " 'cloud technology',\n",
       " 'cluster analysis',\n",
       " 'code editor',\n",
       " 'code enforcement',\n",
       " 'code formatting',\n",
       " 'code generation',\n",
       " 'code injection',\n",
       " 'code insight',\n",
       " 'code inspection',\n",
       " 'code migration',\n",
       " 'code federal regulation',\n",
       " 'code project open licensing',\n",
       " 'code refactoring',\n",
       " 'code reuse',\n",
       " 'code review',\n",
       " 'code sharing',\n",
       " 'code signing',\n",
       " 'code snippet',\n",
       " 'code structure',\n",
       " 'code testing',\n",
       " 'codebase',\n",
       " 'cognitive processing',\n",
       " 'collaborative filtering',\n",
       " 'command data handling',\n",
       " 'common gateway interface',\n",
       " 'compiler theory',\n",
       " 'compiler',\n",
       " 'computational design',\n",
       " 'computational intelligence',\n",
       " 'computational science engineering',\n",
       " 'computer data storage',\n",
       " 'computer design',\n",
       " 'computer display',\n",
       " 'computer engineering',\n",
       " 'computer programming',\n",
       " 'computer science',\n",
       " 'conditional expression',\n",
       " 'confusion matrix',\n",
       " 'continuous integration',\n",
       " 'conversational ai',\n",
       " 'convex optimization',\n",
       " 'convolutional neural network',\n",
       " 'couchdb',\n",
       " 'counterintelligence',\n",
       " 'course catalog',\n",
       " 'course development',\n",
       " 'create react app',\n",
       " 'cross functional integration',\n",
       " 'cross functional project management',\n",
       " 'cross functional team leadership',\n",
       " 'crypto mining',\n",
       " 'crypto',\n",
       " 'cryptocurrency',\n",
       " 'customer analysis',\n",
       " 'customer analytics',\n",
       " 'customer data integration',\n",
       " 'customer data management',\n",
       " 'cyber security management',\n",
       " 'cython',\n",
       " 'j javascript library',\n",
       " 'databus programming language',\n",
       " 'db sql',\n",
       " 'dfsm',\n",
       " 'dashboard',\n",
       " 'data abstraction',\n",
       " 'data access',\n",
       " 'data access object dao pattern',\n",
       " 'data acquisition',\n",
       " 'data administration',\n",
       " 'data analysis',\n",
       " 'data analysis display dadisp',\n",
       " 'data analysis expression dax',\n",
       " 'data annotation',\n",
       " 'data architecture',\n",
       " 'data archive',\n",
       " 'data archiving service',\n",
       " 'data service daas',\n",
       " 'data auditing',\n",
       " 'data base query language',\n",
       " 'data binding',\n",
       " 'data blending',\n",
       " 'data buffer',\n",
       " 'data build tool',\n",
       " 'data cabling',\n",
       " 'data caching',\n",
       " 'data capture',\n",
       " 'data center bridging',\n",
       " 'data center hardware',\n",
       " 'data center infrastructure efficiency',\n",
       " 'data center infrastructure management cim',\n",
       " 'data center unified computing system implementation dcuci',\n",
       " 'data center',\n",
       " 'data class',\n",
       " 'data classification',\n",
       " 'data cleansing',\n",
       " 'data collection',\n",
       " 'data comparison',\n",
       " 'data compression',\n",
       " 'data conditioning',\n",
       " 'data consistency',\n",
       " 'data control',\n",
       " 'data control language',\n",
       " 'data conversion',\n",
       " 'data corruption',\n",
       " 'data cube',\n",
       " 'data curation',\n",
       " 'data definition language',\n",
       " 'data definition specification',\n",
       " 'data dictionary',\n",
       " 'data direct network',\n",
       " 'data discovery',\n",
       " 'data display debugger',\n",
       " 'data distribution service',\n",
       " 'data domain',\n",
       " 'data driven instruction',\n",
       " 'data duplication management',\n",
       " 'data element',\n",
       " 'data encoding',\n",
       " 'data encryption',\n",
       " 'data encryption standard',\n",
       " 'data engineering',\n",
       " 'data engineering scripting language',\n",
       " 'data entry',\n",
       " 'data erasure',\n",
       " 'data ethic',\n",
       " 'data exchange',\n",
       " 'data exploitation',\n",
       " 'data explorer',\n",
       " 'data extraction',\n",
       " 'data facility data set service',\n",
       " 'data facility storage management',\n",
       " 'data farming',\n",
       " 'data feed',\n",
       " 'data file',\n",
       " 'data flow diagram',\n",
       " 'data format description language',\n",
       " 'data frame',\n",
       " 'data fusion',\n",
       " 'data general aviion computer',\n",
       " 'data governance',\n",
       " 'data grid',\n",
       " 'data hiding encapsulation',\n",
       " 'data highway plus',\n",
       " 'data hub',\n",
       " 'data import export',\n",
       " 'data infrastructure',\n",
       " 'data ingestion',\n",
       " 'data integration',\n",
       " 'data integrity',\n",
       " 'data intelligence',\n",
       " 'data interface',\n",
       " 'data item description',\n",
       " 'data lake',\n",
       " 'data language interface',\n",
       " 'data layer',\n",
       " 'data library',\n",
       " 'data link',\n",
       " 'data link connection identifier',\n",
       " 'data link control',\n",
       " 'data link layer',\n",
       " 'data literacy',\n",
       " 'data localization',\n",
       " 'data loss prevention',\n",
       " 'data maintenance',\n",
       " 'data management',\n",
       " 'data management plan',\n",
       " 'data management platform',\n",
       " 'data manipulation',\n",
       " 'data manipulation language',\n",
       " 'data mapper pattern',\n",
       " 'data mapping',\n",
       " 'data mart',\n",
       " 'data masking',\n",
       " 'data migration',\n",
       " 'data mining',\n",
       " 'data mining method',\n",
       " 'data mining query language dmql',\n",
       " 'data modeling',\n",
       " 'data monetization',\n",
       " 'data normalization',\n",
       " 'data ontap server appliance',\n",
       " 'data palette',\n",
       " 'data partitioning',\n",
       " 'data pipeline management',\n",
       " 'data pipeline',\n",
       " 'data plane development kit dpdk',\n",
       " 'data policy development',\n",
       " 'data preprocessing',\n",
       " 'data presentation',\n",
       " 'data privacy law',\n",
       " 'data processing',\n",
       " 'data processing system',\n",
       " 'data processing unit',\n",
       " 'data profiling',\n",
       " 'data protection planning',\n",
       " 'data protection strategy',\n",
       " 'data quality',\n",
       " 'data quality assessment',\n",
       " 'data radio channel',\n",
       " 'data recording',\n",
       " 'data recovery',\n",
       " 'data recovery software',\n",
       " 'data reduction',\n",
       " 'data redundancy',\n",
       " 'data reference model',\n",
       " 'data remanence',\n",
       " 'data retention',\n",
       " 'data retrieval',\n",
       " 'data room',\n",
       " 'data science',\n",
       " 'data scraping',\n",
       " 'data security',\n",
       " 'data selection',\n",
       " 'data server interface',\n",
       " 'data sharing',\n",
       " 'data smoothing',\n",
       " 'data storage',\n",
       " 'data storage device',\n",
       " 'data store',\n",
       " 'data storytelling',\n",
       " 'data strategy',\n",
       " 'data stream management system',\n",
       " 'data streaming',\n",
       " 'data striping',\n",
       " 'data structure alignment',\n",
       " 'data structure',\n",
       " 'data synchronization',\n",
       " 'data synthesis',\n",
       " 'data system',\n",
       " 'data targeting',\n",
       " 'data taxonomy',\n",
       " 'data terminal equipment',\n",
       " 'data transfer object',\n",
       " 'data transformation',\n",
       " 'data transformation service',\n",
       " 'data transmission',\n",
       " 'data transport utility',\n",
       " 'data transposition',\n",
       " 'data validation',\n",
       " 'data vault',\n",
       " 'data verification',\n",
       " 'data virtualization',\n",
       " 'data visualization',\n",
       " 'data warehouse appliance',\n",
       " 'data warehouse architecture',\n",
       " 'data warehouse system',\n",
       " 'data warehousing',\n",
       " 'data warehousing business intelligence dwbi',\n",
       " 'data wrangling',\n",
       " 'data centric testing',\n",
       " 'data driven decision making',\n",
       " 'data driven manufacturing',\n",
       " 'data driven testing',\n",
       " 'data flow analysis',\n",
       " 'data link switching',\n",
       " 'data structured language',\n",
       " 'dataadapters ado net',\n",
       " 'database markup language',\n",
       " 'databasic',\n",
       " 'datacad',\n",
       " 'dataflex',\n",
       " 'dataflux',\n",
       " 'datahub software',\n",
       " 'datanucleus',\n",
       " 'datastax enterprise',\n",
       " 'datastax enterprise graph',\n",
       " 'datatransfer workbench sap',\n",
       " 'databags',\n",
       " 'database abstraction layer',\n",
       " 'database activity monitoring',\n",
       " 'database administration',\n",
       " 'database analysis',\n",
       " 'database application',\n",
       " 'database architecture',\n",
       " 'database service dbaas',\n",
       " 'database audit',\n",
       " 'database availability group',\n",
       " 'database cloning',\n",
       " 'database cluster',\n",
       " 'database comparison',\n",
       " 'database connection',\n",
       " 'database consistency',\n",
       " 'database console command dbcc',\n",
       " 'database consolidation',\n",
       " 'database conversion',\n",
       " 'database cursor',\n",
       " 'database deployment management',\n",
       " 'database design',\n",
       " 'database development',\n",
       " 'database diagram',\n",
       " 'database directive',\n",
       " 'database dump',\n",
       " 'database encryption',\n",
       " 'database engine tuning advisor',\n",
       " 'database engine',\n",
       " 'database',\n",
       " 'database independent',\n",
       " 'database index',\n",
       " 'database life cycle management',\n",
       " 'database management',\n",
       " 'database management system',\n",
       " 'database marketing',\n",
       " 'database mirroring',\n",
       " 'database modeling',\n",
       " 'database normalization',\n",
       " 'database partitioning',\n",
       " 'database performance analyzer',\n",
       " 'database permission',\n",
       " 'database programmer toolkits',\n",
       " 'database programming',\n",
       " 'database publishing',\n",
       " 'database query',\n",
       " 'database query tool',\n",
       " 'database reporting software',\n",
       " 'database scanner',\n",
       " 'database schema',\n",
       " 'database search engine',\n",
       " 'database security',\n",
       " 'database server',\n",
       " 'database software',\n",
       " 'database storage structure',\n",
       " 'database system',\n",
       " 'database testing',\n",
       " 'database theory',\n",
       " 'database transaction',\n",
       " 'database trigger',\n",
       " 'database tuning',\n",
       " 'database upgrade',\n",
       " 'database virtualization',\n",
       " 'databricks',\n",
       " 'datacap',\n",
       " 'datacards',\n",
       " 'datacom db',\n",
       " 'datadog',\n",
       " 'datafeed',\n",
       " 'datafield',\n",
       " 'dataflow',\n",
       " 'dataflow architecture',\n",
       " 'dataframe',\n",
       " 'datagram',\n",
       " 'datagram congestion control protocol',\n",
       " 'datagram transport layer security',\n",
       " 'datakit',\n",
       " 'datalog',\n",
       " 'datamaps',\n",
       " 'datamodel',\n",
       " 'datanet',\n",
       " 'dataportability',\n",
       " 'datapump',\n",
       " 'dataset',\n",
       " 'datasheets',\n",
       " 'dataspaces',\n",
       " 'datastax',\n",
       " 'datatable',\n",
       " 'dataweave',\n",
       " 'datawindow',\n",
       " 'date manipulation',\n",
       " 'datediff',\n",
       " 'datomic',\n",
       " 'daughterboard',\n",
       " 'dbvisualizer',\n",
       " 'dbeaver',\n",
       " 'dbscan',\n",
       " 'dc j javascript library',\n",
       " 'decision model',\n",
       " 'decision science',\n",
       " 'decision support operation maintenance',\n",
       " 'decision support system',\n",
       " 'decision table',\n",
       " 'decision theory',\n",
       " 'decision tree learning',\n",
       " 'decision matrix method',\n",
       " 'deep learning',\n",
       " 'deep learning method',\n",
       " 'descriptive statistic',\n",
       " 'dev testing',\n",
       " 'dev c',\n",
       " 'devexpress',\n",
       " 'devops',\n",
       " 'differential calculus',\n",
       " 'digital data',\n",
       " 'digital data communication message protocol',\n",
       " 'digital data storage',\n",
       " 'digital data system',\n",
       " 'dimension table',\n",
       " 'dimensionality reduction',\n",
       " 'disk partitioning',\n",
       " 'distance learning',\n",
       " 'distributed computing',\n",
       " 'distributed database',\n",
       " 'distributed file system',\n",
       " 'distributed programming',\n",
       " 'docker engine',\n",
       " 'dot product',\n",
       " 'dropbox api',\n",
       " 'dust j javascript library',\n",
       " 'dynamic application security testing dast',\n",
       " 'dynamic data',\n",
       " 'dynamic program analysis',\n",
       " 'dynamic programming',\n",
       " 'ecmascript c programming language family',\n",
       " 'emc cloud computing',\n",
       " 'exec scripting language',\n",
       " 'economic policy analysis',\n",
       " 'edge computing',\n",
       " 'educational data mining',\n",
       " 'eigen c library',\n",
       " 'elasticity computing',\n",
       " 'electronic system level design verification',\n",
       " 'elixir programming language',\n",
       " 'eltron programming language',\n",
       " 'email processing',\n",
       " 'embedded c',\n",
       " 'embedded c',\n",
       " 'embedded code',\n",
       " 'embedded database',\n",
       " 'embedded domain specific language',\n",
       " 'embedded firmware',\n",
       " 'embedded http server',\n",
       " 'embedded intelligence',\n",
       " 'embedded java',\n",
       " 'embedded operating system',\n",
       " 'embedded sql',\n",
       " 'embedded software',\n",
       " 'embedded system',\n",
       " 'ember j javascript library',\n",
       " 'emulator high level language application program interface ehllapi',\n",
       " 'encrypted key exchange',\n",
       " 'encrypting file system',\n",
       " 'encryption',\n",
       " 'encryption software',\n",
       " 'environmental data analysis',\n",
       " 'environmental data management',\n",
       " 'enzyme javascript testing utility',\n",
       " 'error analysis numerical analysis',\n",
       " 'espresso android testing framework',\n",
       " 'espresso java',\n",
       " 'evolutionary programming',\n",
       " 'exception handling',\n",
       " 'expense forecasting',\n",
       " 'experience api xapi',\n",
       " 'express j javascript library',\n",
       " 'ext j',\n",
       " 'ext net',\n",
       " 'extendscript',\n",
       " 'extended file system',\n",
       " 'extract transform load etl',\n",
       " 'f programming language',\n",
       " 'foil programming language',\n",
       " 'facebook api',\n",
       " 'facebook analytics',\n",
       " 'facebook graph api',\n",
       " 'facebook query language',\n",
       " 'factset analytics software',\n",
       " 'famo javascript framework',\n",
       " 'file handling',\n",
       " 'file transfer protocol ftp',\n",
       " 'financial data management',\n",
       " 'financial forecasting',\n",
       " 'financial information exchange fix protocol',\n",
       " 'flask web framework',\n",
       " 'software business analysis',\n",
       " 'software release life cycle',\n",
       " 'software requirement analysis',\n",
       " 'software testing',\n",
       " 'solidity programming language',\n",
       " 'sorting algorithm',\n",
       " 'spacy nlp software',\n",
       " 'spatial analysis',\n",
       " 'spot analysis',\n",
       " 'sql manager',\n",
       " 'sql optimization',\n",
       " 'sql view',\n",
       " 'sql',\n",
       " 'sqlcommand',\n",
       " 'sqlxml',\n",
       " 'stack overflow',\n",
       " 'stakeholder analysis',\n",
       " 'standard generalized markup language',\n",
       " 'standard sql',\n",
       " 'static program analysis',\n",
       " 'statistical analysis',\n",
       " 'statistical coupling analysis',\n",
       " 'statistical method',\n",
       " 'statistical modeling',\n",
       " 'statistical programming',\n",
       " 'statistical static timing analysis ssta',\n",
       " 'statistical time division multiplexing',\n",
       " 'stochastic modeling',\n",
       " 'stochastic programming',\n",
       " 'storage virtualization',\n",
       " 'strategic analysis',\n",
       " 'structured analysis',\n",
       " ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'randstad technology data center site engineer located wilmington de relocation assistance available global financial leader exciting data center opportunity physical hand essential worker creative schedule enables extra day compared normal day week family free weekly covid testing opportunity growth advancement unique workload day thing day world class data center seeking experienced data center technician long term project role located wilmington de area basic responsibility include limited rack stack enterprise class technology network storage enterprise server running tracing dressing testing copper fiber cable patch panel device familiarity patch panel troubleshooting diagnostics hardware connectivity commission decommission hardware able perform component replacement dimms hard drive power supply adhere change control process required skill previous experience working complex high availability data center environment providing layer hardware installation ongoing maintenance problem identification incident resolution strong working knowledge data center process design hand expertise midrange system large scale server environment ibm hp oracle amp dell server hardware exceptional troubleshooting problem resolution skill ability document complex problem resolution summary repetitive task interface online ticketing system understanding switch port configuration critical thinking decision making related outage risk assessment strong cable management skill experience managing fiber amp copper data center environment desired skill certification server network technical writing documentation skill preferred proficient microsoft office role operates hour shift amp rotating schedule benefit condition waiting period apply duration long term long term opportunity growth professional development promote job type time pay hour benefit k dental insurance employee assistance program employee discount health insurance life insurance paid time referral program vision insurance schedule hour shift overtime weekend needed covid consideration regular free testing education high school equivalent preferred experience data center year preferred work location person'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'randstad technology data center site engineer located wilmington de relocation assistance available global financial leader exciting data center opportunity physical hand essential worker creative schedule enables extra day compared normal day week family free weekly covid testing opportunity growth advancement unique workload day thing day world class data center seeking experienced data center technician long term project role located wilmington de area basic responsibility include limited rack stack enterprise class technology network storage enterprise server running tracing dressing testing copper fiber cable patch panel device familiarity patch panel troubleshooting diagnostics hardware connectivity commission decommission hardware able perform component replacement dimms hard drive power supply adhere change control process required skill previous experience working complex high availability data center environment providing layer hardware installation ongoing maintenance problem identification incident resolution strong working knowledge data center process design hand expertise midrange system large scale server environment ibm hp oracle amp dell server hardware exceptional troubleshooting problem resolution skill ability document complex problem resolution summary repetitive task interface online ticketing system understanding switch port configuration critical thinking decision making related outage risk assessment strong cable management skill experience managing fiber amp copper data center environment desired skill certification server network technical writing documentation skill preferred proficient microsoft office role operates hour shift amp rotating schedule benefit condition waiting period apply duration long term long term opportunity growth professional development promote job type time pay hour benefit k dental insurance employee assistance program employee discount health insurance life insurance paid time referral program vision insurance schedule hour shift overtime weekend needed covid consideration regular free testing education high school equivalent preferred experience data center year preferred work location person'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>company</th>\n",
       "      <th>viewJobLink</th>\n",
       "      <th>companyBrandingAttributes</th>\n",
       "      <th>companyOverviewLink</th>\n",
       "      <th>companyRating</th>\n",
       "      <th>companyReviewCount</th>\n",
       "      <th>displayTitle</th>\n",
       "      <th>employerAssistEnabled</th>\n",
       "      <th>employerResponsive</th>\n",
       "      <th>expired</th>\n",
       "      <th>extractedSalary</th>\n",
       "      <th>featuredCompanyAttributes</th>\n",
       "      <th>featuredEmployer</th>\n",
       "      <th>featuredEmployerCandidate</th>\n",
       "      <th>feedId</th>\n",
       "      <th>formattedLocation</th>\n",
       "      <th>formattedRelativeTime</th>\n",
       "      <th>highVolumeHiringModel</th>\n",
       "      <th>hiringEventJob</th>\n",
       "      <th>indeedApplyEnabled</th>\n",
       "      <th>indeedApplyable</th>\n",
       "      <th>isJobVisited</th>\n",
       "      <th>isMobileThirdPartyApplyable</th>\n",
       "      <th>isNoResumeJob</th>\n",
       "      <th>isSubsidiaryJob</th>\n",
       "      <th>jobCardRequirementsModel</th>\n",
       "      <th>jobLocationCity</th>\n",
       "      <th>jobLocationState</th>\n",
       "      <th>jobTypes</th>\n",
       "      <th>locationCount</th>\n",
       "      <th>newJob</th>\n",
       "      <th>normTitle</th>\n",
       "      <th>openInterviewsInterviewsOnTheSpot</th>\n",
       "      <th>openInterviewsJob</th>\n",
       "      <th>openInterviewsOffersOnTheSpot</th>\n",
       "      <th>openInterviewsPhoneJob</th>\n",
       "      <th>overrideIndeedApplyText</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>rankingScoresModel</th>\n",
       "      <th>remoteLocation</th>\n",
       "      <th>resumeMatch</th>\n",
       "      <th>salarySnippet</th>\n",
       "      <th>screenerQuestionsURL</th>\n",
       "      <th>showAttainabilityBadge</th>\n",
       "      <th>showCommutePromo</th>\n",
       "      <th>showEarlyApply</th>\n",
       "      <th>showJobType</th>\n",
       "      <th>showRelativeDate</th>\n",
       "      <th>showSponsoredLabel</th>\n",
       "      <th>showStrongerAppliedLabel</th>\n",
       "      <th>smartFillEnabled</th>\n",
       "      <th>smbD2iEnabled</th>\n",
       "      <th>snippet</th>\n",
       "      <th>sponsored</th>\n",
       "      <th>taxoAttributes</th>\n",
       "      <th>taxoAttributesDisplayLimit</th>\n",
       "      <th>taxoLogAttributes</th>\n",
       "      <th>taxonomyAttributes</th>\n",
       "      <th>thirdPartyApplyUrl</th>\n",
       "      <th>title</th>\n",
       "      <th>translatedAttributes</th>\n",
       "      <th>translatedCmiJobTags</th>\n",
       "      <th>truncatedCompany</th>\n",
       "      <th>urgentlyHiring</th>\n",
       "      <th>vjFeaturedEmployerCandidate</th>\n",
       "      <th>jobDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Randstad Digital</td>\n",
       "      <td>/viewjob?jk=7a1a3b7d5ebc2fee&amp;from=vjs&amp;tk=1hec8...</td>\n",
       "      <td>{'headerImageUrl': 'https://d2q79iu7y748jz.clo...</td>\n",
       "      <td>https://www.indeed.com/cmp/Randstad?campaignid...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>18140.0</td>\n",
       "      <td>Data Center Engineer - DE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'max': 32, 'min': 26, 'type': 'hourly'}</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>50461</td>\n",
       "      <td>Bear, DE</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': -2, 'requireme...</td>\n",
       "      <td>Bear</td>\n",
       "      <td>DE</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1675749600000</td>\n",
       "      <td>{'bid': 16528, 'eApply': 0.008056486, 'eQualif...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': 'USD', 'salaryTextFormatted': Fal...</td>\n",
       "      <td>iq://de2bf8bcc5b264d786f9?v=1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Randstad Technologies Data Center Site Enginee...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [{'label': 'Employee discount'...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=7a1a3b7d5...</td>\n",
       "      <td>data center engineer de</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Randstad Digital</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Randstad Technologies\\nData Center Site Engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Emergent Software</td>\n",
       "      <td>/viewjob?jk=9e15b51f14153cff&amp;from=vjs&amp;tk=1hec8...</td>\n",
       "      <td>{'headerImageUrl': 'https://d2q79iu7y748jz.clo...</td>\n",
       "      <td>https://www.indeed.com/cmp/Emergent-Software?c...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'max': 125000, 'min': 100000, 'type': 'yearly'}</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>50461</td>\n",
       "      <td>Minneapolis-Saint Paul, MN</td>\n",
       "      <td>17 days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Minneapolis-Saint Paul</td>\n",
       "      <td>MN</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1697518800000</td>\n",
       "      <td>{'bid': 4000, 'eApply': 0.030367099, 'eQualifi...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': 'USD', 'salaryTextFormatted': Fal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>_* This is a direct-hire opportunity with our ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [{'label': 'Bonus opportunitie...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=9e15b51f1...</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Emergent Software</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>** This is a direct-hire opportunity with our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CGI Group, Inc.</td>\n",
       "      <td>/viewjob?jk=fabba5776ab608a2&amp;from=vjs&amp;tk=1hec8...</td>\n",
       "      <td>{'headerImageUrl': 'https://d2q79iu7y748jz.clo...</td>\n",
       "      <td>https://www.indeed.com/cmp/CGI?campaignid=mobv...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>Technical Lead - AWS Data Engineer (Python) - ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>111415</td>\n",
       "      <td>Columbia, SC 29228</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>SC</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Technical Lead</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1698814800000</td>\n",
       "      <td>{'bid': 7530, 'eApply': 0.002060637, 'eQualifi...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': 'USD', 'salaryTextFormatted': False}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Technical Lead - AWS Data Engineer (Python) - ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [], 'label': 'dynamic-attribut...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=fabba5776...</td>\n",
       "      <td>technical lead aws data engineer python remote</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>CGI Group, Inc.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Technical Lead - AWS Data Engineer (Python) - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Walmart Advanced Systems &amp; Robotics</td>\n",
       "      <td>/viewjob?jk=e06957ae0887a44e&amp;from=vjs&amp;tk=1hec8...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>66518</td>\n",
       "      <td>Andover, MA</td>\n",
       "      <td>11 days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Andover</td>\n",
       "      <td>MA</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1698037200000</td>\n",
       "      <td>{'bid': 0, 'eApply': 0.03567315, 'eQualified': 0}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': '', 'salaryTextFormatted': False}</td>\n",
       "      <td>https://api.greenhouse.io/v1/boards/alertinnov...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Walmart Advanced Systems &amp;amp; Robotics is a f...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [], 'label': 'dynamic-attribut...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=e06957ae0...</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Walmart Advanced Systems &amp; Robotics</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Walmart Advanced Systems &amp;amp; Robotics is a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Techstra Solutions</td>\n",
       "      <td>/viewjob?jk=0dc1e39dddd71240&amp;from=vjs&amp;tk=1hec8...</td>\n",
       "      <td>{}</td>\n",
       "      <td>https://www.indeed.com/cmp/Techstra-Solutions?...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Azure Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11837</td>\n",
       "      <td>Pittsburgh, PA 15217</td>\n",
       "      <td>Today</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>PA</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1698987600000</td>\n",
       "      <td>{'bid': 0, 'eApply': 0.03509473, 'eQualified': 0}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': '', 'salaryTextFormatted': False}</td>\n",
       "      <td>https://techstrasolutions.applytojob.com/apply...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>As an Azure Data Engineer at Techstra Solution...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [], 'label': 'dynamic-attribut...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=0dc1e39dd...</td>\n",
       "      <td>azure data engineer</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Techstra Solutions</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>As an Azure Data Engineer at Techstra Solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>FlexIT Inc</td>\n",
       "      <td>/viewjob?jk=c6c58603666b2758&amp;from=vjs&amp;tk=1heca...</td>\n",
       "      <td>{'logoUrl': 'https://d2q79iu7y748jz.cloudfront...</td>\n",
       "      <td>https://www.indeed.com/cmp/Flexit-Inc?campaign...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6089</td>\n",
       "      <td>Beaverton, OR 97005</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Beaverton</td>\n",
       "      <td>OR</td>\n",
       "      <td>[]</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1601874000000</td>\n",
       "      <td>{'bid': 0, 'eApply': 0.016640559, 'eQualified'...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': '', 'salaryTextFormatted': False}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Develop and support data solutions in support ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [], 'label': 'dynamic-attribut...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=c6c586036...</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>FlexIT Inc</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Job Duties: \\n \\n  Develop and support data so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>OneSource Regulatory</td>\n",
       "      <td>/viewjob?jk=86c55092db031420&amp;from=vjs&amp;tk=1heca...</td>\n",
       "      <td>{}</td>\n",
       "      <td>https://www.indeed.com/cmp/Onesource-Regulator...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>75791</td>\n",
       "      <td>Boston, MA 02101</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1656392400000</td>\n",
       "      <td>{'bid': 0, 'eApply': 0.01608023, 'eQualified': 0}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': '', 'salaryTextFormatted': False}</td>\n",
       "      <td>https://onesourceregulatory.bamboohr.com/jobs/...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>We are looking for a data engineer to pull dat...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [], 'label': 'dynamic-attribut...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=86c55092d...</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>OneSource Regulatory</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Company Introduction \\n  OneSource Regulatory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>Square Peg Technologies</td>\n",
       "      <td>/viewjob?jk=c03398d7062ca8ff&amp;from=vjs&amp;tk=1heca...</td>\n",
       "      <td>{'logoUrl': 'https://d2q79iu7y748jz.cloudfront...</td>\n",
       "      <td>https://www.indeed.com/cmp/Square-Peg-Technolo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>111935</td>\n",
       "      <td>Washington, DC 20003</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1664946000000</td>\n",
       "      <td>{'bid': 0, 'eApply': 0.015102735, 'eQualified'...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': '', 'salaryTextFormatted': False}</td>\n",
       "      <td>https://workforcenow.adp.com/mascsr/default/ca...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Square Peg Technologies is a boutique technolo...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [{'label': 'Paid training', 's...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=c03398d70...</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Square Peg Technologies</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Full Job Description\\n     Position Requires a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>NS2 Mission</td>\n",
       "      <td>/viewjob?jk=1567c729f9132e96&amp;from=vjs&amp;tk=1heca...</td>\n",
       "      <td>{}</td>\n",
       "      <td>https://www.indeed.com/cmp/Ns2-Mission?campaig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Engineer (ETL)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>111935</td>\n",
       "      <td>Reston, VA 20191</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Reston</td>\n",
       "      <td>VA</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1678856400000</td>\n",
       "      <td>{'bid': 0, 'eApply': 0.014515, 'eQualified': 0}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': '', 'salaryTextFormatted': False}</td>\n",
       "      <td>https://workforcenow.adp.com/mascsr/default/ca...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Components to include user interface, data flo...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [], 'label': 'dynamic-attribut...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=1567c729f...</td>\n",
       "      <td>data engineer etl</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NS2 Mission</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Are you interested in working in a dynamic env...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>Big Bright International</td>\n",
       "      <td>/viewjob?jk=3ff49794581b1faa&amp;from=vjs&amp;tk=1heca...</td>\n",
       "      <td>{}</td>\n",
       "      <td>https://www.indeed.com/cmp/Big-Bright-Internat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DATA ENGINEER</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>563807</td>\n",
       "      <td>Irvine, CA</td>\n",
       "      <td>30+ days ago</td>\n",
       "      <td>{'highVolumeHiring': False}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'additionalRequirementsCount': 0, 'requiremen...</td>\n",
       "      <td>Irvine</td>\n",
       "      <td>CA</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1565672400000</td>\n",
       "      <td>{'bid': 0, 'eApply': 0.014364341, 'eQualified'...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'currency': '', 'salaryTextFormatted': False}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer at various unanticipated client ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'attributes': [], 'label': 'dynamic-attribut...</td>\n",
       "      <td>http://www.indeed.com//applystart?jk=3ff497945...</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Big Bright International</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer at various unanticipated client ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                              company  \\\n",
       "0             0                     Randstad Digital   \n",
       "1             1                    Emergent Software   \n",
       "2             2                      CGI Group, Inc.   \n",
       "3             3  Walmart Advanced Systems & Robotics   \n",
       "4             4                   Techstra Solutions   \n",
       "..          ...                                  ...   \n",
       "995         995                           FlexIT Inc   \n",
       "996         996                 OneSource Regulatory   \n",
       "997         997              Square Peg Technologies   \n",
       "998         998                          NS2 Mission   \n",
       "999         999             Big Bright International   \n",
       "\n",
       "                                           viewJobLink  \\\n",
       "0    /viewjob?jk=7a1a3b7d5ebc2fee&from=vjs&tk=1hec8...   \n",
       "1    /viewjob?jk=9e15b51f14153cff&from=vjs&tk=1hec8...   \n",
       "2    /viewjob?jk=fabba5776ab608a2&from=vjs&tk=1hec8...   \n",
       "3    /viewjob?jk=e06957ae0887a44e&from=vjs&tk=1hec8...   \n",
       "4    /viewjob?jk=0dc1e39dddd71240&from=vjs&tk=1hec8...   \n",
       "..                                                 ...   \n",
       "995  /viewjob?jk=c6c58603666b2758&from=vjs&tk=1heca...   \n",
       "996  /viewjob?jk=86c55092db031420&from=vjs&tk=1heca...   \n",
       "997  /viewjob?jk=c03398d7062ca8ff&from=vjs&tk=1heca...   \n",
       "998  /viewjob?jk=1567c729f9132e96&from=vjs&tk=1heca...   \n",
       "999  /viewjob?jk=3ff49794581b1faa&from=vjs&tk=1heca...   \n",
       "\n",
       "                             companyBrandingAttributes  \\\n",
       "0    {'headerImageUrl': 'https://d2q79iu7y748jz.clo...   \n",
       "1    {'headerImageUrl': 'https://d2q79iu7y748jz.clo...   \n",
       "2    {'headerImageUrl': 'https://d2q79iu7y748jz.clo...   \n",
       "3                                                  NaN   \n",
       "4                                                   {}   \n",
       "..                                                 ...   \n",
       "995  {'logoUrl': 'https://d2q79iu7y748jz.cloudfront...   \n",
       "996                                                 {}   \n",
       "997  {'logoUrl': 'https://d2q79iu7y748jz.cloudfront...   \n",
       "998                                                 {}   \n",
       "999                                                 {}   \n",
       "\n",
       "                                   companyOverviewLink  companyRating  \\\n",
       "0    https://www.indeed.com/cmp/Randstad?campaignid...            3.7   \n",
       "1    https://www.indeed.com/cmp/Emergent-Software?c...            4.8   \n",
       "2    https://www.indeed.com/cmp/CGI?campaignid=mobv...            3.6   \n",
       "3                                                  NaN            NaN   \n",
       "4    https://www.indeed.com/cmp/Techstra-Solutions?...            NaN   \n",
       "..                                                 ...            ...   \n",
       "995  https://www.indeed.com/cmp/Flexit-Inc?campaign...            NaN   \n",
       "996  https://www.indeed.com/cmp/Onesource-Regulator...            NaN   \n",
       "997  https://www.indeed.com/cmp/Square-Peg-Technolo...            NaN   \n",
       "998  https://www.indeed.com/cmp/Ns2-Mission?campaig...            NaN   \n",
       "999  https://www.indeed.com/cmp/Big-Bright-Internat...            NaN   \n",
       "\n",
       "     companyReviewCount                                       displayTitle  \\\n",
       "0               18140.0                          Data Center Engineer - DE   \n",
       "1                  15.0                                      Data Engineer   \n",
       "2                3570.0  Technical Lead - AWS Data Engineer (Python) - ...   \n",
       "3                   NaN                                      Data Engineer   \n",
       "4                   NaN                                Azure Data Engineer   \n",
       "..                  ...                                                ...   \n",
       "995                 NaN                                      Data Engineer   \n",
       "996                 NaN                                      Data Engineer   \n",
       "997                 NaN                                      Data Engineer   \n",
       "998                 NaN                                Data Engineer (ETL)   \n",
       "999                 NaN                                      DATA ENGINEER   \n",
       "\n",
       "     employerAssistEnabled  employerResponsive  expired  \\\n",
       "0                    False               False    False   \n",
       "1                    False               False    False   \n",
       "2                    False               False    False   \n",
       "3                    False               False    False   \n",
       "4                    False               False    False   \n",
       "..                     ...                 ...      ...   \n",
       "995                  False               False    False   \n",
       "996                  False               False    False   \n",
       "997                  False               False    False   \n",
       "998                  False               False    False   \n",
       "999                  False               False    False   \n",
       "\n",
       "                                      extractedSalary  \\\n",
       "0            {'max': 32, 'min': 26, 'type': 'hourly'}   \n",
       "1    {'max': 125000, 'min': 100000, 'type': 'yearly'}   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "..                                                ...   \n",
       "995                                               NaN   \n",
       "996                                               NaN   \n",
       "997                                               NaN   \n",
       "998                                               NaN   \n",
       "999                                               NaN   \n",
       "\n",
       "    featuredCompanyAttributes  featuredEmployer  featuredEmployerCandidate  \\\n",
       "0                          {}             False                      False   \n",
       "1                          {}             False                      False   \n",
       "2                          {}             False                      False   \n",
       "3                          {}             False                      False   \n",
       "4                          {}             False                      False   \n",
       "..                        ...               ...                        ...   \n",
       "995                        {}             False                      False   \n",
       "996                        {}             False                      False   \n",
       "997                        {}             False                      False   \n",
       "998                        {}             False                      False   \n",
       "999                        {}             False                      False   \n",
       "\n",
       "     feedId           formattedLocation formattedRelativeTime  \\\n",
       "0     50461                    Bear, DE          30+ days ago   \n",
       "1     50461  Minneapolis-Saint Paul, MN           17 days ago   \n",
       "2    111415          Columbia, SC 29228            2 days ago   \n",
       "3     66518                 Andover, MA           11 days ago   \n",
       "4     11837        Pittsburgh, PA 15217                 Today   \n",
       "..      ...                         ...                   ...   \n",
       "995    6089         Beaverton, OR 97005          30+ days ago   \n",
       "996   75791            Boston, MA 02101          30+ days ago   \n",
       "997  111935        Washington, DC 20003          30+ days ago   \n",
       "998  111935            Reston, VA 20191          30+ days ago   \n",
       "999  563807                  Irvine, CA          30+ days ago   \n",
       "\n",
       "           highVolumeHiringModel  hiringEventJob  indeedApplyEnabled  \\\n",
       "0    {'highVolumeHiring': False}           False                True   \n",
       "1    {'highVolumeHiring': False}           False               False   \n",
       "2    {'highVolumeHiring': False}           False               False   \n",
       "3    {'highVolumeHiring': False}           False                True   \n",
       "4    {'highVolumeHiring': False}           False                True   \n",
       "..                           ...             ...                 ...   \n",
       "995  {'highVolumeHiring': False}           False                True   \n",
       "996  {'highVolumeHiring': False}           False                True   \n",
       "997  {'highVolumeHiring': False}           False                True   \n",
       "998  {'highVolumeHiring': False}           False                True   \n",
       "999  {'highVolumeHiring': False}           False                True   \n",
       "\n",
       "     indeedApplyable  isJobVisited  isMobileThirdPartyApplyable  \\\n",
       "0               True         False                        False   \n",
       "1              False         False                        False   \n",
       "2              False         False                         True   \n",
       "3               True         False                        False   \n",
       "4               True         False                        False   \n",
       "..               ...           ...                          ...   \n",
       "995             True         False                        False   \n",
       "996             True         False                        False   \n",
       "997             True         False                        False   \n",
       "998             True         False                        False   \n",
       "999             True         False                        False   \n",
       "\n",
       "     isNoResumeJob  isSubsidiaryJob  \\\n",
       "0            False            False   \n",
       "1            False            False   \n",
       "2            False            False   \n",
       "3            False            False   \n",
       "4            False            False   \n",
       "..             ...              ...   \n",
       "995          False            False   \n",
       "996          False            False   \n",
       "997          False            False   \n",
       "998          False            False   \n",
       "999          False            False   \n",
       "\n",
       "                              jobCardRequirementsModel  \\\n",
       "0    {'additionalRequirementsCount': -2, 'requireme...   \n",
       "1    {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "2    {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "3    {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "4    {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "..                                                 ...   \n",
       "995  {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "996  {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "997  {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "998  {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "999  {'additionalRequirementsCount': 0, 'requiremen...   \n",
       "\n",
       "            jobLocationCity jobLocationState jobTypes  locationCount  newJob  \\\n",
       "0                      Bear               DE       []              1   False   \n",
       "1    Minneapolis-Saint Paul               MN       []              2   False   \n",
       "2                  Columbia               SC       []              1    True   \n",
       "3                   Andover               MA       []              1   False   \n",
       "4                Pittsburgh               PA       []              3    True   \n",
       "..                      ...              ...      ...            ...     ...   \n",
       "995               Beaverton               OR       []             11   False   \n",
       "996                  Boston               MA       []              7   False   \n",
       "997              Washington               DC       []              2   False   \n",
       "998                  Reston               VA       []              1   False   \n",
       "999                  Irvine               CA       []              1   False   \n",
       "\n",
       "          normTitle  openInterviewsInterviewsOnTheSpot  openInterviewsJob  \\\n",
       "0     Data Engineer                              False              False   \n",
       "1     Data Engineer                              False              False   \n",
       "2    Technical Lead                              False              False   \n",
       "3     Data Engineer                              False              False   \n",
       "4     Data Engineer                              False              False   \n",
       "..              ...                                ...                ...   \n",
       "995   Data Engineer                              False              False   \n",
       "996   Data Engineer                              False              False   \n",
       "997   Data Engineer                              False              False   \n",
       "998   Data Engineer                              False              False   \n",
       "999   Data Engineer                              False              False   \n",
       "\n",
       "     openInterviewsOffersOnTheSpot  openInterviewsPhoneJob  \\\n",
       "0                            False                   False   \n",
       "1                            False                   False   \n",
       "2                            False                   False   \n",
       "3                            False                   False   \n",
       "4                            False                   False   \n",
       "..                             ...                     ...   \n",
       "995                          False                   False   \n",
       "996                          False                   False   \n",
       "997                          False                   False   \n",
       "998                          False                   False   \n",
       "999                          False                   False   \n",
       "\n",
       "     overrideIndeedApplyText        pubDate  \\\n",
       "0                       True  1675749600000   \n",
       "1                       True  1697518800000   \n",
       "2                       True  1698814800000   \n",
       "3                       True  1698037200000   \n",
       "4                       True  1698987600000   \n",
       "..                       ...            ...   \n",
       "995                     True  1601874000000   \n",
       "996                     True  1656392400000   \n",
       "997                     True  1664946000000   \n",
       "998                     True  1678856400000   \n",
       "999                     True  1565672400000   \n",
       "\n",
       "                                    rankingScoresModel  remoteLocation  \\\n",
       "0    {'bid': 16528, 'eApply': 0.008056486, 'eQualif...           False   \n",
       "1    {'bid': 4000, 'eApply': 0.030367099, 'eQualifi...           False   \n",
       "2    {'bid': 7530, 'eApply': 0.002060637, 'eQualifi...           False   \n",
       "3    {'bid': 0, 'eApply': 0.03567315, 'eQualified': 0}           False   \n",
       "4    {'bid': 0, 'eApply': 0.03509473, 'eQualified': 0}           False   \n",
       "..                                                 ...             ...   \n",
       "995  {'bid': 0, 'eApply': 0.016640559, 'eQualified'...           False   \n",
       "996  {'bid': 0, 'eApply': 0.01608023, 'eQualified': 0}           False   \n",
       "997  {'bid': 0, 'eApply': 0.015102735, 'eQualified'...           False   \n",
       "998    {'bid': 0, 'eApply': 0.014515, 'eQualified': 0}           False   \n",
       "999  {'bid': 0, 'eApply': 0.014364341, 'eQualified'...           False   \n",
       "\n",
       "     resumeMatch                                      salarySnippet  \\\n",
       "0          False  {'currency': 'USD', 'salaryTextFormatted': Fal...   \n",
       "1          False  {'currency': 'USD', 'salaryTextFormatted': Fal...   \n",
       "2          False  {'currency': 'USD', 'salaryTextFormatted': False}   \n",
       "3          False     {'currency': '', 'salaryTextFormatted': False}   \n",
       "4          False     {'currency': '', 'salaryTextFormatted': False}   \n",
       "..           ...                                                ...   \n",
       "995        False     {'currency': '', 'salaryTextFormatted': False}   \n",
       "996        False     {'currency': '', 'salaryTextFormatted': False}   \n",
       "997        False     {'currency': '', 'salaryTextFormatted': False}   \n",
       "998        False     {'currency': '', 'salaryTextFormatted': False}   \n",
       "999        False     {'currency': '', 'salaryTextFormatted': False}   \n",
       "\n",
       "                                  screenerQuestionsURL  \\\n",
       "0                        iq://de2bf8bcc5b264d786f9?v=1   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3    https://api.greenhouse.io/v1/boards/alertinnov...   \n",
       "4    https://techstrasolutions.applytojob.com/apply...   \n",
       "..                                                 ...   \n",
       "995                                                NaN   \n",
       "996  https://onesourceregulatory.bamboohr.com/jobs/...   \n",
       "997  https://workforcenow.adp.com/mascsr/default/ca...   \n",
       "998  https://workforcenow.adp.com/mascsr/default/ca...   \n",
       "999                                                NaN   \n",
       "\n",
       "     showAttainabilityBadge  showCommutePromo  showEarlyApply  showJobType  \\\n",
       "0                     False             False           False        False   \n",
       "1                     False             False           False        False   \n",
       "2                     False             False           False        False   \n",
       "3                     False             False           False        False   \n",
       "4                     False             False           False        False   \n",
       "..                      ...               ...             ...          ...   \n",
       "995                   False             False           False        False   \n",
       "996                   False             False           False        False   \n",
       "997                   False             False           False        False   \n",
       "998                   False             False           False        False   \n",
       "999                   False             False           False        False   \n",
       "\n",
       "     showRelativeDate  showSponsoredLabel  showStrongerAppliedLabel  \\\n",
       "0                True               False                     False   \n",
       "1                True               False                     False   \n",
       "2                True               False                     False   \n",
       "3                True               False                     False   \n",
       "4                True               False                     False   \n",
       "..                ...                 ...                       ...   \n",
       "995              True               False                     False   \n",
       "996              True               False                     False   \n",
       "997              True               False                     False   \n",
       "998              True               False                     False   \n",
       "999              True               False                     False   \n",
       "\n",
       "     smartFillEnabled  smbD2iEnabled  \\\n",
       "0               False          False   \n",
       "1               False          False   \n",
       "2               False          False   \n",
       "3               False          False   \n",
       "4               False          False   \n",
       "..                ...            ...   \n",
       "995             False          False   \n",
       "996             False          False   \n",
       "997             False          False   \n",
       "998             False          False   \n",
       "999             False          False   \n",
       "\n",
       "                                               snippet  sponsored  \\\n",
       "0    Randstad Technologies Data Center Site Enginee...       True   \n",
       "1    _* This is a direct-hire opportunity with our ...       True   \n",
       "2    Technical Lead - AWS Data Engineer (Python) - ...       True   \n",
       "3    Walmart Advanced Systems &amp; Robotics is a f...      False   \n",
       "4    As an Azure Data Engineer at Techstra Solution...      False   \n",
       "..                                                 ...        ...   \n",
       "995  Develop and support data solutions in support ...      False   \n",
       "996  We are looking for a data engineer to pull dat...      False   \n",
       "997  Square Peg Technologies is a boutique technolo...      False   \n",
       "998  Components to include user interface, data flo...      False   \n",
       "999  Data Engineer at various unanticipated client ...      False   \n",
       "\n",
       "    taxoAttributes  taxoAttributesDisplayLimit taxoLogAttributes  \\\n",
       "0               []                           3                []   \n",
       "1               []                           3                []   \n",
       "2               []                           3                []   \n",
       "3               []                           3                []   \n",
       "4               []                           3                []   \n",
       "..             ...                         ...               ...   \n",
       "995             []                           3                []   \n",
       "996             []                           3                []   \n",
       "997             []                           3                []   \n",
       "998             []                           3                []   \n",
       "999             []                           3                []   \n",
       "\n",
       "                                    taxonomyAttributes  \\\n",
       "0    [{'attributes': [{'label': 'Employee discount'...   \n",
       "1    [{'attributes': [{'label': 'Bonus opportunitie...   \n",
       "2    [{'attributes': [], 'label': 'dynamic-attribut...   \n",
       "3    [{'attributes': [], 'label': 'dynamic-attribut...   \n",
       "4    [{'attributes': [], 'label': 'dynamic-attribut...   \n",
       "..                                                 ...   \n",
       "995  [{'attributes': [], 'label': 'dynamic-attribut...   \n",
       "996  [{'attributes': [], 'label': 'dynamic-attribut...   \n",
       "997  [{'attributes': [{'label': 'Paid training', 's...   \n",
       "998  [{'attributes': [], 'label': 'dynamic-attribut...   \n",
       "999  [{'attributes': [], 'label': 'dynamic-attribut...   \n",
       "\n",
       "                                    thirdPartyApplyUrl  \\\n",
       "0    http://www.indeed.com//applystart?jk=7a1a3b7d5...   \n",
       "1    http://www.indeed.com//applystart?jk=9e15b51f1...   \n",
       "2    http://www.indeed.com//applystart?jk=fabba5776...   \n",
       "3    http://www.indeed.com//applystart?jk=e06957ae0...   \n",
       "4    http://www.indeed.com//applystart?jk=0dc1e39dd...   \n",
       "..                                                 ...   \n",
       "995  http://www.indeed.com//applystart?jk=c6c586036...   \n",
       "996  http://www.indeed.com//applystart?jk=86c55092d...   \n",
       "997  http://www.indeed.com//applystart?jk=c03398d70...   \n",
       "998  http://www.indeed.com//applystart?jk=1567c729f...   \n",
       "999  http://www.indeed.com//applystart?jk=3ff497945...   \n",
       "\n",
       "                                              title translatedAttributes  \\\n",
       "0                           data center engineer de                   []   \n",
       "1                                     data engineer                   []   \n",
       "2    technical lead aws data engineer python remote                   []   \n",
       "3                                     data engineer                   []   \n",
       "4                               azure data engineer                   []   \n",
       "..                                              ...                  ...   \n",
       "995                                   data engineer                   []   \n",
       "996                                   data engineer                   []   \n",
       "997                                   data engineer                   []   \n",
       "998                               data engineer etl                   []   \n",
       "999                                   data engineer                   []   \n",
       "\n",
       "    translatedCmiJobTags                     truncatedCompany  urgentlyHiring  \\\n",
       "0                     []                     Randstad Digital            True   \n",
       "1                     []                    Emergent Software           False   \n",
       "2                     []                      CGI Group, Inc.           False   \n",
       "3                     []  Walmart Advanced Systems & Robotics           False   \n",
       "4                     []                   Techstra Solutions           False   \n",
       "..                   ...                                  ...             ...   \n",
       "995                   []                           FlexIT Inc           False   \n",
       "996               ['']                 OneSource Regulatory           False   \n",
       "997                   []              Square Peg Technologies           False   \n",
       "998                   []                          NS2 Mission           False   \n",
       "999                   []             Big Bright International           False   \n",
       "\n",
       "     vjFeaturedEmployerCandidate  \\\n",
       "0                          False   \n",
       "1                          False   \n",
       "2                          False   \n",
       "3                          False   \n",
       "4                          False   \n",
       "..                           ...   \n",
       "995                        False   \n",
       "996                        False   \n",
       "997                        False   \n",
       "998                        False   \n",
       "999                        False   \n",
       "\n",
       "                                        jobDescription  \n",
       "0    Randstad Technologies\\nData Center Site Engine...  \n",
       "1    ** This is a direct-hire opportunity with our ...  \n",
       "2    Technical Lead - AWS Data Engineer (Python) - ...  \n",
       "3    Walmart Advanced Systems &amp; Robotics is a f...  \n",
       "4    As an Azure Data Engineer at Techstra Solution...  \n",
       "..                                                 ...  \n",
       "995  Job Duties: \\n \\n  Develop and support data so...  \n",
       "996  Company Introduction \\n  OneSource Regulatory ...  \n",
       "997  Full Job Description\\n     Position Requires a...  \n",
       "998  Are you interested in working in a dynamic env...  \n",
       "999  Data Engineer at various unanticipated client ...  \n",
       "\n",
       "[1000 rows x 67 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75cbfe992e831883bebc34d351837a73fc44aae74ab7e2dffad989642b16aeb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
