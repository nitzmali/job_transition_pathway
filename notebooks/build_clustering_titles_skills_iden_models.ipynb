{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load SpaCy model for vectorization\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Vectorize skills\n",
    "#skill_vectors = [nlp(skill).vector for skill in skills_list]\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "#Z = linkage(skill_vectors, method='ward')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(skills_list)\n",
    "\n",
    "# Hierarchical clustering\n",
    "Z = linkage(X.toarray(), 'ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z, labels=skills_list, leaf_rotation=90)\n",
    "plt.title(\"Skills Hierarchical Clustering\")\n",
    "plt.show()\n",
    "\n",
    "# From the dendrogram, decide the number of clusters or the level at which to cut the tree\n",
    "# Then assign cluster names based on common themes in each cluster\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Determine the number of clusters, for example, by setting a threshold\n",
    "distance_threshold = 1.5\n",
    "clusters = fcluster(Z, distance_threshold, criterion='distance')\n",
    "clusters\n",
    "# Or determine by specifying the exact number of clusters desired\n",
    "#k = 10\n",
    "#clusters = fcluster(Z, k, criterion='maxclust')\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming you have the following:\n",
    "# clusters: an array with the cluster labels for each job title\n",
    "# job_titles: an array with the corresponding job titles\n",
    "\n",
    "# Create a dictionary where each key is a cluster and the value is a list of titles\n",
    "clustered_titles = {i: [] for i in range(1, len(set(clusters))+1)}\n",
    "for title, cluster_label in zip(skills_list, clusters):\n",
    "    clustered_titles[cluster_label].append(title)\n",
    "\n",
    "# For each cluster, find the most common words\n",
    "cluster_names = {}\n",
    "for cluster, titles in clustered_titles.items():\n",
    "    # Flatten the list of titles into a list of words\n",
    "    words = \" \".join(titles).lower().split()\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    # Remove common stop words (optional, depends on your data)\n",
    "    for stop_word in [ 'and', 'the', '-']:\n",
    "        if stop_word in word_counts:\n",
    "            del word_counts[stop_word]\n",
    "    # Take the top 3 most common words as the name\n",
    "    cluster_names[cluster] = ' '.join([word for word, _ in word_counts.most_common(3)])\n",
    "clustered_titles\n",
    "# Now cluster_names contains a 'name' for each cluster based on the most common words\n",
    "########################\n",
    "xs = set()\n",
    "for i,each in enumerate(df_c_cleaned_2[\"skills_tagged\"]):\n",
    "    if len(each)==0:\n",
    "        print(i)\n",
    "        print(df_c_cleaned_2[i:i+1][\"title\"].unique())\n",
    "        print(df_c_cleaned_2[i:i+1][\"description\"].unique())\n",
    "        print(\"/n\")\n",
    "    for each_v in each:\n",
    "        xs.add(each_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a spacy model to identify skills \n",
    "# create entities \n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the model\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")  # Create the matcher object\n",
    "\n",
    "\n",
    "# Convert the skills list to Doc objects and add them as patterns to the matcher\n",
    "patterns = [nlp.make_doc(skill) for skill in skills_list]\n",
    "matcher.add(\"Skills\", patterns)\n",
    "\n",
    "\n",
    "def get_entities(TRAIN_DATA):\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            print(ent)\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "# Loop through the job descriptions and create training data\n",
    "for job_description in job_descriptions:\n",
    "    # Process the job description to create a Spacy Doc\n",
    "    doc = nlp(job_description)\n",
    "    # Match the patterns to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create Span objects for the matched sequences\n",
    "    spans = [Span(doc, start, end, label=\"SKILLS\") for match_id, start, end in matches]\n",
    "    # Filter the spans to remove overlaps\n",
    "    #print(spans)\n",
    "    filtered_spans = filter_spans(spans)\n",
    "    #print(spans)\n",
    "    entities = [(span.start_char, span.end_char, span) for span in filtered_spans]\n",
    "    TRAIN_DATA.append((job_description, {\"entities\": entities}))\n",
    "\n",
    "#print(TRAIN_DATA)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the model\n",
    "\n",
    "#train the model \n",
    "starting_fresh = False\n",
    "# Load a pre-existing spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')  # for example\n",
    "import random\n",
    "from spacy.util import minibatch\n",
    "# Get the Named Entity Recognizer component in the pipeline\n",
    "ner = nlp.get_pipe('ner')\n",
    "from spacy.training import Example\n",
    "from pathlib import Path\n",
    "\n",
    "# Add new entity labels to 'ner'\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for start,end,label in annotations.get('entities'):\n",
    "        #print(label)\n",
    "        ner.add_label(str(label))\n",
    "\n",
    "\n",
    "# Disable other pipes during training\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "\n",
    "# Begin training\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    if starting_fresh:\n",
    "        nlp.begin_training()\n",
    "\n",
    "    for itn in range(5):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "\n",
    "        # Batch up the examples using spaCy's minibatch\n",
    "        for batch in minibatch(TRAIN_DATA, size=2):\n",
    "            examples = []\n",
    "            for text, annotations in batch:\n",
    "                # Create a Spacy Doc from the text\n",
    "                doc = nlp.make_doc(text)\n",
    "                # Create an Example using the annotations\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                examples.append(example)\n",
    "\n",
    "            # Update the model\n",
    "            nlp.update(\n",
    "                examples,\n",
    "                drop=0.5,  # Dropout - make it harder to memorize data\n",
    "                losses=losses\n",
    "            )\n",
    "        print(losses)\n",
    "\n",
    "'''\n",
    "from pathlib import Path\n",
    "output_dir = Path('/Users/nyzy/nitzmali/job_transition_pathway/models/skills_tag_spacy_nlp_model')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "'''\n",
    "\n",
    "\n",
    "# Split your TRAIN_DATA into train, validate and test sets\n",
    "def train_test_val_split(data, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    # Calculate actual validation set size of the remaining data after test split\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    # Split off test set from available data\n",
    "    train_val_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    # Split remaining data into training and validation sets\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=val_size_adjusted, random_state=random_state)\n",
    "    return train_data, val_data, test_data\n",
    "train_data, val_data, test_data = train_test_val_split(TRAIN_DATA, test_size=0.2, val_size=0.25)\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "# Load the model you want to evaluate\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('/Users/nyzy/nitzmali/job_transition_pathway/models/skills_tag_spacy_nlp_model')  # replace with your model name\n",
    "\n",
    "# Split your TRAIN_DATA into train, validate and test sets\n",
    "train_data, val_data, test_data = train_test_val_split(TRAIN_DATA, test_size=0.2, val_size=0.25)\n",
    "\n",
    "# Convert the validation data to spaCy's Example format\n",
    "examples = []\n",
    "for input_, annots in val_data:\n",
    "    pred = nlp(input_)\n",
    "    example = Example.from_dict(pred, annots)\n",
    "    examples.append(example)\n",
    "\n",
    "\n",
    "# Use the Scorer to score the examples\n",
    "scorer = Scorer(nlp)\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "\n",
    "precision = scores['ents_p']\n",
    "recall = scores['ents_r']\n",
    "f_score = scores['ents_f']\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F-score: {f_score}\")\n",
    "\n",
    "examples[0]\n",
    "\n",
    "\n",
    "# Assume 'nlp' is your loaded NLP model\n",
    "for text, annots in val_data[3:5]:  # Let's use val_data as an example\n",
    "    doc = nlp(text)  # Process the text to predict entities\n",
    "\n",
    "    '''\n",
    "    print(\"Predictions by model:\")\n",
    "    for ent in doc.ents:\n",
    "        print(\"Predictions\")\n",
    "        #print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "    # Now print the correct data for comparison\n",
    "    print(\"\\nCorrect labels:\")\n",
    "    for start, end, label in annots['entities']:\n",
    "        print(\"Actual\")\n",
    "        #print(text[start:end], start, end, label)\n",
    "    '''\n",
    "    # You can use displacy here as well if you prefer visual comparison\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "    # Adding a separation for readability between different examples\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
