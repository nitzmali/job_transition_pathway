{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc9fae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Download necessary NLTK tokens\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "        # Initialize the lemmatizer and stopwords\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Load the spaCy model\n",
    "        self.nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "    def preprocess_text_spacy(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        preprocessed_text = []\n",
    "        for token in doc:\n",
    "            if token.is_stop or token.is_punct or token.is_space:\n",
    "                continue\n",
    "            if token.lemma_ == 'datum':\n",
    "                preprocessed_text.append('data')\n",
    "            else:\n",
    "                preprocessed_text.append(token.text.lower())\n",
    "        return ' '.join(preprocessed_text)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [word for word in words if word not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def clean_lem_stop(self, df, column_name):\n",
    "        df[column_name] = df[column_name].apply(self.preprocess_text_spacy)\n",
    "        df[column_name] = df[column_name].apply(self.clean_text)\n",
    "        return df\n",
    "    \n",
    "# Usage\n",
    "preprocessor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e026ce58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njob_path = \"/Users/gouribenni/Library/CloudStorage/GoogleDrive-gouri.benni@sjsu.edu/My Drive/Data Mining/data/raw/job_data\"\\ncourses_path = \"/Users/gouribenni/Library/CloudStorage/GoogleDrive-gouri.benni@sjsu.edu/My Drive/Data Mining/data/raw/courses_data\"\\nskills_path = \"/Users/gouribenni/Library/CloudStorage/GoogleDrive-gouri.benni@sjsu.edu/My Drive/Data Mining/data/processed_datasets/skills_dataset/skills_df_updated.csv\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, job_path, courses_path, skills_path):\n",
    "        self.job_path = job_path\n",
    "        self.courses_path = courses_path\n",
    "        self.skills_path = skills_path\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "\n",
    "    def load_courses(self):\n",
    "        files_c = [f for f in listdir(self.courses_path) if f.endswith(\".csv\")]\n",
    "        df_c = pd.concat([pd.read_csv(os.path.join(self.courses_path, f)) for f in files_c])\n",
    "        return df_c\n",
    "\n",
    "    def load_skills(self):\n",
    "        df_s = pd.read_csv(self.skills_path)\n",
    "        return df_s\n",
    "\n",
    "    def load_jobs(self):\n",
    "        df_j_main_list = []\n",
    "        job_dir = listdir(self.job_path)\n",
    "        if '.DS_Store' in job_dir:\n",
    "            job_dir.remove('.DS_Store')\n",
    "        for job in job_dir:\n",
    "            files_j = [f for f in listdir(os.path.join(self.job_path, job)) if f.endswith(\".csv\")]\n",
    "            df_j = pd.concat([pd.read_csv(os.path.join(self.job_path, job, f)) for f in files_j])\n",
    "            df_j[\"searched_title\"] = job\n",
    "            df_j_main_list.append(df_j)\n",
    "        df_j = pd.concat(df_j_main_list)\n",
    "        return df_j\n",
    "\n",
    "\n",
    "    \n",
    "    def skills_clean_load(self):\n",
    "        ## clean skills dataset \n",
    "        #clean skills dataset \n",
    "\n",
    "        df_s = self.load_skills()\n",
    "        skills = pd.DataFrame(df_s[df_s[\"Unnamed: 5\"].notna()][\"Unnamed: 5\"].unique())\n",
    "        skills.columns=[\"skills\"]\n",
    "        #skills[\"skills\"] = skills[\"skills\"].apply(preprocess_text_spacy)\n",
    "        skills[\"skills\"] = skills[\"skills\"].apply(self.preprocessor.clean_text)\n",
    "        skills_list = skills[\"skills\"].tolist()\n",
    "        skills_list.append('python')\n",
    "        skills_list.append('python programming')\n",
    "        skills_list.append('statistical')\n",
    "        skills_list.append(\"r programming\")\n",
    "\n",
    "        for ach in [\"docker\",\n",
    "         \"neural network\",\"matlab\",\"google bard ai\",\"ai governance\",\"machine learning\",\"tensorflow\",\"computer vision\",\"prompts\",\"generate prompts\",\"generative\",\"generative ai\",\"nlp\",\"natural language processing\",\"langchain\",\n",
    "         \"pytorch\",\"llm\",\"scala\",\"opencv\"]:\n",
    "            skills_list.append(ach)\n",
    "\n",
    "            \n",
    "        df_s_skills_list = pd.DataFrame(skills_list, columns=['skills_list'])\n",
    "        return df_s_skills_list, skills_list\n",
    "        \n",
    "# Usage\n",
    "'''\n",
    "job_path = \"/Users/gouribenni/Library/CloudStorage/GoogleDrive-gouri.benni@sjsu.edu/My Drive/Data Mining/data/raw/job_data\"\n",
    "courses_path = \"/Users/gouribenni/Library/CloudStorage/GoogleDrive-gouri.benni@sjsu.edu/My Drive/Data Mining/data/raw/courses_data\"\n",
    "skills_path = \"/Users/gouribenni/Library/CloudStorage/GoogleDrive-gouri.benni@sjsu.edu/My Drive/Data Mining/data/processed_datasets/skills_dataset/skills_df_updated.csv\"\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2610884d-c392-434a-b19c-a7b8efb7bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "\n",
    "class DataWriter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def save_data_frame(self,df,path):\n",
    "        df.to_csv(path)\n",
    "    def save_json(self,json_f,path):\n",
    "        with open(path, 'w') as json_file:\n",
    "            json.dump(json_f, json_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662c906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d10404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean jobs dataset \n",
    "import ast \n",
    "import numpy as np\n",
    "class clean_jobs_data:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        \n",
    "    def clean_job_df(self, df):\n",
    "        df = self.preprocessor.clean_lem_stop(df,\"title\")\n",
    "        df = self.preprocessor.clean_lem_stop(df,\"jobDescription\")\n",
    "        job_descriptions = df_j[\"jobDescription\"].tolist()\n",
    "        return df,job_descriptions\n",
    "\n",
    "    def sdnjsj():\n",
    "        columns_of_interest = ['company',\n",
    "         'companyRating',\n",
    "         'companyReviewCount',\n",
    "         'displayTitle',\n",
    "         'employerAssistEnabled',\n",
    "         'employerResponsive',\n",
    "         'extractedSalary',\n",
    "         'featuredEmployer',\n",
    "         'featuredEmployerCandidate',\n",
    "         'formattedLocation',\n",
    "         'formattedRelativeTime',\n",
    "         'highVolumeHiringModel',\n",
    "         'hiringEventJob',\n",
    "         'indeedApplyEnabled',\n",
    "         'indeedApplyable',\n",
    "         'isJobVisited',\n",
    "         'isMobileThirdPartyApplyable',\n",
    "         'isNoResumeJob',\n",
    "         'isSubsidiaryJob',\n",
    "         'jobCardRequirementsModel',\n",
    "         'jobLocationCity',\n",
    "         'jobLocationState',\n",
    "         'locationCount',\n",
    "         'newJob',\n",
    "         'normTitle',\n",
    "         'openInterviewsInterviewsOnTheSpot',\n",
    "         'openInterviewsJob',\n",
    "         'openInterviewsOffersOnTheSpot',\n",
    "         'openInterviewsPhoneJob',\n",
    "         'overrideIndeedApplyText',\n",
    "         'remoteLocation',\n",
    "         'resumeMatch',\n",
    "         'salarySnippet',\n",
    "         'showAttainabilityBadge',\n",
    "         'showCommutePromo',\n",
    "         'showEarlyApply',\n",
    "         'showJobType',\n",
    "         'showRelativeDate',\n",
    "         'showSponsoredLabel',\n",
    "         'showStrongerAppliedLabel',\n",
    "         'smartFillEnabled',\n",
    "         'smbD2iEnabled',\n",
    "         'snippet',\n",
    "         'sponsored',\n",
    "         'title',\n",
    "         'truncatedCompany',\n",
    "         'urgentlyHiring',\n",
    "         'vjFeaturedEmployerCandidate',\n",
    "         'jobDescription','searched_title']\n",
    "\n",
    "    def salary_snippet_to_dict(self,row):\n",
    "        if pd.isna(row):\n",
    "            # Return a dictionary with NaN values if the row is NaN\n",
    "            return {'currency': np.nan, 'salaryTextFormatted': np.nan, 'source': np.nan,'text':np.nan}\n",
    "        elif isinstance(row, dict):\n",
    "            # Return the row as is if it's already a dictionary\n",
    "            return row\n",
    "        else:\n",
    "            # If the row is a string representation of a dictionary (assumed if not NaN or dict),\n",
    "            # safely evaluate it to a dictionary here\n",
    "            # Note: Be cautious with `eval`. Here it's mentioned for potential string to dict conversion.\n",
    "            # In a secure context, confirm the string format and consider `ast.literal_eval` instead.\n",
    "            try:\n",
    "                # Convert string to dictionary safely\n",
    "                dict_row = eval(row)\n",
    "                return dict_row if isinstance(dict_row, dict) else {'max': np.nan, 'min': np.nan, 'type': np.nan}\n",
    "            except:\n",
    "                # In case of error during eval, return NaN values\n",
    "                return {'currency': np.nan, 'salaryTextFormatted': np.nan, 'source': np.nan,'text':np.nan}\n",
    "\n",
    "    def salary_to_dict(self,row):\n",
    "        if pd.isna(row):\n",
    "            # Return a dictionary with NaN values if the row is NaN\n",
    "            return {'max': np.nan, 'min': np.nan, 'type': np.nan}\n",
    "        elif isinstance(row, dict):\n",
    "            # Return the row as is if it's already a dictionary\n",
    "            return row\n",
    "        else:\n",
    "            # If the row is a string representation of a dictionary (assumed if not NaN or dict),\n",
    "            # safely evaluate it to a dictionary here\n",
    "            # Note: Be cautious with `eval`. Here it's mentioned for potential string to dict conversion.\n",
    "            # In a secure context, confirm the string format and consider `ast.literal_eval` instead.\n",
    "            try:\n",
    "                # Convert string to dictionary safely\n",
    "                dict_row = eval(row)\n",
    "                return dict_row if isinstance(dict_row, dict) else {'max': np.nan, 'min': np.nan, 'type': np.nan}\n",
    "            except:\n",
    "                # In case of error during eval, return NaN values\n",
    "                return {'max': np.nan, 'min': np.nan, 'type': np.nan}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49be95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde3507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "class phrase_matcher:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "\n",
    "    def phrase_matcher_model(self,description,skills_list):\n",
    "        nlp = spacy.load(\"en_core_web_md\")  # Load the model\n",
    "        matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")  # Create the matcher object\n",
    "    \n",
    "        # Assuming 'skills_list' is a list of skills, and 'job_descriptions' is a list containing job descriptions\n",
    "    \n",
    "        # Add patterns to the matcher. Patterns are made by converting each skill string into a Doc object\n",
    "        patterns = [nlp.make_doc(skill) for skill in skills_list]\n",
    "        matcher.add(\"Skills\", patterns)\n",
    "    \n",
    "        # Process the job description to create a Spacy Doc\n",
    "        doc = nlp(description)\n",
    "    \n",
    "        # Match the patterns to the doc\n",
    "        matches = matcher(doc)\n",
    "    \n",
    "        # Create Span objects for the matched sequences\n",
    "        spans = [Span(doc, start, end, label=\"SKILL\") for match_id, start, end in matches]\n",
    "    \n",
    "        # Filter the spans to remove overlaps\n",
    "        filtered_spans = filter_spans(spans)\n",
    "    \n",
    "        # Now you can create new entities in the doc using the filtered spans\n",
    "        doc.ents = filtered_spans  # Overwrite or append to doc.ents with the non-overlapping skill entities\n",
    "        entities_extracted = []\n",
    "        # Print the entities in the document\n",
    "        for ent in doc.ents:\n",
    "            entities_extracted.append(ent.text)\n",
    "        '''\n",
    "        for each in matches:\n",
    "            #print(each)\n",
    "            if each.lower_ == \"statistical\":\n",
    "                for skills_identify in skills_list:\n",
    "                    if each.lower_ in skills_identify:\n",
    "                        #print(skills_identify)\n",
    "                        entities_extracted.append(skills_identify)\n",
    "        '''\n",
    "        return set(entities_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617b57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799de19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd791af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9794ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class define_job_progression_dictionary:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "\n",
    "    def define_job_progression(self):\n",
    "\n",
    "            Job_progression_dictionary =  {\n",
    "            \"Junior Data Analyst\" : [\"Data Analyst\", \"Senior Data Analyst\", \"Data Engineer\", \"Senior Data Engineer\", \"Lead Data Engineer\", \"Data Architect\", \"Data Manager\", \"Director of Analytics\", \"Chief Data Officer\"],\n",
    "            \"Junior Data Scientist\" : [\"Data Scientist\", \"Senior Data Scientist\", \"Lead Data Scientist\", \"Principal Data Scientist\", \"Data Science Manager\", \"Director of Data Science\", \"Vice President of Data Science\", \"Chief Data Scientist\"],\n",
    "            \"Junior Data Engineer\" : [\"Data Engineer\", \"Senior Data Engineer\", \"Data Pipeline Architect\", \"Data Engineering Manager\", \"Director of Data Engineering\", \"Chief Technology Officer (CTO)\"],\n",
    "            \"Business Intelligence Analyst\" : [\"Senior Business Intelligence Analyst\", \"Business Intelligence Manager\", \"Director of Business Intelligence\", \"Vice President of Business Intelligence\", \"Chief Intelligence Officer\"],\n",
    "            \"Machine Learning Engineer\" : [\"Senior Machine Learning Engineer\", \"Machine Learning Architect\", \"Machine Learning Manager\", \"Head of Machine Learning\", \"Chief AI Officer\"],\n",
    "            \"Data Analyst\" : [\"Data Engineer\", \"Senior Data Engineer\", \"Data Architect\", \"Senior Data Architect\", \"Enterprise Architect\", \"Chief Technology Officer (CTO)\"],\n",
    "            \"Data Analyst\" : [\"Big Data Engineer\", \"Senior Big Data Engineer\", \"Big Data Architect\", \"Head of Big Data\", \"Director of Data Engineering\", \"Chief Information Officer (CIO)\"],\n",
    "            \"Junior AI Developer\" : [\"AI Developer\", \"Senior AI Developer\", \"AI Architect\", \"AI Project Manager\", \"Head of AI\", \"Chief AI Officer\"],\n",
    "            \"Junior DBA\" : [\"Mid-level DBA\", \"Senior DBA\", \"Database Manager\", \"Data Architect\", \"Director of Database Management\", \"Chief Information Officer\"],\n",
    "            \"Junior Statistician\" : [\"Statistician\" , \"Senior Statistician\", \"Quantitative Analyst\", \"Senior Quantitative Analyst\", \"Quantitative Research Manager\", \"Director of Quantitative Research\"],\n",
    "            \"Junior Business Analyst\" : [\"Business Analyst\", \"Senior Business Analyst\", \"Business Analysis Manager\", \"Business Intelligence Analyst\", \"Director of Business Analysis\", \"Chief Strategy Officer\"],\n",
    "            \"Research Assistant\" : [\"Research Analyst\", \"Research Scientist\", \"Senior Research Scientist\", \"Principal Scientist\", \"Director of Research\", \"Chief Science Officer\"],\n",
    "            \"Data Analyst\" : [\"Product Analyst\", \"Data Product Manager\", \"Senior Data Product Manager\", \"Director of Product Management\", \"Vice President of Product\", \"Chief Product Officer\"]\n",
    "            }\n",
    "\n",
    "            original_dict = {\n",
    "                \"Junior Data Analyst\": [\"Data Analyst\", \"Senior Data Analyst\", \"Data Engineer\", \"Senior Data Engineer\", \n",
    "                                        \"Lead Data Engineer\", \"Data Architect\", \"Data Manager\", \"Director of Analytics\", \n",
    "                                        \"Chief Data Officer\"],\n",
    "                \"Junior Data Scientist\": [\"Data Scientist\", \"Senior Data Scientist\", \"Lead Data Scientist\", \n",
    "                                          \"Principal Data Scientist\", \"Data Science Manager\", \"Director of Data Science\", \n",
    "                                          \"Vice President of Data Science\", \"Chief Data Scientist\"],\n",
    "                \"Junior Data Engineer\": [\"Data Engineer\", \"Senior Data Engineer\", \"Data Pipeline Architect\", \n",
    "                                         \"Data Engineering Manager\", \"Director of Data Engineering\", \"Chief Technology Officer\"],\n",
    "                \"Business Intelligence Analyst\": [\"Senior Business Intelligence Analyst\", \"Business Intelligence Manager\", \n",
    "                                                  \"Director of Business Intelligence\", \"Vice President of Business Intelligence\", \n",
    "                                                  \"Chief Intelligence Officer\"],\n",
    "                \"Machine Learning Engineer\": [\"Senior Machine Learning Engineer\", \"Machine Learning Architect\", \n",
    "                                              \"Machine Learning Manager\", \"Head of Machine Learning\", \"Chief AI Officer\"],\n",
    "                \"Junior AI Developer\": [\"AI Developer\", \"Senior AI Developer\", \"AI Architect\", \"AI Project Manager\", \n",
    "                                        \"Head of AI\", \"Chief AI Officer\"],\n",
    "                \"Junior DataBase Administrator\": [\"DataBase Administrator\", \"Senior DataBase Administrator\", \"Database Manager\", \"Data Architect\", \"Director of Database Management\", \n",
    "                               \"Chief Information Officer\"],\n",
    "                \"Junior Statistician\": [\"Statistician\", \"Senior Statistician\", \"Quantitative Analyst\", \"Senior Quantitative Analyst\", \n",
    "                                        \"Quantitative Research Manager\", \"Director of Quantitative Research\"],\n",
    "                \"Junior Business Analyst\": [\"Business Analyst\", \"Senior Business Analyst\", \"Business Analysis Manager\", \n",
    "                                            \"Business Intelligence Analyst\", \"Director of Business Analysis\", \"Chief Strategy Officer\"],\n",
    "                \"Research Assistant\": [\"Research Analyst\", \"Research Scientist\", \"Senior Research Scientist\", \"Principal Scientist\", \n",
    "                                       \"Director of Research\", \"Chief Science Officer\"],\n",
    "                # Multiple entries for Data Analyst have been combined to include all unique progressions\n",
    "                \"Data Analyst\": [\"Data Engineer\", \"Senior Data Engineer\", \"Lead Data Engineer\", \"Data Architect\", \n",
    "                                 \"Senior Data Architect\", \"Enterprise Architect\", \"Product Analyst\", \"Data Product Manager\", \n",
    "                                 \"Senior Data Product Manager\", \"Director of Product Management\", \"Vice President of Product\", \n",
    "                                 \"Chief Product Officer\", \"Big Data Engineer\", \"Senior Big Data Engineer\", \"Big Data Architect\", \n",
    "                                 \"Head of Big Data\", \"Director of Data Engineering\", \"Chief Information Officer\"]\n",
    "            }\n",
    "\n",
    "            # Define lateral moves for the given roles\n",
    "            lateral_moves = {\n",
    "                \"Data Analyst\": [\"Business Intelligence Analyst\", \"Machine Learning Engineer\"],\n",
    "                \"Data Scientist\": [\"Data Engineer\", \"AI Developer\"],\n",
    "                \"Data Engineer\": [\"Machine Learning Engineer\", \"Big Data Engineer\"],\n",
    "                \"Business Intelligence Analyst\": [\"Data Analyst\", \"Data Scientist\"],\n",
    "                \"Machine Learning Engineer\": [\"Data Scientist\", \"AI Developer\"],\n",
    "                \"AI Developer\": [\"Machine Learning Engineer\", \"Data Engineer\"],\n",
    "                \"Data Base Administrator\": [\"Data Engineer\", \"Data Analyst\"],\n",
    "                \"Statistician\": [\"Data Analyst\", \"Data Scientist\"],\n",
    "                \"Business Analyst\": [\"Data Analyst\", \"Business Intelligence Analyst\"],\n",
    "                \"Research Analyst\": [\"Data Scientist\", \"Statistician\"]\n",
    "            }\n",
    "            return Job_progression_dictionary,original_dict,lateral_moves\n",
    "\n",
    "    \n",
    "    # Since we want to include lateral moves for each value in the original dictionary, \n",
    "    # we will create a function that merges the direct progressions and lateral moves into one list.\n",
    "    \n",
    "    # Function to merge progression and lateral moves\n",
    "    def merge_progression_and_lateral_moves(self,direct_progression, lateral_move_titles):\n",
    "        # Start with direct progression\n",
    "        full_progression = direct_progression.copy()\n",
    "        \n",
    "        # Add lateral moves for each title in the direct progression if they exist\n",
    "        for title in direct_progression:\n",
    "            lateral_titles = lateral_moves.get(title, [])\n",
    "            for lateral_title in lateral_titles:\n",
    "                if lateral_title not in full_progression:  # Avoid duplicates\n",
    "                    full_progression.append(lateral_title)\n",
    "        \n",
    "        return full_progression\n",
    "    \n",
    "    # Function to build a full job progression dictionary for each title\n",
    "    def build_full_progression_dict(self,original_dict, lateral_moves):\n",
    "        full_progression_dict = {}\n",
    "        \n",
    "        # Iterate over each starting job title\n",
    "        for start_title, progression in original_dict.items():\n",
    "            # Get the full progression for the starting title\n",
    "            full_progression = self.merge_progression_and_lateral_moves(progression, lateral_moves)\n",
    "            \n",
    "            # Add the full progression to the dictionary for the starting title\n",
    "            full_progression_dict[start_title] = full_progression\n",
    "            \n",
    "            # Now iterate over each job within the progression to build their own progression paths\n",
    "            for i, title in enumerate(progression):\n",
    "                if title not in full_progression_dict:  # Only add if it doesn't already exist to avoid overwriting\n",
    "                    # Get the progression for this title (which is the rest of the list after this title)\n",
    "                    next_progression = self.merge_progression_and_lateral_moves(progression[i + 1:], lateral_moves)\n",
    "                    full_progression_dict[title] = next_progression\n",
    "        \n",
    "        return full_progression_dict\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d239c34-34cf-4b29-8c72-0688a815a83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a8f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027c232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "class title_matcher:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Function to normalize job titles\n",
    "    def normalize_title(self,title):\n",
    "        # Lowercase and remove non-alphanumeric characters, replace with spaces\n",
    "        title = re.sub(r'[^a-z0-9]', ' ', title.lower())\n",
    "        # Remove extra whitespace\n",
    "        title = re.sub(r'\\s+', ' ', title).strip()\n",
    "    \n",
    "            # Remove common prefixes/suffixes\n",
    "        #title = re.sub(r'\\b(senior|junior|associate|expert|technical|lead|l\\d+)\\b', '', title)\n",
    "        title = re.sub(r'\\b(sr|senior|expert|technical|lead|l\\d+)\\b', 'Senior', title)\n",
    "        title = re.sub(r'\\b(vp|vice president|l\\d+)\\b', 'Senior', title)\n",
    "        title = re.sub(r'\\b(III|6|5|4|3|l\\d+)\\b', 'Senior', title)\n",
    "        # Replace specific terms with standardized equivalents\n",
    "        title = re.sub(r'\\b(sr\\.?|senior)\\b', 'senior', title)  # Replace 'sr' or 'senior' with 'senior'\n",
    "        title = re.sub(r'\\b(jr\\.?|junior)\\b', 'junior', title) \n",
    "    \n",
    "        # Convert to lower case and remove special characters\n",
    "        title = re.sub(r'[^a-z\\s]', '', title.lower())\n",
    "    \n",
    "        # Strip extra whitespace\n",
    "        title = re.sub(r'\\s+', ' ', title).strip()\n",
    "        return title\n",
    "    # Function to find the best matching title from the base job titles\n",
    "    def match_title_to_base(self,scraped_title, base_job_titles):\n",
    "        # Normalize the scraped job title\n",
    "        normalized_title = self.normalize_title(scraped_title)\n",
    "        # Check if the normalized title exactly matches one of the base job titles\n",
    "        if normalized_title in base_job_titles:\n",
    "            return normalized_title  # Return the matching base title\n",
    "        \n",
    "        # Partial match checking - longer base titles are checked first to match more specific job titles\n",
    "        sorted_base_titles = sorted(base_job_titles, key=len, reverse=True)\n",
    "        for base_title in sorted_base_titles:\n",
    "            if base_title in normalized_title:\n",
    "                if \"senior\" in normalized_title and \"senior\" not in base_title:\n",
    "                    return \"senior \"+base_title\n",
    "                if \"junior\" in normalized_title and \"junior\" not in base_title:\n",
    "                    return \"junior \"+base_title\n",
    "                if \"chief\" in normalized_title and \"chief\" not in base_title:\n",
    "                    return \"chief \"+base_title\n",
    "                return base_title\n",
    "        \n",
    "        return \"unmatched\"  # Return \"unmatched\" or some default value if no match is found\n",
    "    \n",
    "    # Define a function to match job titles using fuzzy string matching\n",
    "    def fuzzy_match_title(self,scraped_title, base_job_titles, threshold=90):\n",
    "        # Use the process function to find the closest match above a certain score threshold\n",
    "        best_match, score = process.extractOne(self.normalize_title(scraped_title), base_job_titles)\n",
    "        # Only accept the match if the score is above the threshold\n",
    "        return best_match if score >= threshold else \"unmatched\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f74b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37356a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042eae51-6f40-4103-bc0e-d3ff4ddd2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "class clean_salary:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    def fill_na_type(self,min_sal,max_sal,type):\n",
    "        if pd.isna(type):\n",
    "            if min_sal//1000 or max_sal//1000:\n",
    "                return \"yearly\"\n",
    "            else:\n",
    "                return \"hourly\"\n",
    "        else:\n",
    "            return type\n",
    "\n",
    "    def adjust_max_salary(self,min_sal,type):\n",
    "        if type==\"yearly\":\n",
    "            return min_sal+10000\n",
    "        else:\n",
    "            return min_sal+10\n",
    "\n",
    "    def clean_df_job(self,df_j_cleaned):\n",
    "        for each in df_j_cleaned[\"title_cleaned\"].unique():\n",
    "            condition = df_j_cleaned[\"title_cleaned\"]==each\n",
    "            # Condition for filtering ColumnB\n",
    "        \n",
    "            # Calculate the average of ColumnB based on the condition\n",
    "            average_value_min = df_j_cleaned.loc[condition, 'min'].mean()\n",
    "            average_value_max = df_j_cleaned.loc[condition, 'max'].mean()\n",
    "        \n",
    "            # Fill missing values in ColumnA with this average\n",
    "            df_j_cleaned.loc[condition,\"min\"] = df_j_cleaned.loc[condition,\"min\"].fillna(average_value_min)\n",
    "            df_j_cleaned.loc[condition,\"max\"] = df_j_cleaned.loc[condition,\"max\"].fillna(average_value_max)\n",
    "        \n",
    "        # Assuming a standard work year of 40 hours/week and 52 weeks/year\n",
    "        HOURS_PER_WEEK = 40\n",
    "        WEEKS_PER_YEAR = 52\n",
    "        condition = df_j_cleaned[\"type\"]==\"hourly\"\n",
    "        df_j_cleaned.loc[condition,\"min\"]=df_j_cleaned.loc[condition,\"min\"]*HOURS_PER_WEEK * WEEKS_PER_YEAR\n",
    "        df_j_cleaned.loc[condition,\"max\"]=df_j_cleaned.loc[condition,\"max\"]*HOURS_PER_WEEK * WEEKS_PER_YEAR\n",
    "        df_j_cleaned.loc[condition,\"type\"]=\"yearly\"\n",
    "        df_j_cleaned[\"min\"].fillna(df_j_cleaned[\"min\"].mean(),inplace=True)\n",
    "        df_j_cleaned[\"max\"].fillna(df_j_cleaned[\"max\"].mean(),inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Assuming df is your DataFrame and 'current_salary' is the salary column\n",
    "        scaler = StandardScaler()\n",
    "        df_j_cleaned['normalized_min'] = scaler.fit_transform(df_j_cleaned[['min']])\n",
    "        df_j_cleaned['normalized_max'] = scaler.fit_transform(df_j_cleaned[['max']])\n",
    "    \n",
    "        return df_j_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e309269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b7df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "615a51fc-d1cf-45e2-8526-763a3b776d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b4e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class course_clean:\n",
    "    def __init__(self):\n",
    "        self.tp_cl = TextPreprocessor()\n",
    "    def salary_to_dict(self,row):\n",
    "        if pd.isna(row):\n",
    "            # Return a dictionary with NaN values if the row is NaN\n",
    "            return {'max': np.nan, 'min': np.nan, 'type': np.nan}\n",
    "        elif isinstance(row, dict):\n",
    "            # Return the row as is if it's already a dictionary\n",
    "            return row\n",
    "        else:\n",
    "            # If the row is a string representation of a dictionary (assumed if not NaN or dict),\n",
    "            # safely evaluate it to a dictionary here\n",
    "            # Note: Be cautious with `eval`. Here it's mentioned for potential string to dict conversion.\n",
    "            # In a secure context, confirm the string format and consider `ast.literal_eval` instead.\n",
    "            try:\n",
    "                # Convert string to dictionary safely\n",
    "                dict_row = eval(row)\n",
    "                return dict_row if isinstance(dict_row, dict) else {'max': np.nan, 'min': np.nan, 'type': np.nan}\n",
    "            except:\n",
    "                # In case of error during eval, return NaN values\n",
    "                return {'max': np.nan, 'min': np.nan, 'type': np.nan}\n",
    "    \n",
    "    def  course_price_detail(self,x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        else:\n",
    "            cou_dict = ast.literal_eval(x)\n",
    "            return cou_dict[\"amount\"]\n",
    "    \n",
    "    def  category_primary(self,x):\n",
    "        cou_dict = ast.literal_eval(x)\n",
    "        return cou_dict[\"url\"]\n",
    "    \n",
    "    def  labels_return_title(self,x):\n",
    "        cou_dict = ast.literal_eval(x)\n",
    "        if len(cou_dict)==0:\n",
    "            return \"UnDefined\"\n",
    "        return cou_dict[0][\"title\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def  labels_return_url(self,x):\n",
    "        cou_dict = ast.literal_eval(x)\n",
    "        if len(cou_dict)==0:\n",
    "            return \"UnDefined\"\n",
    "        return cou_dict[0][\"url\"]\n",
    "    \n",
    "    \n",
    "    def  labels_return_display_name(self,x):\n",
    "        cou_dict = ast.literal_eval(x)\n",
    "        if len(cou_dict)==0:\n",
    "            return \"UnDefined\"\n",
    "        return cou_dict[0][\"display_name\"]\n",
    "\n",
    "    #clean course data \n",
    "    def clean_course_df(self,df):\n",
    "        df = df.reset_index(drop=True)\n",
    "        # Create an empty DataFrame with a column named 'single_space' that has one row with a space\n",
    "        df_space = pd.DataFrame({\"single_space\": [\" \"] * len(df)})\n",
    "        \n",
    "        df[\"full_description\"] = df[\"title\"]+df_space[\"single_space\"]+df[\"description\"]+df_space[\"single_space\"]+df[\"headline\"]+df_space[\"single_space\"]+df[\"what_you_will_learn_data\"]+df_space[\"single_space\"]+df[\"objectives\"]\n",
    "        df = self.tp_cl.clean_lem_stop(df,\"full_description\")\n",
    "        course_description = df[\"full_description\"].tolist()\n",
    "        return df,course_description\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa554bf8-2681-4a6a-8acc-e318f0030179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    job_path = \"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/raw/job_data\"\n",
    "    courses_path = \"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/raw/courses_data\"\n",
    "    skills_path = \"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/skills_dataset/skills_df_updated.csv\"\n",
    "    \n",
    "    data_loader = DataLoader(job_path, courses_path, skills_path)\n",
    "    df_c = data_loader.load_courses()\n",
    "    df_s = data_loader.load_skills()\n",
    "    df_j = data_loader.load_jobs()\n",
    "    df_j,df_s,df_c = df_j[:10],df_s[:10],df_c[:10]\n",
    "    df_s_skills_list, skills_list = data_loader.skills_clean_load()\n",
    "    \n",
    "    clean_jobs = clean_jobs_data(df_j)\n",
    "    df_j_cleaned, job_descriptions = clean_jobs.clean_job_df(df_j)\n",
    "    \n",
    "    # Apply the function to each row of the 'extractedSalary' column\n",
    "    salary_dicts = df_j_cleaned['extractedSalary'].apply(clean_jobs.salary_to_dict)\n",
    "    # Now that we have a series of dictionaries, use `json_normalize` to create a DataFrame\n",
    "    salary_df = pd.json_normalize(salary_dicts)\n",
    "    \n",
    "    # Concatenate the new DataFrame with the original one\n",
    "    df_j_cleaned = pd.concat([df_j_cleaned.drop('extractedSalary', axis=1).reset_index(), salary_df], axis=1)\n",
    "    \n",
    "    # Apply the function to each row of the 'extractedSalary' column\n",
    "    salary_snippet_dicts = df_j_cleaned['salarySnippet'].apply(clean_jobs.salary_snippet_to_dict)\n",
    "    # Now that we have a series of dictionaries, use `json_normalize` to create a DataFrame\n",
    "    salary_snippet_df = pd.json_normalize(salary_snippet_dicts)\n",
    "    \n",
    "    # Concatenate the new DataFrame with the original one\n",
    "    df_j_cleaned = pd.concat([df_j_cleaned.drop('salarySnippet', axis=1).reset_index(), salary_snippet_df], axis=1)\n",
    "    \n",
    "    pm = phrase_matcher()\n",
    "    df_j_cleaned[\"skills_tagged\"]=df_j_cleaned[\"jobDescription\"].apply(lambda x:pm.phrase_matcher_model(x,skills_list))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #df[\"extractedSalary\"]=df[\"extractedSalary\"].astype(str)\n",
    "    #df[\"extractedSalary\"]=df['extractedSalary'].apply(ast.literal_eval)\n",
    "    \n",
    "    job_progress_cl = define_job_progression_dictionary()\n",
    "    Job_progression_dictionary,original_dict,lateral_moves = job_progress_cl.define_job_progression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Building the full job progression dictionary\n",
    "    full_job_progression_dict = job_progress_cl.build_full_progression_dict(original_dict, lateral_moves)\n",
    "    \n",
    "    # Sorting the dictionary for better readability\n",
    "    sorted_full_job_progression_dict = {k: full_job_progression_dict[k] for k in sorted(full_job_progression_dict)}\n",
    "    \n",
    "    \n",
    "    unique_job_title_full = set()\n",
    "    for key,value in Job_progression_dictionary.items():\n",
    "        unique_job_title_full.add(key)\n",
    "        for each in value:\n",
    "            unique_job_title_full.add(each)\n",
    "    \n",
    "    \n",
    "    unique_job_title = set()\n",
    "    for key,value in sorted_full_job_progression_dict.items():\n",
    "        unique_job_title.add(key)\n",
    "        for each in value:\n",
    "            unique_job_title.add(each)\n",
    "    \n",
    "    unique_job_title.difference(unique_job_title_full)\n",
    "    \n",
    "    sorted_full_job_progression_dict\n",
    "    \n",
    "    sorted_full_job_progression_dict_lower={}\n",
    "    for k,v in sorted_full_job_progression_dict.items():\n",
    "        sorted_full_job_progression_dict_lower[k.lower()]=[each.lower() for each in v]\n",
    "    \n",
    "    sorted_full_job_progression_dict_lower\n",
    "    \n",
    "    title_matcher_cl =title_matcher()\n",
    "\n",
    "    # Create a set of base job titles from your job progression dictionary\n",
    "    base_job_titles = set()\n",
    "    for titles_list in sorted_full_job_progression_dict.values():\n",
    "        for title in titles_list:\n",
    "            base_job_titles.add(title_matcher_cl.normalize_title(title))  # Add the normalized base title\n",
    "    \n",
    "    df_j_cleaned[\"title_cleaned\"] = df_j_cleaned[\"title\"].apply(lambda x: title_matcher_cl.fuzzy_match_title(x,base_job_titles))\n",
    "    df_j_cleaned[\"title_cleaned\"] = df_j_cleaned[\"title_cleaned\"].apply(lambda x: title_matcher_cl.match_title_to_base(x,base_job_titles))\n",
    "    df_j_cleaned[\"title_cleaned\"] = df_j_cleaned.apply(\n",
    "        lambda x: x[\"title_cleaned\"] if x[\"title_cleaned\"] != \"unmatched\" else title_matcher_cl.fuzzy_match_title(x[\"normTitle\"], base_job_titles),\n",
    "        axis=1\n",
    "    )\n",
    "    df_j_cleaned[\"title_cleaned\"] = df_j_cleaned.apply(\n",
    "        lambda x: x[\"title_cleaned\"] if x[\"title_cleaned\"] != \"unmatched\" else title_matcher_cl.match_title_to_base(x[\"normTitle\"], base_job_titles),\n",
    "        axis=1\n",
    "    )\n",
    "    #df_j_cleaned[\"displayTitle\"] = df_j_cleaned[\"displayTitle\"].apply(lambda x: fuzzy_match_title(x,base_job_titles))\n",
    "    #df_j_cleaned[\"displayTitle\"] = df_j_cleaned[\"displayTitle\"].apply(lambda x: match_title_to_base(x,base_job_titles))\n",
    "    \n",
    "    #df_j_cleaned[\"title\"] = df_j_cleaned[\"title\"].apply(lambda x: match_title_to_base(x,base_job_titles))\n",
    "    \n",
    "    columns_most_important = ['company', 'truncatedCompany',\n",
    "     'companyRating',\n",
    "     'companyReviewCount',\n",
    "      'searched_title',\n",
    "        'title',\n",
    "     'title_cleaned',\n",
    "     'normTitle',\n",
    "     'min',\n",
    "     'max',\n",
    "     'type',\n",
    "     'snippet',\n",
    "     'jobDescription',\n",
    "     'skills_tagged']\n",
    "    \n",
    "    df_j_cleaned = df_j_cleaned[columns_most_important]\n",
    "    \n",
    "    condition =df_j_cleaned[\"title_cleaned\"]==\"unmatched\"\n",
    "    df_j_cleaned.loc[condition,\"title_cleaned\"]=df_j_cleaned.loc[condition,\"searched_title\"]\n",
    "    df_j_cleaned[\"title_cleaned\"] = df_j_cleaned[\"title_cleaned\"].apply(lambda x:x.lower()) \n",
    "    \n",
    "    df_j_cleaned[\"title_cleaned\"] = df_j_cleaned.apply(\n",
    "        lambda x: x[\"title_cleaned\"] if x[\"title_cleaned\"] != \"unmatched\" else fuzzy_match_title(x[\"searched_title\"], base_job_titles),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    df_j_cleaned[\"title_cleaned\"]=df_j_cleaned[\"title_cleaned\"].apply(lambda x:x.replace(\"(cto)\",\"\").strip())\n",
    "    df_j_cleaned[\"title_cleaned\"]=df_j_cleaned[\"title_cleaned\"].apply(lambda x:x.replace(\"(cio)\",\"\").strip())\n",
    "    df_j_cleaned[\"title_cleaned\"].unique()\n",
    "    \n",
    "    \n",
    "    clean_salary_cl =clean_salary()\n",
    "    df_j_cleaned[\"type\"]=df_j_cleaned.apply(lambda x: clean_salary_cl.fill_na_type(x[\"min\"],x[\"max\"],x[\"type\"]),axis=1)\n",
    "    df_j_cleaned[\"max\"]=df_j_cleaned[[\"min\",\"type\"]].apply(lambda x :clean_salary_cl.adjust_max_salary(x[\"min\"],x[\"type\"]),axis=1)\n",
    "    df_j_cleaned = clean_salary_cl.clean_df_job(df_j_cleaned)\n",
    "\n",
    "    ## courses clean\n",
    "    course_clean_cl =course_clean()\n",
    "    df_c_cleaned = df_c\n",
    "    df_c_cleaned.columns.tolist()\n",
    "    \n",
    "    ## Lemmitization and cleaning columns\n",
    "    \n",
    "    df_c_cleaned[\"price\"] = df_c_cleaned[\"price_detail\"].apply(lambda x: course_clean_cl.course_price_detail(x))\n",
    "    df_c_cleaned[\"requirements_data\"] = df_c_cleaned[\"requirements_data\"].apply(lambda x:ast.literal_eval(x)[0])\n",
    "    df_c_cleaned[\"course_title\"]=df_c_cleaned[\"labels\"].apply(lambda x:course_clean_cl.labels_return_title(x))\n",
    "    df_c_cleaned[\"course_url\"] =df_c_cleaned[\"labels\"].apply(lambda x:course_clean_cl.labels_return_url(x))\n",
    "    df_c_cleaned[\"course_display_name\"] =df_c_cleaned[\"labels\"].apply(lambda x:course_clean_cl.labels_return_display_name(x))\n",
    "    df_c_cleaned[\"objectives\"] =df_c_cleaned[\"objectives\"].apply(lambda x:ast.literal_eval(x)[0])\n",
    "    del df_c_cleaned[\"labels\"]\n",
    "    df_c_cleaned[\"what_you_will_learn_data\"] = df_c_cleaned[\"what_you_will_learn_data\"].apply(lambda x:ast.literal_eval(x)[0])\n",
    "    df_c_cleaned[\"target_audiences\"] = df_c_cleaned[\"target_audiences\"].apply(lambda x:ast.literal_eval(x)[0])\n",
    "    del df_c_cleaned[\"price_detail\"]\n",
    "    \n",
    "    ## merge all description into one    \n",
    "    \n",
    "    \n",
    "    clean_course_columns =[\"title\",\n",
    "    \"description\",\n",
    "    \"headline\",\n",
    "    \"requirements_data\",\n",
    "    \"what_you_will_learn_data\",\n",
    "    \"target_audiences\",\n",
    "    \"objectives\",\n",
    "    #\"full_description\",\n",
    "    \"course_title\",\n",
    "    \"course_display_name\"]\n",
    "    \n",
    "    tp_cl = TextPreprocessor()\n",
    "    for clean_column in clean_course_columns:\n",
    "        df_c_cleaned[clean_column]=df_c_cleaned[clean_column].astype(str)\n",
    "        df_c_cleaned = tp_cl.clean_lem_stop(df_c_cleaned,clean_column)\n",
    "    \n",
    "    df_c_cleaned,course_descriptions = course_clean_cl.clean_course_df(df_c_cleaned)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_c_cleaned_2 = df_c_cleaned.copy()\n",
    "    df_c_cleaned_2[\"skills_tagged\"] = df_c_cleaned_2[\"full_description\"].apply(lambda x:pm.phrase_matcher_model(x,skills_list))\n",
    "    \n",
    "    course_columns_imp = [\n",
    "     'title',\n",
    "     'url',\n",
    "     'description',\n",
    "     'headline',\n",
    "     'num_subscribers',\n",
    "     'rating',\n",
    "     'num_reviews',\n",
    "     'num_quizzes',\n",
    "     'num_lectures',\n",
    "     'num_curriculum_items',\n",
    "     'requirements_data',\n",
    "     'what_you_will_learn_data',\n",
    "     'target_audiences',\n",
    "     'estimated_content_length',\n",
    "     'content_info',\n",
    "     'instructional_level',\n",
    "     'objectives',\n",
    "     'full_description',\n",
    "     'skills_tagged']\n",
    "    \n",
    "    df_c_cleaned_2 = df_c_cleaned_2[course_columns_imp]\n",
    "\n",
    "    ## save dataframes\n",
    "    '''\n",
    "    dw = DataWriter()\n",
    "    dw.save_data_frame(df_c_cleaned_2,\"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/course_cleaned.csv\")\n",
    "\n",
    "    dw.save_data_frame(df_j_cleaned,\"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/job_cleaned_all_titles_salary.csv\"\")\n",
    "    '''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3a5dc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nyzy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cb32030-d7d1-4b1e-8b4d-9eca04e981ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999e385-4f9f-4572-a1c5-8c3e2a35d718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d910ee-3b0e-47ce-94cb-d86b3053cf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abf6d4-5510-4745-ac35-06795b20ea36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208aa7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e546e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04355871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a44089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468c041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e274e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3a78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff725b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d3088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5307e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e907c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8f9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c3890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4a8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f4817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb235fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
