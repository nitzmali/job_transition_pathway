{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80527b1-e28c-48b1-9fe0-dc2bf6ebe0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 00:15:10.583862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import os \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "# Set a random seed for reproducibility\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "import pandas as pd \n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94394659-5e3b-4372-9c73-47e9b1e42863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loaddata:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "\n",
    "    ## Load data \n",
    "    def load_d(self):\n",
    "        synthetic_user_data = pd.read_csv(\"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/synthetic_data_cleaned_v5.csv\")\n",
    "        import ast \n",
    "        #synthetic_user_data=synthetic_user_data[:100]\n",
    "        df = synthetic_user_data.copy()\n",
    "        # Fill missing values if necessary\n",
    "        df.fillna({'Course Taken': 'no course','course title': 'no course'}, inplace=True)\n",
    "        df[\"Course Taken\"]=df[\"Course Taken\"].apply(lambda x:x.replace(\"NAN\",\"No Course\"))\n",
    "        \n",
    "        #df[\"Course Skills\"]=df[\"Course Skills\"].apply(lambda x:ast.literal_eval(x))\n",
    "        df[\"Course Skills\"].apply(lambda x: x if len(x)!=0 else \"no skills\")\n",
    "        #df[\"Course title\"]=df[\"Course title\"].apply(lambda x:x.replace(\"NAN\",\"No Course\"))\n",
    "        def set_to_string(skill_set):\n",
    "            if not skill_set:  # Checks if the set is empty\n",
    "                return 'no skills' \n",
    "            # Check if skill_set is a string that needs to be evaluated\n",
    "            if isinstance(skill_set, str):\n",
    "                try:\n",
    "                    # Try to evaluate the string as a set\n",
    "                    skill_set = ast.literal_eval(skill_set)\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # Handle cases where the string is not a valid set\n",
    "                    pass  # You might want to return a default value or handle this case as needed\n",
    "            # Convert to string if it's a set or list\n",
    "            if isinstance(skill_set, (set, list)):\n",
    "                return ', '.join(skill_set)\n",
    "            return skill_set \n",
    "        \n",
    "        set_columns = ['Current Skills', 'Next Skills', 'Skill Gap', 'Course Skills']\n",
    "        for col in set_columns:\n",
    "            print(col)\n",
    "            #df[col]=df[col].apply(lambda x:ast.literal_eval(x))\n",
    "            df[col] = df[col].apply(set_to_string)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #df = pd.read_csv(\"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/df_embedding_bert.csv\")\n",
    "        \n",
    "        # Loading the JSON file back into a dictionary\n",
    "        import json\n",
    "        Job_progression_dictionary_file_path = \"/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/job_progression_dictionary.json\"\n",
    "        \n",
    "        with open(Job_progression_dictionary_file_path, 'r') as json_file:\n",
    "            sorted_full_job_progression_dict_lower = json.load(json_file)\n",
    "        \n",
    "        #print(sorted_full_job_progression_dict_lower)\n",
    "        # Output: {'name': 'John', 'age': 30, 'city': 'New York'}\n",
    "\n",
    "        df = df.drop(columns=[ 'Unnamed: 0'])\n",
    "        df = df[df[\"random next job\"]!=1].drop_duplicates()\n",
    "        df = df[['Current Job', 'Next Job', 'Current Skills']].drop_duplicates()\n",
    "        return df,sorted_full_job_progression_dict_lower\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36a4508a-02a8-47c6-95fa-55b93795b56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Skills\n",
      "Next Skills\n",
      "Skill Gap\n",
      "Course Skills\n"
     ]
    }
   ],
   "source": [
    "ld_cl = loaddata()\n",
    "df,sorted_full_job_progression_dict_lower = ld_cl.load_d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "242f6ea3-7098-4522-a916-6e2336ae534c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "class create_embedding:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def batch_encode(texts, batch_size=32):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
    "            outputs = model(inputs)\n",
    "            batch_embeddings = tf.reduce_mean(outputs.last_hidden_state, 1).numpy()\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        return embeddings\n",
    "    #text_columns = ['Current Job', 'Next Job', 'Current Skills', 'Next Skills', 'Skill Gap', 'course title', 'Course Skills', 'Course Taken']\n",
    "    text_columns = ['Current Job', 'Next Job', 'Current Skills']\n",
    "    embeddings_dict = {}\n",
    "    for col in text_columns:\n",
    "        embeddings_dict[col] = batch_encode(df[col].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "145af01f-8377-45b8-a602-5e9892101b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "414e59d9-b3e4-4b67-940a-94fbfeb5a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#job prediction\n",
    "\n",
    "# load embeddings \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "class prep_data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def prepare_data(self,df,tokenizer,model):\n",
    "        # Add 'start' and 'end' tokens to each target sequence\n",
    "        df['Next Job'] = df['Next Job'].apply(lambda x: 'start ' + x + ' end')\n",
    "        \n",
    "        \n",
    "        loaded_embeddings = np.load('/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/processed_datasets_v2/embeddings_data/all_embeddings_job_only.npz')\n",
    "        \n",
    "        #'Next Skills', 'Skill Gap', 'course title', 'Course Skills'\n",
    "        feature_columns = ['Current Job', 'Current Skills' ]\n",
    "        # Stack embeddings horizontally (axis=1)\n",
    "        all_embeddings = np.hstack((loaded_embeddings[each] for each in feature_columns)) # Add other embeddings as needed\n",
    "        \n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(df['Next Job'])  # Replace with your target column\n",
    "        target_seqs = tokenizer.texts_to_sequences(df['Next Job'])\n",
    "        target_seqs_padded = pad_sequences(target_seqs, padding='post')\n",
    "        \n",
    "        # Vocabulary size for the output\n",
    "        target_vocab_size = len(tokenizer.word_index) + 1\n",
    "        \n",
    "        # Shift target sequences for the decoder's training\n",
    "        decoder_input_data = target_seqs_padded[:, :-1]  # all except last token\n",
    "        decoder_target_data = target_seqs_padded[:, 1:]  # all except first token\n",
    "        \n",
    "        \n",
    "        #y_course = loaded_embeddings['Course Taken_embedding']\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # Assuming 'target' is your target array\n",
    "        X_train, X_test, y_job_train, y_job_test = train_test_split(all_embeddings, target_seqs_padded, test_size=0.2, random_state=42)\n",
    "        decoder_input_train, decoder_input_test = train_test_split(decoder_input_data, test_size=0.2, random_state=42)\n",
    "        decoder_target_train, decoder_target_test = train_test_split(decoder_target_data, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Reshape input embeddings for the model\n",
    "        # This depends on the expected input shape of your model. \n",
    "        # For example, if your model expects 3D input (samples, timesteps, features):\n",
    "        X_train_reshaped = X_train.reshape(X_train.shape[0], len(feature_columns), -1)\n",
    "        X_test_reshaped = X_test.reshape(X_test.shape[0], len(feature_columns), -1)\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        X_train_tensor = tf.convert_to_tensor(X_train_reshaped, dtype=tf.float32)\n",
    "        X_test_tensor = tf.convert_to_tensor(X_test_reshaped, dtype=tf.float32)\n",
    "        decoder_input_train_tensor = tf.convert_to_tensor(decoder_input_train, dtype=tf.float32)\n",
    "        decoder_input_test_tensor = tf.convert_to_tensor(decoder_input_test, dtype=tf.float32)\n",
    "        decoder_target_train_tensor = tf.convert_to_tensor(decoder_target_train, dtype=tf.float32)\n",
    "        decoder_target_test_tensor = tf.convert_to_tensor(decoder_target_test, dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        # Verify the new shapes\n",
    "        print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "        print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
    "        \n",
    "        print(\"decoder_input_train_tensor shape:\", decoder_input_train_tensor.shape)\n",
    "        print(\"decoder_input_test_tensor shape:\", decoder_input_test_tensor.shape)\n",
    "        print(\"decoder_target_train_tensor shape:\", decoder_target_train_tensor.shape)\n",
    "        print(\"decoder_target_test_tensor shape:\", decoder_target_test_tensor.shape)\n",
    "\n",
    "        return X_train_tensor,X_test_tensor,decoder_input_train_tensor,decoder_input_test_tensor,decoder_target_train_tensor,\\\n",
    "                decoder_target_test_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848e1f2-2604-4b16-b41a-84bdaede7011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac0eeb-bbd6-4af9-a86d-c93934c41f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2a30-f324-4bbe-a7c2-a56c1b30b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "class define_model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def model_creation(self,feature_columns):\n",
    "        # Model parameters\n",
    "        bert_embedding_dim = 768\n",
    "        latent_dim = 256  # Dimensionality of the LSTM layer\n",
    "        target_vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size for the output\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_inputs = Input(shape=(len(feature_columns), bert_embedding_dim))  # 2 features, each with a BERT embedding of size 768\n",
    "        encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_inputs = Input(shape=(None,))  # 'None' allows the model to handle variable length sequences\n",
    "        decoder_embedding = Embedding(target_vocab_size, latent_dim)\n",
    "        decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "        decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
    "        \n",
    "        # Embed and decode the sequence\n",
    "        dec_emb = decoder_embedding(decoder_inputs)\n",
    "        decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        \n",
    "        # Define the model\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "        return model\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input sequence to get the internal states\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with only the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    start_token_index = tokenizer.word_index['start']  # Assuming you have a 'start' token\n",
    "    target_seq[0, 0] = start_token_index\n",
    "\n",
    "    # Sampling loop to generate sequence\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token and add its corresponding word to the decoded sequence\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer.index_word[sampled_token_index]\n",
    "        if sampled_word != 'end':  # Assuming you have an 'end' token\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n",
    "        if sampled_word == 'end' or len(decoded_sentence) > 50:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1) and states\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db21ad94-3002-4ffe-aa36-fa0976f289f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    prep_data_cl = prep_data()\n",
    "    X_train_tensor,X_test_tensor,decoder_input_train_tensor,decoder_input_test_tensor,decoder_target_train_tensor,\\\n",
    "                    decoder_target_test_tensor = prep_data_cl.prepare_data(df,tokenizer,model)\n",
    "    md_cl =define_model()\n",
    "    model = md_cl.model_creation(feature_columns)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_train_tensor, decoder_input_train_tensor], \n",
    "        np.expand_dims(decoder_target_train_tensor, -1),  # Add an extra dimension to the target\n",
    "        batch_size=32,\n",
    "        epochs=20,\n",
    "        validation_data=(\n",
    "            [X_test_tensor, decoder_input_test_tensor], \n",
    "            np.expand_dims(decoder_target_test_tensor, -1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Encoder model for inference\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    # Decoder setup for inference\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    dec_emb2 = decoder_embedding(decoder_inputs)  # Reuse the same embedding layer\n",
    "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "    decoder_states2 = [state_h2, state_c2]\n",
    "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs2] + decoder_states2\n",
    "    )\n",
    "    \n",
    "    for i in range(100):  # Generate predictions for the first 10 test samples\n",
    "        input_seq = X_test_tensor[i: i + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq)\n",
    "        print('Predicted sequence:', decoded_sentence)\n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    def strip_target_variable(target_value):\n",
    "        target_value=target_value.replace(\"start\",\"\").replace(\"end\",\"\").lstrip().rstrip()\n",
    "        return target_value\n",
    "    # Generate predictions for the test set\n",
    "    y_pred = [decode_sequence(X_test_tensor[i: i + 1]) for i in range(len(X_test_tensor))]\n",
    "    \n",
    "    # You need to have your test target sequences in text format for comparison\n",
    "    y_true = [\" \".join(tokenizer.sequences_to_texts([y])) for y in y_job_test]\n",
    "    \n",
    "    y_pred = list(map(strip_target_variable,y_pred))\n",
    "    y_true = list(map(strip_target_variable,y_true))\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    ## save the models for prediction \n",
    "    \n",
    "    from tensorflow.keras.models import Model, load_model\n",
    "    \n",
    "    # Assuming encoder_model and decoder_model are already defined as shown in your snippet\n",
    "    \n",
    "    \n",
    "    # Save the encoder model\n",
    "    encoder_model.save('/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/processed_datasets_v2/models/job/encoder_model.h5')\n",
    "    \n",
    "    # Save the decoder model\n",
    "    decoder_model.save('/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/processed_datasets_v2/models/job/decoder_model.h5')\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    # Assuming 'tokenizer' is your tokenizer object\n",
    "    with open('/Users/nyzy/Library/CloudStorage/GoogleDrive-nitinmali999@gmail.com/.shortcut-targets-by-id/1yEbTjc1DwwTDd2CB86Il6F-3DhFUn8pU/Data Mining/data/processed_datasets/processed_datasets_v2/models/job/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce89ef-19f1-4180-8170-ae61f175e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
